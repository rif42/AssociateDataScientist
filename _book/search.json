[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Associate Data Scientist",
    "section": "",
    "text": "1 Associate Data Scientist\nModul ini dibuat untuk mendampingi mahasiswa yang mengikuti pelatihan Associate Data Scientist."
  },
  {
    "objectID": "modul_1.html",
    "href": "modul_1.html",
    "title": "2  Data Science Tools",
    "section": "",
    "text": "3 Konsep Data Science\nData science adalah bidang yang berkaitan dengan ekstraksi pengetahuan dan wawasan dari data. Data science menggabungkan konsep dan metode dari berbagai disiplin ilmu seperti statistika, matematika, komputer, dan pemodelan untuk menganalisis dan menginterpretasikan data secara sistematis.\nTahukah anda bahwa 70% pengembangan software gagal? Data science adalah bagian dari pengembangan software yang mempunyai tingkat kegagalan paling tinggi di angka 80% tingkat kegagalan.\n\nBanyak sekali masalah yang dihadapi di dalam proses pengembangan software. Mulai dari perubahan requirement, maintenance yang kurang, hingga dokumentasi yang tidak lengkap. Oleh karena itu, pemahaman terhadap pengembangan model sangatlah penting, agar semua usaha kita untuk membuat sebuah aplikasi tidak sia sia, dan dapat membantu orang banyak."
  },
  {
    "objectID": "modul_1.html#python-versi-3.10.10",
    "href": "modul_1.html#python-versi-3.10.10",
    "title": "2  Data Science Tools",
    "section": "2.1 Python versi 3.10.10",
    "text": "2.1 Python versi 3.10.10\n\nBuka browser, kunjungi https://www.python.org/downloads/windows/ , cari bagian Python 3.10.10 - Feb 8, 2023, lalu pilih Download Windows Installer.\nJalankan file yang di download, tunggu sampai proses installing selesai.\nBuka Command Prompt, lalu jalankan command “python –version”\n\nJika hasil dari command tersebut adalah “3.10.10” maka instalasi python sudah selesai.\n\n\n\nGambar3. Python di cmd"
  },
  {
    "objectID": "modul_1.html#development-environment",
    "href": "modul_1.html#development-environment",
    "title": "2  Data Science Tools",
    "section": "2.2 Development Environment",
    "text": "2.2 Development Environment\nEnvironment adalah lingkungan yang kita tinggali selama proses koding. Di course ini kita bisa menggunakan salah satu dari tiga aplikasi, yaitu VSCode, Jupyter dan Google Colab. Cukup pilih salah satu dari ketiga opsi Kami merekomendasikan untuk menggunakan Jupyter notebook. Untuk alasannya, silahkan simak slide selanjutnya\n\n2.2.1 VSCode Notebook\nVSCode adalah sebuah code editor multifungsi yang cukup ringan dan handal. Kelebihan dari VSCode adalah fleksibilitas dari VSCode. Anda dapat mengkustomisasi tema, menambah ekstensi, dan mengubah semua settingan yang ada di VSCode. Anda mempunyai kendali penuh. Kekurangan dari VSCode adalah terkadang settingan VSCode terlalu kompleks untuk digunakan.\nBerikut langkah-langkah untuk menginstalasi VScode Notebook :\n\nBuka browser, kunjungi https://code.visualstudio.com/download# pilih salah satu link yang berada di System Installer.\n\nJalankan file yang di download, tunggu sampai proses installing selesai.\n\nSelanjutnya kita akan menginstall beberapa ekstensi.\n\nBuka VSCode, pergi ke bagian Extensions, (ctrl+shift+x) lalu ketik “Python”, lalu install ekstensi yang muncul di paling atas.\n\n\nMasih di tab Extensions, ketik “Jupyter”, lalu instal ekstensi yang muncul di paling atas\n\n\nReload VSCode\n\n\n\n2.2.2 Jupyter Notebook\nJupyter Notebook adalah aplikasi yang digunakan untuk memproses dan menampilkan teks, kode, dan data secara bersamaan. Jupyter sangat cocok untuk data science karena Jupyter sangat ringan, mudah di install, dan mudah untuk digunakan. Oleh karena itu, kita merekomendasikan anda untuk menggunakan Jupyter Notebook untuk pelatihan ini.\nBerikut langkah-langkah untuk menginstalasi Jupyter Notebook :\n- Buka Windows start menu, ketik “cmd”\n- Klik kanan pada item command prompt, lalu pilih run as administrator\n- Di dalam command prompt, ketik “pip install jupyterlab”\n- Tunggu sampai proses selesai\n- Ketik “jupyter-lab” untuk menjalankan aplikasi jupyter"
  },
  {
    "objectID": "modul_1.html#install-libraries",
    "href": "modul_1.html#install-libraries",
    "title": "2  Data Science Tools",
    "section": "2.3 Install Libraries",
    "text": "2.3 Install Libraries\n\nBuka terminal (ctrl+shift+~) lalu ketik “pip install [package name]” Contoh : “pip install pandas”, dimana pandas adalah sebuah nama dari package Untuk membuka terminal di Jupyter Notebook, buka launcher baru (ctrl+shift+L) lalu pilih terminal.\n\nPackage name diisi dengan list dibawah ini\n\nnumpy\nscipy\npandas\nmatplotlib\nseaborn\nscikit"
  },
  {
    "objectID": "modul_1.html#workflow",
    "href": "modul_1.html#workflow",
    "title": "2  Data Science Tools",
    "section": "3.1 Workflow",
    "text": "3.1 Workflow\nDi proses pekerjaan pengolahan data, Data Scientist bekerja untuk mengembangkan model terbaik dari data untuk menjawab permasalahan bisnis. Data Scientist mengambil data dengan bantuan Data Engineer dan DevOps Engineer, lalu menganalisa data tersebut. Hasil analisa data dikembalikan ke mereka atau dipresentasikan ke anggota lain.\n\n\n\nGambar1. Contoh workflow data scientist"
  },
  {
    "objectID": "modul_1.html#mengapa-data-scientist-dibutuhkan",
    "href": "modul_1.html#mengapa-data-scientist-dibutuhkan",
    "title": "2  Data Science Tools",
    "section": "3.2 Mengapa Data Scientist Dibutuhkan?",
    "text": "3.2 Mengapa Data Scientist Dibutuhkan?\nAplikasi Modern mempunyai skala yang jauh lebih besar dari aplikasi-aplikasi sebelumnya. Dalam satu waktu, jumlah user bisa mencapai ratusan ribu, bahkan sampai jutaan. Masing-masing user menghasilkan beberapa request ke server untuk mengambil data, dan menghasilkan data tersendiri. Hasilnya, data yang dibuat, disimpan, dan diproses sangatlah besar. Fenomena ini dinamakan: BIG DATA"
  },
  {
    "objectID": "modul_1.html#apa-yang-dimaksud-big-data",
    "href": "modul_1.html#apa-yang-dimaksud-big-data",
    "title": "2  Data Science Tools",
    "section": "3.3 Apa Yang Dimaksud Big Data?",
    "text": "3.3 Apa Yang Dimaksud Big Data?\nBig data mempunyai karakteristik yang dinamakan 5V"
  },
  {
    "objectID": "modul_1.html#tahap-pengembangan-sistem-ai-berbasis-big-data",
    "href": "modul_1.html#tahap-pengembangan-sistem-ai-berbasis-big-data",
    "title": "2  Data Science Tools",
    "section": "4.1 Tahap Pengembangan Sistem AI Berbasis Big Data:",
    "text": "4.1 Tahap Pengembangan Sistem AI Berbasis Big Data:\n\nPelatihan, Adalah tahap dimana kita membangun sebuah model dari nol. Proses pelatihan meliputi pemilihan dataset, pemilihan algoritma, penyetelan parameter, testing, dan lain lain\n\nPenggunaan, Ketika model sudah dibuat dan siap untuk digunakan, model akan di deploy di internet agar dapat digunakan oleh semua orang.\n\n\n4.1.1 Pelatihan\n\n\n\nGambar7. Flow dari proses training model machine learning\n\n\n\n\n4.1.2 Penggunaan\n\n\n\nGambar8. Flow dari proses penggunaan model machine learning"
  },
  {
    "objectID": "modul_1.html#software-development-life-cycle-sdlc",
    "href": "modul_1.html#software-development-life-cycle-sdlc",
    "title": "2  Data Science Tools",
    "section": "4.2 Software Development Life Cycle (SDLC)",
    "text": "4.2 Software Development Life Cycle (SDLC)\nMetode SDLC (Software Development Life Cycle) adalah proses pembuatan dan pengubahan sistem serta model dan metodologi yang digunakan untuk mengembangkan sistem rekayasa perangkat lunak.\n\nProses logika yang digunakan oleh seorang analis sistem untuk mengembangkan sebuah sistem informasi yang melibatkan requirements, validation, training dan pemilik sistem proses yang memproduksi sebuah software dengan kualitas setinggi-tingginya tetapi dengan biaya yang serendah-rendahnya (Stackify)"
  },
  {
    "objectID": "modul_1.html#jenis-jenis-sdlc",
    "href": "modul_1.html#jenis-jenis-sdlc",
    "title": "2  Data Science Tools",
    "section": "4.3 Jenis-Jenis SDLC",
    "text": "4.3 Jenis-Jenis SDLC\nBerikut adalah beberapa jenis dari SDLC:\n\n\n\nGambar10. Jenis jenis SDLC\n\n\n\n4.3.1 Agile\nMetode pengembangan Agile adalah pendekatan kolaboratif dan iteratif dalam pengembangan perangkat lunak. Dalam metode ini, pengembangan dilakukan dalam iterasi kecil yang disebut “sprints”\nKelebihan\n- Fleksibilitas tinggi - Kolaborasi erat antar tim - Aplikasi di-update secara rutin - Fokus kepada kebutuhan pengguna\nKekurangan\n- Sulitnya merencanakan anggaran dan timeline - Memerlukan tingkat disiplin yang tinggi dan pengawasan konstan - Kurang efektif untuk project skala besar\n\n\n4.3.2 Waterfall\nMetode pengembangan Waterfall adalah pendekatan linier dan berurutan dalam pengembangan perangkat lunak. Dalam metode ini, setiap fase pengembangan (analisis kebutuhan, desain, implementasi, pengujian, dan pemeliharaan) dilakukan secara berurutan, di mana setiap fase harus selesai sebelum melanjutkan ke fase berikutnya.\nKelebihan\n- Struktur yang jelas\n- Dokumentasi lengkap dan rapi\n- Cocok untuk proyek yang kebutuhannya tidak berubah-ubah\nKekurangan\n- Kurang fleksibel ketika menghadapi kebutuhan yang berubah\n- Keterbatasan dalam kolaborasi dan feedback\n- Risiko terlambatnya mendeteksi kesalahan/glitch\n\n\n4.3.3 Prototype\nMetode pengembangan prototyping adalah pendekatan dalam pengembangan perangkat lunak di mana prototipe perangkat lunak awal dibuat dan digunakan untuk mengumpulkan umpan balik dari pengguna dan pemangku kepentingan. Prototipe tersebut dapat berupa versi sederhana atau parsial dari perangkat lunak yang akhir.\nKelebihan\n- Feedback didapatkan lebih awal\n- Klarifikasi kebutuhan lebih jelas\n- Meningkatkan kolaborasi tim\nKelemahan\n- Membutuhkan banyak waktu dan sumber daya\n- Potensi kesalahan di aspek desain dan efisiensi besar\n- Kesulitan dalam timing untuk berhenti mengembangkan prototipe"
  },
  {
    "objectID": "modul_1.html#rapid-application-development-rad",
    "href": "modul_1.html#rapid-application-development-rad",
    "title": "2  Data Science Tools",
    "section": "4.4 Rapid Application Development (RAD)",
    "text": "4.4 Rapid Application Development (RAD)\nMetode pengembangan Rapid Application Development (RAD) adalah pendekatan iteratif dan kolaboratif dalam pengembangan perangkat lunak yang menekankan kecepatan, fleksibilitas, dan keterlibatan pengguna. Dalam metode ini, pengembangan dilakukan dalam siklus pendek yang disebut “iterasi” dan melibatkan partisipasi aktif dari pengguna.\nKelebihan\n- Waktu pengembangan cepat\n- Keterlibatan pengguna secara sering dan langsung\n- Fleksibilitas tinggi\n- Tingginya kolaborasi terhadap developer dan pengguna\nKelemahan\n- Ketergantungan terhadap pengguna tinggi\n- Sulit untuk diskalakan\n- Kualitas dan efisiensi arsitektur rendah"
  },
  {
    "objectID": "modul_1.html#kesimpulan",
    "href": "modul_1.html#kesimpulan",
    "title": "2  Data Science Tools",
    "section": "4.5 Kesimpulan",
    "text": "4.5 Kesimpulan\nKesimpulannya, Di dunia software development, tidak ada metode yang lebih baik atau buruk, hanya ada pengorbanan atau ‘tradeoff’.\nUntuk melakukan proses development yang baik dan efisien, kita harus memahami kebutuhan, kondisi, dan sumber daya yang kita punya. Pemahaman yang baik memungkinkan kita untuk memilih tradeoff mana yang akan diambil. Setelah itu kita menyesuaikan tradeoff yang kita pilih dengan jenis SDLC yang sudah ada."
  },
  {
    "objectID": "modul_2.html",
    "href": "modul_2.html",
    "title": "2  Data Understanding",
    "section": "",
    "text": "3 Sumber, Susunan, Tipe dan Model Data\nData mempunyai berbagai jenis, bentuk, ukuran dan nilai. Sebagai Data Scientist, kita harus mengetahui sifat, jenis, asal, dan bentuk dari data yang kita olah.\nProses pengambilan data, juga dikenal sebagai proses ekstraksi data, merujuk pada langkah-langkah yang dilakukan untuk mengumpulkan, mengakses, dan memperoleh data dari sumber yang relevan\nAda beberapa Cara untuk mengambil data, yaitu:"
  },
  {
    "objectID": "modul_2.html#mengapa-kita-perlu-data-understanding",
    "href": "modul_2.html#mengapa-kita-perlu-data-understanding",
    "title": "2  Data Understanding",
    "section": "2.1 Mengapa Kita Perlu Data Understanding",
    "text": "2.1 Mengapa Kita Perlu Data Understanding\nDi bidang machine learning, ada sebuah mantra yang semua data scientists harus ikuti; Garbage in, garbage out. Artinya, jika data yang dimasukkan ke model itu jelek, maka apapun yang terjadi, hasil dari model akan selalu jelek.\nSemua data yang kita dapatkan belum tentu bagus dan bisa kita olah lebih lanjut karena:\n\nMaksud dan tujuan data berbeda\n\nKeadaan data terlalu terpisah, atau terlalu terintegrasi\n\nKekayaan atau value data berbeda beda\n\nKeandalan atau reliability data berbeda beda"
  },
  {
    "objectID": "modul_2.html#data-understanding-documentation",
    "href": "modul_2.html#data-understanding-documentation",
    "title": "2  Data Understanding",
    "section": "2.2 Data Understanding Documentation",
    "text": "2.2 Data Understanding Documentation\nData Understanding Documentation adalah sebuah prosedur yang harus kita ikuti untuk mempelajari data yang akan kita gunakan dan menulis hasil pekerjaan kita secara terstruktur agar dapat dicerna oleh orang lain dengan mudah.\nData Understanding Documentation mempunyai 4 komponen, yaitu:\n\nCollection\n\nData Sources\n\nData Attribute\n\nData Count\n\nData Merge\n\nDescription\n\nDescribing data\n\nData Quantity\n\nData Type\n\nKey Attributes\n\nPriority of Attributes\n\nExploration\n\nHypothesis of Data\n\nAttribute selection\n\nExploration of data characteristics\n\nCompare with goal\n\nIdentify subset of data\n\nQuality\n\nMissing values\n\nData errors/corruptions\n\nInconsistencies\n\nOutliers\n\nNoise\n\n\n\n\n\nGambar2. Komponen Data Understanding"
  },
  {
    "objectID": "modul_2.html#sumber-data",
    "href": "modul_2.html#sumber-data",
    "title": "2  Data Understanding",
    "section": "3.1 Sumber Data",
    "text": "3.1 Sumber Data\n\n3.1.1 Internal\nData internal merupakan data yang sumbernya berasal dari dalam pihak yang dijadikan objek penelitian. Contoh data meliputi :\n- Spreadsheet (excel, csv, json)\n- Database (sql)\n- Teks (txt, rtf)\n- Media (video, audio)\n\n\n3.1.2 External\nData eksternal merupakan data yang sumbernya berasal dari luar pihak objek yang diteliti. Contoh data meliputi :\n- Website repository data (Kaggle, sklearn)\n- Web page domain public (wikipedia, dbdata, data.go.id)\n- Public Dataset (worldbank, UNICEF, WHO)"
  },
  {
    "objectID": "modul_2.html#susunan-data",
    "href": "modul_2.html#susunan-data",
    "title": "2  Data Understanding",
    "section": "3.2 Susunan Data",
    "text": "3.2 Susunan Data\nSusunan data, juga dikenal sebagai struktur data, merujuk pada cara data disimpan, diatur, dan dihubungkan satu sama lain dalam suatu sistem komputasi. Ini melibatkan pemilihan format dan metode penyimpanan yang tepat agar data dapat diakses, dimanipulasi, dan dicari dengan efisien. Susunan data yang baik membantu meningkatkan efisiensi operasional, performa sistem, dan kemampuan pengambilan keputusan.\n\n\n\nGambar4. Struktur Data\n\n\nDatum adalah satuan terkecil, sebuah kumpulan teks dan angka. Pada bentuk ini, data tidak mempunyai nilai jual apapun.\nNamun ketika kita menyusun beberapa datum menjadi satu kolom atau baris, kita dapat mendeskripsikan sebuah objek atau makna tertentu.\nKumpulkan banyak data maka kita menciptakan sebuah Informasi, dataset, atau konteks yang mudah dicerna oleh manusia.\nJika kita analisa sebuah dataset, kita akan mendapatkan sebuah hipotesis yang dapat dikonversi menjadi fakta atau knowledge setelah diverifikasi.\nDari sekumpulan knowledge, kita dapat menyusun sebuah decision atau keputusan yang sangat berpengaruh di kehidupan kita sehari-hari\nSusunan Data dibagi menjadi 2, yaitu :\n\n3.2.1 Structured\nData terstruktur adalah jenis data yang memiliki format atau skema yang terorganisir dengan jelas. Setiap kolom dalam data terstruktur memiliki tipe data yang konsisten, dan setiap baris berisi entitas atau objek yang serupa.\nContoh data terstruktur meliputi :\n- Tabular Data\n- Object Oriented Data\n- Time-series data\n\n\n3.2.2 Unstructured\nData tidak terstruktur merujuk pada jenis data yang tidak memiliki format atau skema yang terorganisir dengan jelas. Data ini seringkali memiliki struktur yang tidak teratur atau tidak terprediksi, sehingga sulit untuk mengklasifikasikan, mengatur, atau memodelkannya secara tradisional.\nContoh data tidak terstruktur meliputi :\n- Video atau audio\n- Dokumen HTML\n- Tweet atau postingan sosial media"
  },
  {
    "objectID": "modul_2.html#tipe-data",
    "href": "modul_2.html#tipe-data",
    "title": "2  Data Understanding",
    "section": "3.3 Tipe Data",
    "text": "3.3 Tipe Data\n\n3.3.1 Tipe Data Berdasarkan Sifatnya\n\nData dikotomi, merupakan data yang bersifat pilah satu sama lain, misalnya suku, agama, jenis kelamin, pendidikan, dan lain sebagainya.\n\nData diskrit, merupakan data yang proses pengumpulan datanya dijalankan dengan cara menghitung atau membilang. Seperti, jumlah anak, jumlah penduduk, jumlah kematian dan sebagainya.\n\nData kontinum, merupakan data pengumpulan datanya didapatkan dengan cara mengukur dengan alat ukur yang memakai skala tertentu. Seperti misalnya, Suhu, berat, bakat, kecerdasan, dan lainnya.\n\n\n\n3.3.2 Tipe Data Berdasarkan Cara Pengumpulan\n\nData primer, merupakan data yang didapatkan dari sumber pertama, atau dapat dikatakan pengumpulannya dilakukan sendiri oleh si peneliti secara langsung, seperti hasil wawancara dan hasil pengisian kuesioner (angket).\n\nData sekunder, merupakan data yang didapatkan dari sumber kedua. Menurut Purwanto (2007), data sekunder yaitu data yang dikumpulkan oleh orang atau lembaga lain. Data sekunder adalah data yang digunakan atau diterbitkan oleh organisasi yang bukan pengolahnya (Soeratno dan Arsyad (2003;76).\n\n\n\n3.3.3 Tipe Data Berdasarkan Seri Waktu\n\nData Cross Section, Data cross-section adalah data yang menunjukkan titik waktu tertentu. Contohnya laporan keuangan per 31 Desember 2020, data pelanggan PT. Data Indah bulan mei 2004, dan lain sebagainya.\n\nData Time Series / Berkala, Data berkala adalah data yang datanya menggambarkan sesuatu dari waktu ke waktu atau periode secara historis. Contoh data time series adalah data perkembangan nilai tukar dollar amerika terhadap rupiah tahun 2016 - 2020."
  },
  {
    "objectID": "modul_2.html#web-scraping",
    "href": "modul_2.html#web-scraping",
    "title": "2  Data Understanding",
    "section": "4.1 Web Scraping",
    "text": "4.1 Web Scraping\nWeb scraping artinya mengekstraksi data secara langsung dari suatu halaman web.\nLangkah-langkah umum (contoh detil dapat dilihat di https://realpython.com/beautiful-soup-web-scraper-python/)\n\nTentukan URL halaman web (HTML) yang akan di-scrape.\n\nGunakan fungsi requests.get untuk mengakses URL tersebut. Teks HTML akan tersimpan pada atribut text dari object yang dikembalikan requests.get.\n\nLakukan parsing pada HTML dengan library beautifulsoup untuk memperoleh tabel data yang diinginkan (dengan mengekstraksi elemen-elemen HTML yang relevan)."
  },
  {
    "objectID": "modul_2.html#application-program-interface-api",
    "href": "modul_2.html#application-program-interface-api",
    "title": "2  Data Understanding",
    "section": "4.2 Application Program Interface (API)",
    "text": "4.2 Application Program Interface (API)\nAPI adalah sebuah alat untuk memudahkan website dan pengguna saling bertukar informasi. API disediakan oleh berbagai website atau perusahaan seperti Kaggle dan Twitter. Biasanya API yang dimiliki perusahaan bersifat private, sehingga dibutuhkan sebuah token khusus untuk mengaksesnya. Namun ada beberapa API publik yang dapat diakses oleh siapa saja seperti PokeAPI.\n\n\n\nGambar6. Contoh Penggunaan API"
  },
  {
    "objectID": "modul_2.html#manual-kaggle.com",
    "href": "modul_2.html#manual-kaggle.com",
    "title": "2  Data Understanding",
    "section": "4.3 Manual (Kaggle.com)",
    "text": "4.3 Manual (Kaggle.com)\n\nKita akan mengakses data dari “Goal Dataset – Top 5 European Leagues” dari Kaggle.\n\nKunjungi Kaggle.com dan login (buat akun jika perlu)\n\nLakukan pencarian “goal dataset top 5 European leagues”\n\nKlik “Goal Dataset – Top 5 European Leagues”\n\n\n\n\nGambar7. Contoh Pencarian Data di Kaggle"
  },
  {
    "objectID": "modul_2.html#direct-database",
    "href": "modul_2.html#direct-database",
    "title": "2  Data Understanding",
    "section": "4.4 Direct Database",
    "text": "4.4 Direct Database\nData juga dapat bersumber dari basis data relasional (RDB, Mysql, Postgres) organisasi. Berikut langkah-langkahnya :\n\nInstall sqlalchemy (pip install sqlalchemy), pandas (pip install pandas), dan mysql connector (pip install mysql-connector-python)\n\nImport semua library\n\nTentukan username (root), password (kosong), port (3306), dan nama dari database (emp), lalu masukkan ke variable url.\n\nBuat engine menggunakan function create_engine, masukkan url sebagai parameternya\n\nBuat query yang akan dijalankan di database, masukkan ke variable sql\n\nJalankan query menggunakan function execute\n\nTampung hasil query di dataframe\n\nLakukan operasi data di dataframe"
  },
  {
    "objectID": "modul_3.html#apa-itu-telaah-data",
    "href": "modul_3.html#apa-itu-telaah-data",
    "title": "3  Telaah Data",
    "section": "3.1 Apa itu telaah data?",
    "text": "3.1 Apa itu telaah data?\n\nTelaah data merujuk pada proses pengumpulan, pembersihan, eksplorasi, dan analisis data untuk mendapatkan pemahaman yang lebih yang terkandung dalam data tersebut. Tujuan dari telaah data adalah mendukung pengambilan keputusan berdasarkan bukti yang ditemukan dalam data."
  },
  {
    "objectID": "modul_3.html#mengapa-perlu-telaah-data",
    "href": "modul_3.html#mengapa-perlu-telaah-data",
    "title": "3  Telaah Data",
    "section": "3.2 Mengapa perlu telaah data?",
    "text": "3.2 Mengapa perlu telaah data?\n\n\nData dari masing-masing sumber belum tentu dapat langsung dipakai karena:\n\n\nmaksud dan tujuan data berbeda-beda\nkeadaan asal terpisah-pisah atau justru terintegrasi secara ketat.\ntingkat kekayaan (richness) berbeda-beda\ntingkat keandalan (reliability) berbeda-beda\n\n\nData understanding memberikan gambaran awal tentang:\n\n\nkekuatan data\nkekurangan dan batasan penggunaan data\ntingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan\nketersediaan data (terbuka/tertutup, biaya akses, dsb.)"
  },
  {
    "objectID": "modul_3.html#bentuk-data",
    "href": "modul_3.html#bentuk-data",
    "title": "3  Telaah Data",
    "section": "3.3 Bentuk Data",
    "text": "3.3 Bentuk Data\n\nData dapat memiliki berbagai bentuk, diantaranya:\n\nSpreadsheet(excel, csv, dll)\nDatabase(SQL, Oracle, dll)\nText file(txt, doc, pdf, dll)\nMultimedia documents(audio, video, gambar, dll)"
  },
  {
    "objectID": "modul_3.html#sumber-data",
    "href": "modul_3.html#sumber-data",
    "title": "3  Telaah Data",
    "section": "3.4 Sumber Data",
    "text": "3.4 Sumber Data\n\n\nSumber Internal\n\n\nData yang dihasilkan oleh perusahaan sendiri\nData yang dihasilkan oleh perusahaan lain yang terkait dengan perusahaan\n\n\nSumber Eksternal\n\n\nRepositori data publik\nHalaman web publik"
  },
  {
    "objectID": "modul_3.html#daftar-sumber-data-daring",
    "href": "modul_3.html#daftar-sumber-data-daring",
    "title": "3  Telaah Data",
    "section": "3.5 Daftar Sumber Data Daring",
    "text": "3.5 Daftar Sumber Data Daring\n\nPortal Satu Data Indonesia\nPortal Data Jakarta\nPortal Data Bandung\n\nBadan Pusat Statistik\n\nBadan Informasi Geospasial\n\nUCI Machine Learning repository\nKaggle\n\nWorld Bank Open Data"
  },
  {
    "objectID": "modul_3.html#tipe-data",
    "href": "modul_3.html#tipe-data",
    "title": "3  Telaah Data",
    "section": "3.6 Tipe Data",
    "text": "3.6 Tipe Data\n\n3.6.1 Bedasarkan sifat\n\nData dikotomi, merupakan data yang bersifat pilah satu sama lain, misalnya suku, agama, jenis kelamin, pendidikan, dan lain sebagainya.\nData diskrit, merupakan data yang proses pengumpulan datanya dijalankan dengan cara menghitung atau membilang. Seperti, jumlah anak, jumlah penduduk, jumlah kematian dan sebagainya.\nData kontinu, merupakan data pengumpulan datanya didapatkan dengan cara mengukur dengan alat ukur yang memakai skala tertentu. Seperti misalnya, Suhu, berat, bakat, kecerdasan, dan lainnya.\n\n\n\n3.6.2 Bedasarkan waktu\n\nData Cross Section, adalah data yang menunjukkan titik waktu tertentu. Contohnya laporan keuangan per 31 Desember 2020, data pelanggan PT.Data Indah bulan mei 2004, dan lain sebagainya.\nData Time Series / Berkala, adalah data yang datanya menggambarkan sesuatu dari waktu ke waktu atau periode secara historis. Contoh data time series adalah data perkembangan nilai tukar dollar amerika terhadap rupiah tahun 2016 - 2020"
  },
  {
    "objectID": "modul_3.html#pengambilan-data",
    "href": "modul_3.html#pengambilan-data",
    "title": "3  Telaah Data",
    "section": "3.7 Pengambilan Data",
    "text": "3.7 Pengambilan Data\n\nPengambilan data secara manual.\nPengambilan data melalui API\n\nContoh melalui API Kaggle (pip install kaggle)(kaggle datasets download -d mauryansshivam/paytm-revenue-users-transactions)\nContoh melalui API Portal Data Bandung (http://data.bandung.go.id/index.php/portal/api/1db4a0cc-dc0f-4362-91f1-d0ad1b4bce98)\n\nPengambilan data melalui akses langsung ke basis data relasional yang ada.\nPengambilan data melalui web scraping.\n\nContoh pengambilan data melalui web scraping kometar video youtube\n\n\n\n\nCode\n# Jalankan instalasi library berikut jika belum terinstall\n# pip install google-api-python-client\n\nimport csv\nfrom googleapiclient.discovery import build\n\n# Set up the API key and YouTube Data API service\n## Masukkan API key yang sudah dibuat di Google Cloud Platform\n## Contoh : AIzaSyCFjru8dOZbGtZUi_AQu1Cz1MLoANaY22k\nAPI_KEY = \"\"\nyoutube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n\ndef scrape_comments(video_id):\n    # Get the video details\n    video_response = youtube.videos().list(\n        part=\"snippet\",\n        id=video_id\n    ).execute()\n\n    video_title = video_response['items'][0]['snippet']['title']\n    print(\"Scraping comments for video:\", video_title)\n\n    # Get the video comments\n    comments = []\n    next_page_token = None\n\n    while True:\n        comment_response = youtube.commentThreads().list(\n            part=\"snippet\",\n            videoId=video_id,\n            maxResults=100,\n            pageToken=next_page_token\n        ).execute()\n\n        for item in comment_response[\"items\"]:\n            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n            comments.append(comment)\n\n        next_page_token = comment_response.get(\"nextPageToken\")\n\n        if not next_page_token:\n            break\n\n    return comments\n\n# Test the function\n## Masukkan ID video youtube yang ingin diambil komentarnya\n## Contoh : 5kAF9QV5nYQ\nvideo_id = \"\"\ncomments = scrape_comments(video_id)\n\n# Save comments to CSV file\nfilename = \"comments.csv\"\nwith open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow([\"Comment\"])\n    writer.writerows(zip(comments))\n\nprint(\"Comments saved to\", filename)"
  },
  {
    "objectID": "modul_3.html#library-pandas",
    "href": "modul_3.html#library-pandas",
    "title": "3  Telaah Data",
    "section": "3.8 Library Pandas",
    "text": "3.8 Library Pandas\n\n\n\n\n\n\nPandas adalah library yang digunakan untuk melakukan manipulasi, analisis, dan visualisasi data.\nPandas menyediakan struktur data dan fungsi high-level untuk membuat pekerjaan dengan data terstruktur/tabular lebih cepat, mudah, dan ekspresif.\n\n\n3.8.1 Memuat data ke Pandas\n\nNyalakan Jupyter Notebook di folder kerja Anda.\nBuka atau buat baru suatu skrip ipynb (Python 3)\nImport pandas dan numpy. (Pastikan sudah terinstal sebelumnya).\nLoad file CSV yang sudah diunduh sebelumnya (Mengambil Data secara Manual) ke dalam sebuah DataFrame\nGunakan perintah read_csv(dataset)\n\n\nimport pandas as pd\n# Contoh csv\ndf = pd.read_csv('titanic_dataset.csv')\n\n# Contoh excel\ndf = pd.read_excel('titanic_dataset.xlsx')\n\n# Contoh sqlite\nimport sqlite3\nconn = sqlite3.connect('titanic_dataset.sqlite3')\ndf = pd.read_sql_query(\"SELECT * FROM titanic\", conn)\n\n\n\n3.8.2 Menampilkan Data\n\nMethod head() dan tail() pada DataFrame membantu kita menampilkan 5 beberapa baris pertama/terakhir dari data yang kita muat. Dengan memberikan argumen pada method tersebut kita juga bisa mengatur jumlah data yang ditampilkan\n\n# 5 baris pertama\ndf.head()\n\n# 5 baris terakhir\ndf.tail()\n\n# 10 baris pertama\ndf.head(10)\n\n\n\n3.8.3 Melihat Tipe Data\n\nMethod dtypes pada DataFrame membantu kita menampilkan tipe data dari setiap kolom pada data yang kita muat. Jika terdapat tipe data yang tidak tepat maka sebaiknya dilakukan pengecekan nialai pada kolom tersebut. Jika memang sudah benar, maka kita bisa mengubah tipe data tersebut menjadi tipe data yang tepat.\n\n# Melihat tipe data\ndf.dtypes\n\n# Mengubah tipe data menjadi int\ndf['nama_kolom'] = df['nama_kolom'].astype('int')\n\n\n\n3.8.4 Deskripsi statistik data\n\nMethod describe() pada DataFrame membantu kita menampilkan deskripsi statistik dari data yang kita muat. Deskripsi statistik yang ditampilkan adalah deskripsi statistik untuk kolom-kolom dengan tipe data numerik. Jika terdapat kolom dengan tipe data selain numerik, maka deskripsi statistik tersebut tidak akan ditampilkan. Untuk tetap menampilkan deskripsi statistik dari kolom non-numerik, kita bisa menambahkan argumen include='all' pada method describe().\n\n# Deskripsi statistik data\ndf.describe()\n\n# Deskripsi statistik data termasuk kolom non-numerik\ndf.describe(include='all')\n\n\n\n3.8.5 Fungsi statistik dalam Pandas\n\nPandas menyediakan fungsi statistik untuk menghitung nilai rata-rata, median, standar deviasi, dan variansi. Fungsi-fungsi tersebut adalah:\n\n\n\n\n\n\n\nFungsi\nKeterangan\n\n\n\n\ncount\nJumlah observasi non-NULL\n\n\nsum\nJumlah\n\n\nmean\nRata-rata\n\n\nmad\nDeviasi absolut rata-rata\n\n\nmedian\nNilai tengah\n\n\nmin\nNilai minimum\n\n\nmax\nNilai maksimum\n\n\nmode\nModus (nilai yang paling sering muncul)\n\n\nabs\nNilai absolut (nilai mutlak)\n\n\nprod\nHasil kali dari nilai-nilai\n\n\nquantile\nKuantil sampel (nilai pada persentil), kuartil pertama = quantile(0.25)\n\n\nstd\nStandar deviasi\n\n\nvar\nVarians\n\n\nsem\nStandar error mean (standar error dari rata-rata)\n\n\nskew\nSkewness (kecondongan distribusi data)\n\n\nkurt\nKurtosis (tingkat “tumpul” atau “tajam” distribusi data)\n\n\ncumsum\nAkumulasi jumlah\n\n\ncumprod\nAkumulasi hasil kali\n\n\ncummax\nNilai maksimum akumulasi\n\n\ncummin\nNilai minimum akumulasi\n\n\n\n\n\n3.8.6 Mentukan Outlier\n\nOutlier adalah data yang memiliki nilai ekstrim dibandingkan dengan data lainnya. Outlier dapat mempengaruhi hasil analisis data sehingga perlu ditangani dengan baik. Untuk mengetahui apakah suatu data merupakan outlier atau bukan, kita bisa menggunakan beberapa metode, diantaranya:\n\nTukey Fences (IQR)\n\nPlus: IQR adalah metode yang tahan terhadap nilai ekstrim dan lebih baik digunakan pada data dengan distribusi yang condong (skewed) karena mengandalkan kuartil (quartiles) yang tidak terpengaruh oleh ekstrem nilai.\nMinus: IQR tidak dapat digunakan pada data dengan distribusi normal karena mengandalkan kuartil (quartiles).\n\nZ-Score\n\nPlus: Z-score dapat digunakan pada data dengan distribusi normal karena mengandalkan mean dan standard deviasi.\nMinus: Z-score tidak tahan terhadap nilai ekstrim.\n\nMetode Hampiran Berbasis Probabilitas (Contoh: grubb’s test)\n\nPlus: Metode hampiran berbasis probabilitas seperti Grubbs’ Test dapat memberikan kepercayaan statistik dalam menentukan apakah sebuah data benar-benar outlier atau hanya perbedaan alami dalam distribusi data.\nMinus: Memerlukan asumsi bahwa data harus berdistribusi normal. Selain itu, metode ini hanya cocok untuk mengidentifikasi satu outlier dalam satu arah (outlier yang memiliki nilai ekstrim di atas atau di bawah rata-rata).\n\n\n\n\n3.8.7 Nilai Unik\n\nMethod unique() pada Pandas digunakan untuk mengetahui nilai unik dari suatu kolom. Method ini mengembalikan nilai unik dari suatu kolom dalam bentuk array. Method value_counts() pada Pandas digunakan untuk menghitung berapa kali suatu nilai muncul dalam suatu kolom. Method ini mengembalikan Series yang berisi frekuensi setiap nilai yang muncul dalam suatu kolom.\n\n# Penggunaan unique\ndf[\"nama_kolom\"].unique()\n\n# Penggunaan value_counts\ndf[\"nama_kolom\"].value_counts()\n\n\n\n3.8.8 Analisis dengan groupby\n\nMethod groupby() pada Pandas digunakan untuk melakukan grouping berdasarkan kolom tertentu. Method ini mengembalikan objek DataFrameGroupBy.\n\n# Penggunaan groupby dengan fungsi mean \ndf.groupby(\"kolom1\")[\"kolom2\"].mean()\n\n# menghitung statistik deskriptif dari setiap kelompok data\ndf.groupby(\"kolom1\")[\"kolom2\"].describe()\n\n# agregasi kustom dengan fungsi agg\ndf.groupby(\"kolom1\")[\"kolom2\"].agg([sum, min, max])\n\n#Menggunakan operasi agregasi kustom (menghitung rasio nilai maksimum dan minimum dalam setiap kelompok\ndf.groupby(\"kolom1\")[\"kolom2\"].agg(lambda x: x.max() / x.min())\n\n\n\n3.8.9 Korelasi\n\nKorelasi adalah salah satu metode statistik yang digunakan untuk mengetahui seberapa besar hubungan antara satu variabel dengan variabel lainnya. Korelasi memiliki nilai berkisar antara -1 hingga 1. Nilai 1 menunjukkan hubungan yang sempurna, nilai -1 menunjukkan hubungan yang sempurna pula dengan arah yang berlawanan, dan nilai 0 menunjukkan tidak ada hubungan antara kedua variabel tersebut. Kita dapat menggunakan methon corr() pada Pandas untuk menghitung korelasi dari setiap pasang kolom pada suatu DataFrame.\n\n# Menghitung korelasi\ndf.corr()\n\n# Terdapat 3 pilihan metode perhitungan korelasi, yaitu:\n# pearson (default), kendall, dan spearman\ndf.corr(method=\"kendall\")"
  },
  {
    "objectID": "modul_4.html#tugas-validasi-data",
    "href": "modul_4.html#tugas-validasi-data",
    "title": "5  Validasi Data",
    "section": "5.1 Tugas Validasi Data",
    "text": "5.1 Tugas Validasi Data\n\nPeriksa/Nilai Kualitas Data\nPeriksa/Nilai Tingkat Kecukupan Data\nPeriksa/Nilai Kesesuaian Data\nPeriksa/Nilai Konsistensi Data"
  },
  {
    "objectID": "modul_4.html#manfaat-validasi-data",
    "href": "modul_4.html#manfaat-validasi-data",
    "title": "5  Validasi Data",
    "section": "5.2 Manfaat Validasi Data",
    "text": "5.2 Manfaat Validasi Data\n\nTidak merusak perhitungan pada tahapan selanjutnya.\nMemvisualisasikan sebaran data, mendeteksi pola, dan mengidentifikasi anomali.\nMembantu menjelaskan dan mengkomunikasikan hasil analisis dengan lebih jelas."
  },
  {
    "objectID": "modul_4.html#laporan-dokumentasi-data-validasi",
    "href": "modul_4.html#laporan-dokumentasi-data-validasi",
    "title": "5  Validasi Data",
    "section": "5.3 Laporan Dokumentasi Data Validasi",
    "text": "5.3 Laporan Dokumentasi Data Validasi\nLaporan dokumentasi data validasi, setidaknya memiliki parameter berikut:\n\nKebenaran, misal di Indonesia isian Gender yang diakui hanya 2 P/W; Agama hanya 6 (Islam, Protestan, Katholik, Hindu, Budha, Konghucu)\nKelengkapan, misal data provinsi seluruh Indonesia (34 prov), namun hanya sebagian yg ada.\nKonsistensi, misal penulisan STM atau SMK;"
  },
  {
    "objectID": "modul_4.html#validasi-vs-verifikasi",
    "href": "modul_4.html#validasi-vs-verifikasi",
    "title": "5  Validasi Data",
    "section": "5.4 Validasi vs Verifikasi",
    "text": "5.4 Validasi vs Verifikasi\n\nValidasi: memastikan bahwa data yang diinputkan sesuai dengan ketentuan yang berlaku.\nVerifikasi: memastikan bahwa data yang diinputkan sesuai dengan data yang ada."
  },
  {
    "objectID": "modul_4.html#tahapan-kritikal-dalam-validasi",
    "href": "modul_4.html#tahapan-kritikal-dalam-validasi",
    "title": "5  Validasi Data",
    "section": "5.5 Tahapan kritikal dalam validasi",
    "text": "5.5 Tahapan kritikal dalam validasi\n\nTipe Data (integer, float, string)\nEkspresi Konsisten (mis. Jalan, Jl., Jln.)\nFormat Data (mis. utk tgl “YYYY-MM-DD” vs “DD-MM-YYYY.”)\nNilai Null/Missing Values\nMisspelling/Type\nInvalid Data (gender: L/P: L; Laki-laki; P: Pria/Perempuan? )"
  },
  {
    "objectID": "modul_4.html#teknik-validasi-data",
    "href": "modul_4.html#teknik-validasi-data",
    "title": "5  Validasi Data",
    "section": "5.6 Teknik Validasi Data",
    "text": "5.6 Teknik Validasi Data\n\nManual: melihat data secara langsung, misalnya melihat data di Excel.\nStatistik: menggunakan statistik deskriptif, misalnya melihat jumlah data, nilai maksimum, nilai minimum, dan lain-lain.\nVisualisasi: menggunakan visualisasi data, misalnya melihat sebaran data menggunakan histogram, boxplot, dan lain-lain."
  },
  {
    "objectID": "modul_4.html#validasi-dengan-pandas",
    "href": "modul_4.html#validasi-dengan-pandas",
    "title": "5  Validasi Data",
    "section": "5.7 Validasi Dengan Pandas",
    "text": "5.7 Validasi Dengan Pandas\nMethod info() dapat digunakan untuk melihat informasi data frame, seperti jumlah baris, kolom, nilai non-NULL, tipe data, dan total penggunaan memori. Method ini sangat berguna dalam melakukan validasi tahap awal pada data.\n\nimport pandas as pd\ndf = pd.read_csv('spreadsheet.csv')\n\n# Penggunaan info\ndf.info()"
  },
  {
    "objectID": "modul_4.html#visualisasi-data",
    "href": "modul_4.html#visualisasi-data",
    "title": "5  Validasi Data",
    "section": "5.8 Visualisasi Data",
    "text": "5.8 Visualisasi Data\n\nVisualisasi data dapat membantu dalam memvalidasi data.\nMemvisualisasikan sebaran data, mendeteksi pola, dan mengidentifikasi anomali.\nVisualisasi yang baik dapat menceritakan sebuah cerita tentang data Anda dengan cara yang tidak dapat dilakukan oleh sebuah kalimat.\n\n\n5.8.1 Jenis Visualisasi Data\n\nPerbandingan (Comparison)\n\nBar Chart\nLine Chart\nCombo Chart\n\nKomposisi (Composition)\n\nPie Chart\nStacked Bar Chart\nTreemap\nWaterfall Chart\n\nDistribusi (Distribution)\n\nHistogram\nBox Plot\nViolin Plot\n\nHubungan (Relationship)\n\nScatter Plot\nBubble Chart\nHeatmap\n\n\n\n\n5.8.2 Library Visualisasi Data\nTerdapat dua library populer untuk visualisasi data di Python, yaitu:\n\nMatplotlib: library yang paling populer untuk visualisasi data di Python. Umumnya diberi alias plt.\n\n\n\n\n\n\n\nSeaborn: library yang dibangun di atas Matplotlib, sehingga memiliki sintaks yang mirip. Umumnya diberi alias sns.\n\n\n\n\n\n\n\n\n5.8.3 Korelasi & Causation\n\nKorelasi merupakan suatu pengukuran sejauh mana nilai saling ketergantungan antar variabel.\nCausation merupakan hubungan antara sebab dan akibat antara dua variabel\nPenting untuk mengetahui perbedaan antara keduanya dan bahwa korelasi tidak mendeskripsikan sebab-akibat.\nMenentukan korelasi jauh lebih sederhana, menentukan sebab memerlukan analisis lebih lanjut\n\n\n\n5.8.4 Analisis Korelasi (Heatmap)\n\nKorelasi Pearson\n\nMengukur hubungan linier antara dua variabel\n\nKorelasi Spearman\n\nMengukur hubungan monotonik antara dua variabel\n\n\nVisualisasi korelasi dapat dilakukan dengan menggunakan heatmap.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(df.corr('pearson'), annot=True, cmap='YlGnBu', linewidths=0.2)\n\n\n\n\n\n\n\n\n5.8.5 Histogram\nHistogram dapat membantu dalam melihat persebaran data.\n\nsns.histplot(data = df, x= 'fc' ,color='orange', kde=False)\n\n\n\n\n\n\n\n\n5.8.6 Bar Chart\nBar chart digunakan untuk membandingkan data-data yang berbentuk kategorikal.\n\nsns.barplot(x='price_range',y='battery_power',data = df, order=[3,2,1,0], hue='dual_sim', hue_order=[1,0], errcolor='teal')\n\n\n\n\n\n\n\n\n5.8.7 Pie Chart\nPie chart membantu memvisualisasikan komposisi elemen-elemen dalam sebuah data.\n\nax = plt.gca()\nplt.pie(x=x, explode=[0, 0, 0, 0, 0.15, 0], labels=x)\nplt.legend(x, loc='upper right')\nplt.show()"
  },
  {
    "objectID": "modul_5.html#menentukan-sumber-data",
    "href": "modul_5.html#menentukan-sumber-data",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.1 Menentukan Sumber Data",
    "text": "5.1 Menentukan Sumber Data\nSumber data merupakan tempat atau lokasi sebuah data disimpan dan atau diakses untuk dapat dilakukan analisis serta penggunaan lainnya. Pemilihan sumber data harus mempertimbangkan sebuah kendala, akurasi, ketersediaan, serta relvansi data dengan tujuan analisis atau model dari machine learning yang akan digunakan atau di bangun."
  },
  {
    "objectID": "modul_5.html#menelaah-susunan-data",
    "href": "modul_5.html#menelaah-susunan-data",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.2 Menelaah Susunan Data",
    "text": "5.2 Menelaah Susunan Data\n\nMelakukan pemeriksaan struktur data untuk memahami bagaimana data diorganisir.\nMelihat dimensi data, jumlah atribut/kolom, dan jumlah sampel/baris.\nMenilai apakah data terstruktur (misalnya, data tabel) atau tidak terstruktur (misalnya, data teks atau gambar)."
  },
  {
    "objectID": "modul_5.html#menentukan-tipe-dan-model-data-yang-dimiliki",
    "href": "modul_5.html#menentukan-tipe-dan-model-data-yang-dimiliki",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.3 Menentukan Tipe dan Model Data yang dimiliki",
    "text": "5.3 Menentukan Tipe dan Model Data yang dimiliki\n\nIdentifikasi tipe data untuk setiap atribut (numerik, kategorikal, teks, tanggal, dll.).\nPenentuan model data yang sesuai untuk analisis atau model machine learning berdasarkan tipe data.\nPemahaman tentang apakah data bersifat kontinu, diskret, ordinal, atau nominal."
  },
  {
    "objectID": "modul_5.html#mengambil-data",
    "href": "modul_5.html#mengambil-data",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.4 Mengambil Data",
    "text": "5.4 Mengambil Data\n\nProses pengambilan data dari sumber data ke lingkungan analisis atau pengembangan model machine learning.\nMenggunakan berbagai metode seperti mengimpor file data (misalnya CSV, Excel), mengakses database, atau menggunakan API untuk mengambil data dari sumber online."
  },
  {
    "objectID": "modul_5.html#menelaah-data-dalam-machine-learning",
    "href": "modul_5.html#menelaah-data-dalam-machine-learning",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.5 Menelaah Data dalam Machine Learning",
    "text": "5.5 Menelaah Data dalam Machine Learning\n\nMelakukan eksplorasi data (data exploration) untuk memahami karakteristik data secara lebih mendalam.\nVisualisasi data dengan grafik atau plot untuk memahami pola, distribusi, dan korelasi antara atribut.\nIdentifikasi missing value, outlier, dan data yang tidak konsisten untuk diatasi sebelum analisis atau pemodelan."
  },
  {
    "objectID": "modul_5.html#contoh-source-code",
    "href": "modul_5.html#contoh-source-code",
    "title": "5  Menentukan Objek atau Memilih Data",
    "section": "5.6 Contoh Source Code",
    "text": "5.6 Contoh Source Code\nBerikut adalah contoh sederhana menggunakan Python untuk mengambil data dari file CSV, menelaah susunan data, menentukan tipe data, dan melakukan eksplorasi data menggunakan pandas dan matplotlib.\nSilahkan Unduh dataset berikut Iris_Dataset. Pastikan dataset disimpan dalam file CSV dengan nama “iris_dataset.csv” dalam folder yang sama dengan script Python.\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Mengambil data dari file CSV\ndata_path = 'iris_dataset.csv'\ndf = pd.read_csv(data_path)\n\n# Menelaah susunan data\nprint(\"Dimensi data:\", df.shape)\nprint(\"Info data:\")\nprint(df.info())\nprint(\"Sepuluh data pertama:\")\nprint(df.head(10))\n\n# Menentukan tipe data dan model data yang dimiliki\nprint(\"Tipe data untuk setiap atribut:\")\nprint(df.dtypes)\n\n# Preprocessing data (jika diperlukan)\n# Tidak ada preprocessing yang diperlukan dalam contoh ini\n\n# Mengambil statistik deskriptif untuk data numerik\nprint(\"Statistik deskriptif:\")\nprint(df.describe())\n\n# Eksplorasi data dengan visualisasi\nplt.figure(figsize=(10, 6))\nplt.scatter(df['sepal_length'], df['sepal_width'])\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('Scatter Plot: Sepal Length vs. Sepal Width')\nplt.show()\n\nplt.figure(figsize=(8, 6))\ndf['species'].value_counts().plot(kind='bar')\nplt.xlabel('Species')\nplt.ylabel('Count')\nplt.title('Bar Plot: Species Distribution')\nplt.show()\nPastikan bahwa dataset “Iris” (iris_dataset.csv) berisi kolom dengan nama “sepal_length”, “sepal_width”, “petal_length”, “petal_width”, dan “species”. Jika dataset tersebut memiliki atribut lain atau format yang berbeda, Anda perlu menyesuaikan kode di atas sesuai dengan dataset yang digunakan.\nContoh di atas akan membaca dataset Iris, menampilkan informasi dasar tentang dataset, menampilkan sepuluh baris pertama dari data, menentukan tipe data untuk setiap atribut, menampilkan statistik deskriptif untuk data numerik, dan melakukan eksplorasi data dengan visualisasi menggunakan scatter plot untuk melihat hubungan antara panjang dan lebar kelopak bunga serta bar plot untuk melihat distribusi spesies bunga.\nHarap dicatat bahwa contoh di atas hanya merupakan contoh sederhana, dan dalam penerapan sebenarnya, langkah-langkah analisis dan eksplorasi data bisa lebih canggih dan lengkap sesuai dengan kompleksitas dan karakteristik data.\nPemahaman mendalam tentang sumber data, susunan data, tipe data, dan karakteristik data adalah langkah kritis sebelum membangun model machine learning yang efektif."
  },
  {
    "objectID": "modul_6.html#data-cleaning",
    "href": "modul_6.html#data-cleaning",
    "title": "7  Cleaning Data",
    "section": "7.1 Data Cleaning",
    "text": "7.1 Data Cleaning\nPada proses machine learning, terdapat tahapan preprocessing. Preprocessing merupakan tahapan yang penting dalam machine learning. Hasil dari preprocessing dapat mempengaruhi nilai akurasi dari sebuah model. Tujuan dari Preprocessing adalah untuk memastikan data siap untuk diproses dan atau digunakan pada machine learning. Dalam melakukan preprocessing memiliki beberapa tantangan, diantara adalah missing value, outlier, format tidak konsisten, dan malformed record"
  },
  {
    "objectID": "modul_6.html#cara-mengatasi-missing-value",
    "href": "modul_6.html#cara-mengatasi-missing-value",
    "title": "7  Cleaning Data",
    "section": "7.2 Cara Mengatasi Missing Value",
    "text": "7.2 Cara Mengatasi Missing Value\n\nMising Value merupakan data yang kosong atau tidak lengkap.\nTantangan dalam mengatasi missing value adalah bagaimana mengisi nilai kosong tersebut.\nStrategi yang dapat dilakukan diantara lain :\n\nMenghapus baris atau kolom yang memiliki missing value\nMengisi nilai kosong dengan nilai rata-rata atau median\nMengisi nilai kosong dengan nilai yang sering muncul\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis\n\n- Menghapus baris atau kolom yang memiliki missing value\n\n- Tanpa Menggunakan Dataset\nimport pandas as pd\n\n# Contoh dataset dengan nilai yang hilang\ndata = {\n    'A': [1, 2, None, 4],\n    'B': [5, None, 7, 8],\n    'C': [9, 10, 11, None]\n}\ndf = pd.DataFrame(data)\n\n# Tampilkan dataset sebelum penghapusan\nprint(\"Dataset sebelum penghapusan:\")\nprint(df)\n\n# Hapus baris yang memiliki nilai yang hilang\ndf_cleaned_rows = df.dropna()\nprint(\"\\nDataset setelah menghapus baris yang memiliki nilai yang hilang:\")\nprint(df_cleaned_rows)\n\n# Hapus kolom yang memiliki nilai yang hilang\ndf_cleaned_cols = df.dropna(axis=1)\nprint(\"\\nDataset setelah menghapus kolom yang memiliki nilai yang hilang:\")\nprint(df_cleaned_cols)\n\n\n- Menggunakan Dataset\nuntuk mengunduh dataset agar dapat digunakan pada source code disini\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom\nprint(\"Informasi tentang dataset Titanic:\")\nprint(titanic_df.info())\n\n# Hapus baris yang memiliki nilai yang hilang\ntitanic_cleaned_rows = titanic_df.dropna()\nprint(\"\\nDataset Titanic setelah menghapus baris yang memiliki nilai yang hilang:\")\nprint(titanic_cleaned_rows)\n\n# Hapus kolom yang memiliki nilai yang hilang\ntitanic_cleaned_cols = titanic_df.dropna(axis=1)\nprint(\"\\nDataset Titanic setelah menghapus kolom yang memiliki nilai yang hilang:\")\nprint(titanic_cleaned_cols)\n\n\n\n- Mengisi nilai kosong dengan nilai rata-rata atau median\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom\nprint(\"Informasi tentang dataset Titanic sebelum pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n# Mengisi nilai kosong pada kolom 'Age' dengan nilai median dari kolom tersebut\nage_median = titanic_df['Age'].median()\ntitanic_df['Age'].fillna(age_median, inplace=True)\n\n# Mengisi nilai kosong pada kolom 'Fare' dengan nilai rata-rata dari kolom tersebut\nfare_mean = titanic_df['Fare'].mean()\ntitanic_df['Fare'].fillna(fare_mean, inplace=True)\n\n# Tampilkan informasi tentang dataset setelah pengisian nilai kosong\nprint(\"\\nInformasi tentang dataset Titanic setelah pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n\n- Mengisi nilai kosong dengan nilai yang sering muncul\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom sebelum pengisian\nprint(\"Informasi tentang dataset Titanic sebelum pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n# Mengisi nilai kosong pada kolom 'Embarked' dengan nilai yang sering muncul (mode) dari kolom tersebut\nembarked_mode = titanic_df['Embarked'].mode()[0]\ntitanic_df['Embarked'].fillna(embarked_mode, inplace=True)\n\n# Tampilkan informasi tentang dataset setelah pengisian nilai kosong\nprint(\"\\nInformasi tentang dataset Titanic setelah pengisian nilai kosong:\")\nprint(titanic_df.info())"
  },
  {
    "objectID": "modul_6.html#menangani-outlier",
    "href": "modul_6.html#menangani-outlier",
    "title": "7  Cleaning Data",
    "section": "7.3 Menangani Outlier",
    "text": "7.3 Menangani Outlier\nOutlier merupakan data yang jauh dari nilai rata-rata atau nilai normal. Outlier dapat mempengaruhi hasil analisis dan machine learning.\n\nStrategi untuk mengatasi Outlier :\n\nMenghapus outlier.\nMenggantikan nilai outlier dengan nilai lain seperti nilai rata-rata atau median.\nMenggunakan teknik scaling atau normalisasi.\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis.\n\n- Menghapus Outlier\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom scipy import stats\n\n# Load dataset Iris dari scikit-learn\ndata = load_iris()\niris_df = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Tampilkan informasi tentang dataset Iris sebelum penghapusan outlier\nprint(\"Informasi tentang dataset Iris sebelum penghapusan outlier:\")\nprint(iris_df.describe())\n\n# Definisikan fungsi untuk menghapus outlier berdasarkan z-score\ndef remove_outliers_zscore(df, z_threshold=3):\n    z_scores = stats.zscore(df)\n    return df[(z_scores &lt; z_threshold).all(axis=1)]\n\n# Hapus outlier dari dataset Iris berdasarkan z-score\niris_cleaned = remove_outliers_zscore(iris_df)\n\n# Tampilkan informasi tentang dataset Iris setelah penghapusan outlier\nprint(\"\\nInformasi tentang dataset Iris setelah penghapusan outlier:\")\nprint(iris_cleaned.describe())\nPada contoh di atas, digunakan metode z-score untuk mendeteksi outlier dalam dataset Iris. Outlier adalah data yang memiliki z-score lebih besar dari z_threshold yang telah ditentukan (z_threshold=3). Fungsi remove_outliers_zscore digunakan untuk menghapus baris yang mengandung outlier berdasarkan z-score, yaitu baris yang memiliki setidaknya satu fitur (kolom) dengan z-score melebihi z_threshold. Penting untuk berhati-hati karena penghapusan outlier dapat mempengaruhi hasil analisis atau model machine learning. Selain z-score, ada banyak teknik deteksi outlier lainnya seperti IQR dan pendekatan berbasis model machine learning atau domain knowledge.\n\n\n- Menggantikan nilai outlier dengan nilai lain seperti nilai rata-rata atau median.\nimport pandas as pd\n\n# membaca dataset publik\ndata = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n\n# menentukan batas outlier\nq1 = data[0].quantile(0.25)\nq3 = data[0].quantile(0.75)\niqr = q3 - q1\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n\n# menggantikan outlier dengan nilai rata-rata\ndata[0] = data[0].apply(lambda x: data[0].mean() if x &lt; lower_bound or x &gt; upper_bound else x)\n\n# menggantikan outlier dengan nilai median\ndata[0] = data[0].apply(lambda x: data[0].median() if x &lt; lower_bound or x &gt; upper_bound else x)\n\n# menampilkan dataset yang telah diubah\nprint(data)\nPada contoh kode di atas, dataset publik yang digunakan adalah Iris Dataset yang tersedia di UCI Machine Learning Repository. Pertama-tama, kita menentukan batas outlier dengan menghitung kuartil pertama (Q1), kuartil ketiga (Q3), dan rentang antar kuartil (IQR). Kemudian, kita menentukan batas bawah dan batas atas untuk menentukan nilai outlier.\nKemudian, kita menggantikan nilai outlier dengan nilai rata-rata atau median. Untuk menggantikan dengan nilai rata-rata, kita menggunakan fungsi apply() pada kolom yang menghasilkan nilai rata-rata jika nilai kurang dari batas bawah atau lebih dari batas atas, sedangkan untuk menggantikan dengan nilai median, kita juga menggunakan fungsi apply() pada kolom yang menghasilkan nilai median jika nilai kurang dari batas bawah atau lebih dari batas atas.\nTerakhir, kita menampilkan dataset yang telah diubah dengan fungsi print().\n\n\n- Mengisi nilai kosong dengan nilai yang sering muncul\nimport pandas as pd\n\n# Load dataset Titanic dari file CSV\ntitanic_df = pd.read_csv(\"titanic.csv\")\n\n# Tampilkan informasi mengenai dataset, termasuk jumlah nilai yang hilang di setiap kolom sebelum pengisian\nprint(\"Informasi tentang dataset Titanic sebelum pengisian nilai kosong:\")\nprint(titanic_df.info())\n\n# Mengisi nilai kosong pada kolom 'Embarked' dengan nilai yang sering muncul (mode) dari kolom tersebut\nembarked_mode = titanic_df['Embarked'].mode()[0]\ntitanic_df['Embarked'].fillna(embarked_mode, inplace=True)\n\n# Tampilkan informasi tentang dataset setelah pengisian nilai kosong\nprint(\"\\nInformasi tentang dataset Titanic setelah pengisian nilai kosong:\")\nprint(titanic_df.info())\nPada contoh di atas, menggunakan metode mode() dari pandas untuk menghitung nilai yang sering muncul (mode) pada kolom ‘Embarked’, lalu mengisi nilai kosong pada kolom tersebut dengan nilai mode yang dihitung. Penggunaan parameter inplace=True memastikan perubahan dilakukan langsung pada DataFrame asli. Setelah mengisi nilai kosong, hasilnya dapat diperiksa untuk memastikan tidak ada lagi nilai yang hilang pada kolom ‘Embarked’.\nPerlu diingat, pengisian nilai kosong dengan nilai yang sering muncul merupakan salah satu teknik imputasi yang umum. Teknik lainnya termasuk pengisian dengan nilai rata-rata, nilai median, atau menggunakan model machine learning untuk memprediksi nilai yang hilang berdasarkan data lainnya. Pilihan teknik imputasi tergantung pada karakteristik dataset dan tujuan analisis atau model machine learning yang ingin diimplementasikan."
  },
  {
    "objectID": "modul_6.html#menangani-format-yang-tidak-konsisten",
    "href": "modul_6.html#menangani-format-yang-tidak-konsisten",
    "title": "7  Cleaning Data",
    "section": "7.4 - Menangani Format yang Tidak Konsisten",
    "text": "7.4 - Menangani Format yang Tidak Konsisten\nFormat Tidak Konsisten terjadi ketika data memiliki format yang sama atau tidak sesuai dengan format yang diharapkan. Sering terjadi pada pemformatan tanggal, bulan, dan tahun.\n\nStrategi dalam mengatasi Format Tidak Konsisten\n\nMengubah format data menjadi format yang konsisten seperti mengubah format tanggal menjadi format yang sama\nMemperbaiki data yang salah ketik atau typo\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis\n\n- Mengubah format data menjadi format yang konsisten seperti mengubah format tanggal menjadi format yang sama\nimport pandas as pd\n\n# Contoh dataset dengan kolom tanggal dalam format yang berbeda\ndata = {\n    'Tanggal': ['2021-08-01', '02-08-2021', '2021/08/03', '20210804']\n}\ndf = pd.DataFrame(data)\n\n# Tampilkan dataset sebelum perubahan format\nprint(\"Dataset sebelum perubahan format:\")\nprint(df)\n\n# Ubah format tanggal menjadi format yang sama (YYYY-MM-DD)\ndf['Tanggal'] = pd.to_datetime(df['Tanggal'], errors='coerce').dt.strftime('%Y-%m-%d')\n\n# Tampilkan dataset setelah perubahan format\nprint(\"\\nDataset setelah perubahan format:\")\nprint(df)\n\n\n- Memperbaiki data yang salah ketik atau typo\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load dataset Iris dari scikit-learn\ndata = load_iris()\niris_df = pd.DataFrame(data.data, columns=data.feature_names)\niris_df['species'] = data.target_names[data.target]\n\n# Contoh data dengan salah ketik atau typo\niris_df.iloc[0, 0] = 5.1\niris_df.iloc[1, 1] = 3.6\n\n# Tampilkan dataset sebelum pembersihan data\nprint(\"Dataset sebelum pembersihan data:\")\nprint(iris_df)\n\n# Koreksi data salah ketik atau typo\n# Misalnya, jika nilai 3.6 pada kolom 'sepal width (cm)' seharusnya 3.0\niris_df.loc[iris_df['sepal width (cm)'] == 3.6, 'sepal width (cm)'] = 3.0\n\n# Tampilkan dataset setelah pembersihan data\nprint(\"\\nDataset setelah pembersihan data:\")\nprint(iris_df)"
  },
  {
    "objectID": "modul_6.html#menangani-malformed-record",
    "href": "modul_6.html#menangani-malformed-record",
    "title": "7  Cleaning Data",
    "section": "7.5 Menangani Malformed Record",
    "text": "7.5 Menangani Malformed Record\nMalformed Record terjadi saat ketika data tidak memenuhi format atau struktur yang diharapkan.\n\nStrategi yang dapat dilakukan diantara lain :\n\nMenghapus record yang tidak sesuai dengan format atau struktur yang diharapkan\nMengubah record yang tidak sesuai dengan format atau struktur yang diharapkan\n\n\nPilih strategi yang paling sesuai tergantung pada tipe data dan tujuan analisis.\n\n- Menghapus record yang tidak sesuai dengan format atau struktur yang diharapkan\nimport pandas as pd\n\n# membaca dataset\ndata = pd.read_csv(\"nama_file.csv\")\n\n# mengecek dan menghapus record yang tidak sesuai dengan format atau struktur yang diharapkan\nfor i, row in data.iterrows():\n    if not format_check(row):\n        data.drop(i, inplace=True)\n\n# menampilkan dataset yang telah diubah\nprint(data)\nKode di atas menggunakan library pandas untuk membaca dataset dari file csv dan melakukan pengecekan format atau struktur pada setiap record dalam dataset dengan fungsi format_check(). Jika record tidak sesuai dengan format atau struktur yang diharapkan, maka record tersebut dihapus dari dataset menggunakan fungsi drop(). Fungsi iterrows() digunakan untuk mengiterasi setiap record dalam dataset. Setelah proses penghapusan selesai, dataset yang telah diubah ditampilkan menggunakan fungsi print(). Penting untuk menyesuaikan kode dengan format atau struktur dataset yang digunakan dan memastikan menggunakan fungsi format_check() yang sesuai.\n\n\n- Mengubah record yang tidak sesuai dengan format atau struktur yang diharapkan\n#| code-fold: true\nimport pandas as pd\n\n# membaca dataset dari file csv\ndata = pd.read_csv('nama_file.csv')\n\n# fungsi untuk melakukan pengecekan format atau struktur pada setiap record dalam dataset\ndef format_check(record):\n    # implementasi pengecekan format atau struktur pada satu record\n    # return True jika format atau struktur sesuai, False jika tidak sesuai\n\n# melakukan iterasi pada setiap record dalam dataset\nfor index, row in data.iterrows():\n    # cek apakah format atau struktur record sesuai dengan yang diharapkan\n    if not format_check(row):\n        # jika tidak sesuai, hapus record dari dataset\n        data = data.drop(index)\n\n# menampilkan dataset yang telah diubah\nprint(data)\nPada contoh di atas, dataset dibaca dari file csv menggunakan fungsi read_csv() dari library pandas. Kemudian, dilakukan iterasi pada setiap record dalam dataset menggunakan fungsi iterrows(). Pada setiap iterasi, record dicek dengan fungsi format_check() untuk memastikan bahwa format atau struktur record sesuai dengan yang diharapkan. Jika tidak sesuai, record tersebut dihapus dari dataset menggunakan fungsi drop(). Setelah proses penghapusan selesai, dataset yang telah diubah ditampilkan menggunakan fungsi print(). Harap diingat bahwa fungsi format_check() harus disesuaikan dengan format atau struktur yang diharapkan pada dataset yang digunakan."
  },
  {
    "objectID": "modul_6.html#kesimpulan",
    "href": "modul_6.html#kesimpulan",
    "title": "7  Cleaning Data",
    "section": "7.6 Kesimpulan",
    "text": "7.6 Kesimpulan\nPreprocessing merupakan tahapan penting dalam proses machine learning. Tantangan seperti contoh diatas dapat diatasi dengan berbagai strategi, dengan memilih strategi yang tepat dan pemahaman tipe atau jenis data yang ada akan makin memudahkan dalam melakukan preprocessing."
  },
  {
    "objectID": "modul_7.html#rekayasafitur",
    "href": "modul_7.html#rekayasafitur",
    "title": "8  Transformasi Data",
    "section": "8.1 RekayasaFitur",
    "text": "8.1 RekayasaFitur\nRekayasa fitur adalah proses penambahan atau modifikasi fitur dengan mengaplikasikan penghitungan matematik, statistika, atau pengetahuan terhadap fitur.\nSebagai contoh, anda dapat membuat fitur baru bernama average atau rata rata yang mengambil nilai dari fitur-fitur lain. Atau anda dapat membuat fitur kategori baru dengan mengolah data dari fitur-fitur lain.\nDiharapkan fitur-fitur yang di modifikasi atau ditambah dapat menambah kualitas dataset sehingga dapat menghasilkan model yang lebih akurat dan efisien.\n\n8.1.1 Hands On Coding\nKita akan melakukan proses feature enginnering ke sebuah dataset dibawah ini\nJangan lupa untuk menginstall library pandas menggunakan command pip install pandas\n\nfrom spacy import displacy\nimport spacy\nfrom matplotlib import pyplot as plt\nimport cv2\nfrom sklearn.preprocessing import MinMaxScaler\nimport time\nimport random\nimport plotly.express as px\nfrom sklearn.impute import KNNImputer\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom scipy.stats import pearsonr\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.DataFrame({'Country': ['Afghanistan', 'Cameroon', 'Indonesia', 'Guatemala'], 'UrbanPopulation': [10142913, 15248270, 153983073,\n                  8738685], 'RuralPopulation': [28829316, 11242817, 117874900, 8119648], 'SlumPopulation': [7434756, 4981883, 29889391, 3285745]})\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nUrbanPopulation\nRuralPopulation\nSlumPopulation\n\n\n\n\n0\nAfghanistan\n10142913\n28829316\n7434756\n\n\n1\nCameroon\n15248270\n11242817\n4981883\n\n\n2\nIndonesia\n153983073\n117874900\n29889391\n\n\n3\nGuatemala\n8738685\n8119648\n3285745\n\n\n\n\n\n\n\nDataset ini mempunyai 4 fitur:\n- Country, Nama negara\n- UrbanPopulation, populasi manusia yang hidup di daerah urban (perumahan kota)\n- RuralPopulation, populasi manusia yang hidup di daerah rural (pinggiran kota)\n- SlumPopulation, populasi manusia yang hidup di daerah slum (pemukiman kumuh)\nPertama kita akan menghitung presentase jumlah populasi yang hidup di slum (pemukiman kumuh) dari populasi yang hidup di urban (perumahan kota). Nilai ini didapat menggunakan rumus:\n\\[\nSlumPrecentage = \\dfrac{SlumPopulation}{UrbanPopulation} * 100\n\\]\n\ndf['SlumPopulation'] = round(\n    (df['SlumPopulation']/(df['UrbanPopulation']))*100, 2)\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nUrbanPopulation\nRuralPopulation\nSlumPopulation\n\n\n\n\n0\nAfghanistan\n10142913\n28829316\n73.30\n\n\n1\nCameroon\n15248270\n11242817\n32.67\n\n\n2\nIndonesia\n153983073\n117874900\n19.41\n\n\n3\nGuatemala\n8738685\n8119648\n37.60\n\n\n\n\n\n\n\nSelanjutnya, kita menggabungkan dua fitur (urban dan rural) menjadi satu, dan mengubah nilainya dari jumlah penduduk ke presentase penduduk. Rumusnya cukup simple:\n\\[\nUrbanPrecentage = \\dfrac{UrbanPopulation}{UrbanPopulation+RuralPopulation} * 100\n\\]\n\ndf['UrbanPopulation'] = round(\n    (df['UrbanPopulation']/(df['UrbanPopulation']+df['RuralPopulation']))*100, 2)\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nUrbanPopulation\nRuralPopulation\nSlumPopulation\n\n\n\n\n0\nAfghanistan\n26.03\n28829316\n73.30\n\n\n1\nCameroon\n57.56\n11242817\n32.67\n\n\n2\nIndonesia\n56.64\n117874900\n19.41\n\n\n3\nGuatemala\n51.84\n8119648\n37.60\n\n\n\n\n\n\n\nDan untuk sentuhan akhir, kita akan hapus kolom SlumPopulation karena nilainya sudah ter-representasikan di kolom UrbanPopulation\n\ndf = df.drop(columns=['RuralPopulation'])\ndf.head()\n\n\n\n\n\n\n\n\nCountry\nUrbanPopulation\nSlumPopulation\n\n\n\n\n0\nAfghanistan\n26.03\n73.30\n\n\n1\nCameroon\n57.56\n32.67\n\n\n2\nIndonesia\n56.64\n19.41\n\n\n3\nGuatemala\n51.84\n37.60\n\n\n\n\n\n\n\nHasilnya, nilai atau value di dataset lebih mudah dibaca, dan dapat direpresentasikan menggunakan fitur yang lebih sedikit. Dataset sudah siap untuk diproses lebih lanjut."
  },
  {
    "objectID": "modul_7.html#imputasi",
    "href": "modul_7.html#imputasi",
    "title": "8  Transformasi Data",
    "section": "8.2 Imputasi",
    "text": "8.2 Imputasi\nImputasi adalah proses penggantian nilai data yang hilang dengan data yang baru. Seperti contoh rekayasa fitur sebelumnya, nilai NaN termasuk data yang perlu kita olah.\nDalam bab ini kita akan mempelajari beberapa hal terkait imputasi antara lain: - Jenis-jenis imputasi\n- Teknik imputasi\n\n8.2.1 Jenis-jenis Imputasi\nJenis data yang hilang dikelompokkan menjadi 3, yaitu - Missing completely at random (MCAR) - Missing at random (MAR) - Missing not at random (MNAR)\n\n\n\nGambar1. Jenis-jenis Imputasi\n\n\n\n8.2.1.1 Missing Completely At Random (MCAR)\nJika probabilitas hilangnya data dalam suatu fitur sama antara satu data dengan yang lain. Asumsi ini dapat diuji dengan memisahkan data yang hilang dan yang lengkap serta memeriksa karakteristik data. Jika karakteristik data tidak sama untuk kedua fitur, asumsi MCAR tidak berlaku\n\n\n\nGambar2. Contoh MCAR\n\n\n\n\n8.2.1.2 Missing At Random (MAR)\nKemungkinan data yang hilang dipengaruhi oleh variabel lain, namun tidak dipengaruhi oleh variabel yang hilang. Sebagai contoh, untuk data di samping, hanya peserta dengan umur yang dibawah 31 yang nilainya hilang. Berarti fitur age mempengaruhi probabilitas missing data IQ score.\n\n\n\nGambar3. Contoh MAR\n\n\n\n\n\n8.2.2 Missing Not At Random\nKemungkinan data yang hilang tidak dipengaruhi oleh fitur lain, namun dipengaruhi oleh fitur pada data yang hilang. Sebagai contoh, untuk data di samping, ada kemungkinan bahwa data IQ score yang hilang hanya data yang nilainya dibawah 110. Sedangkan variabel age tidak berpengaruh atas hilangnya data IQ score karena age yang kecil dan besar sama sama mempunyai data yang hilang\n\n\n\nGambar4. Contoh MNAR\n\n\n\n\n8.2.3 Teknik-Teknik Imputasi\nPerlu diingat bahwa jika 70% data hilang/missing, maka semua fitur(kolom) dan data (row) harus dihapus.\n\n\n\nGambar5. Flow Proses Imputasi\n\n\nData yang hilang harus dimutasikan berdasarkan jenis datanya, yaitu :\nNumerik\n- Mean/Median\n- Arbitrary\n- End of tail\n- Regresi\n- KNN\nKategorik\n- Frequent/Modus\n- KNN\n\n8.2.3.1 Mean\nDataset imputasi mean adalah metode untuk mengisi nilai yang hilang dalam dataset dengan menggunakan nilai rata-rata (mean) dari variabel yang bersangkutan. Ketika ada nilai yang hilang dalam suatu variabel dalam dataset, imputasi mean menggantikan nilai-nilai yang hilang tersebut dengan nilai rata-rata dari seluruh nilai yang ada dalam variabel tersebut.\nKelebihan\n- Mudah dan cepat.\n- Bekerja efektif untuk dataset numerik berukuran kecil.\n- Cocok untuk variabel numerik.\n- Cocok untuk data missing completely at random (MCAR).\n- Dapat digunakan dalam produksi (mis. dalam model deployment)\nKekurangan\n- Tidak memperhitungkan korelasi antar fitur, berfungsi pada tingkat kolom.\n- Kurang akurat.\n- Tidak memperhitungkan probabilitas/ketidakpastian.\n- Tidak cocok utk &gt;5% missing data.\n\n8.2.3.1.1 Hands On Coding\nPertama kita akan buat sebuah dataset menggunakan pandas dataframe\n\n# buat dataset dengan format dataframe\ndf = pd.DataFrame({'age': [25, 26, 29, 30, 30, 31, 44, 46],\n                   'IQ': [np.NaN, 121, 91, np.NaN, 110, np.NaN, 118, 93]})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nNaN\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\nNaN\n\n\n4\n30\n110.0\n\n\n5\n31\nNaN\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\nLalu kita akan hitung mean atau rata rata dari dataset\n\nmean = df['IQ'].mean()\nprint(f'Mean: {mean}, dibulatkan menjadi {round(mean)}')\ndisplay(df)\n\nMean: 106.6, dibulatkan menjadi 107\n\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nNaN\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\nNaN\n\n\n4\n30\n110.0\n\n\n5\n31\nNaN\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\nLalu kita masukkan nilai mean tersebut ke data yang kosong\n\n# masukkan nilai mean ke missing value\ndf['IQ'] = df['IQ'].fillna(round(mean))\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n107.0\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\n107.0\n\n\n4\n30\n110.0\n\n\n5\n31\n107.0\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\n\n\n\n8.2.3.2 Arbiter\nTeknik imputasi arbiter (arbiter imputation) adalah metode untuk mengisi nilai yang hilang dalam dataset dengan menggunakan hasil gabungan dari beberapa metode imputasi yang berbeda. Dalam metode ini, beberapa teknik imputasi yang berbeda diterapkan pada dataset yang sama, dan hasil dari setiap teknik imputasi digabungkan menjadi satu nilai yang digunakan untuk mengisi nilai yang hilang.\nKelebihan - Sangat mudah dan cepat\n- Cocok untuk missing dataset dengan asumsi tidak missing at random\nKekurangan - Mengganggu variansi dan distribusi variable original\n- Dapat membentuk outlier\n- Semakin besar nilai arbitrary, maka semakin besar distorsi\n\n8.2.3.2.1 Hands On Coding\nKita akan gunakan dataset yang sama seperti sebelumnya\n\n# buat dataset dengan format dataframe\ndf = pd.DataFrame({'age': [25, 26, 29, 30, 30, 31, 44, 46],\n                   'IQ': [np.NaN, 121, 91, np.NaN, 110, np.NaN, 118, 93]})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nNaN\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\nNaN\n\n\n4\n30\n110.0\n\n\n5\n31\nNaN\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\nCukup masukkan nilai arbiter ke data yang kosong. Dalam koding ini, nilai arbiter adalah 130.\n\ndf['IQ'] = df['IQ'].fillna(130)\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n130.0\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\n130.0\n\n\n4\n30\n110.0\n\n\n5\n31\n130.0\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\n\n\n\n8.2.3.3 End Of Tail\nTeknik imputasi end of tail adalah metode untuk mengisi nilai yang hilang dalam dataset dengan menggunakan nilai ekstrem (tail) dari distribusi data yang ada. Metode ini didasarkan pada asumsi bahwa nilai yang hilang cenderung berada di ekor distribusi data. Namun untuk teknik ini kita harus mengikuti sebuah ketentuan khusus, yaitu:\nJika distribusi data bersifat normal, maka gunakan rumus :\n\\[\n{\\sigma = \\sqrt{\\dfrac{\\Sigma |x-\\mu|^2}{N}}}\n\\]\nJika distribusi data bersifat skewed, maka gunakan rumus IQR proximity\n\\[\n{IQR = Q_{3} - Q_{1}}\n\\]\n\n8.2.3.3.1 Distribusi Normal\nData yang terdistribusi secara normal, juga dikenal sebagai distribusi Gaussian atau kurva lonceng, mengacu pada distribusi statistik di mana titik-titik data secara simetris terdistribusi di sekitar nilai rata-rata, menciptakan kurva berbentuk lonceng yang khas.\nUntuk menghitung data yg mempunyai distribusi normal, kita harus mengetahui standar deviasinya. Berikut rumus untuk menghitung standar deviasi :\n\\[\n{\\mu + 3 * \\sigma}\n\\] Keterangan:\nσ = Standar Deviasi\nΣ = jumlah\nx = data yang dihitung\nμ = Mean/rata rata\nN = Jumlah data\n\n\n8.2.3.3.2 Distribusi Normal - Hands On Coding\nPertama kita akan buat sebuah dataset yang cukup besar jika dibandingkan dengan dataset yang diatas.\nJangan lupa untuk menginstall library yang dibutuhkan :\n- pip install numpy\n- pip install pandas\n- pip install matplotlib\n- pip install keras - pip install tensorflow\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\ndata = [85, 90, 95, 95, 100, np.nan, 100, 110, 105, 105, 110,\n        np.nan, 110, 110, 115, 115, 115, 120, np.nan, 125, 130]\n\n# make a dataframe\ndf = pd.DataFrame({'age': age, 'IQ': data})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n85.0\n\n\n1\n26\n90.0\n\n\n2\n29\n95.0\n\n\n3\n30\n95.0\n\n\n4\n30\n100.0\n\n\n5\n31\nNaN\n\n\n6\n44\n100.0\n\n\n7\n46\n110.0\n\n\n8\n22\n105.0\n\n\n9\n33\n105.0\n\n\n10\n35\n110.0\n\n\n11\n27\nNaN\n\n\n12\n21\n110.0\n\n\n13\n23\n110.0\n\n\n14\n45\n115.0\n\n\n15\n47\n115.0\n\n\n16\n41\n115.0\n\n\n17\n38\n120.0\n\n\n18\n37\nNaN\n\n\n19\n21\n125.0\n\n\n20\n24\n130.0\n\n\n\n\n\n\n\nSelanjutnya kita akan mengecek jenis distribusi data menggunakan library matplotlib\n\ndata = [85, 90, 95, 95, 100, np.nan, 100, 110, 105, 105, 110,\n        np.nan, 110, 110, 115, 115, 115, 120, np.nan, 125, 130]\n\nplt.hist(data, bins=5)\nplt.xlabel('Data')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\nDapat dilihat bahwa grafik histogram membentuk seperti gunung atau lonceng, dengan puncak tepat di tengah-tengah grafik. Inilah salah satu karakteristik dataset dengan distribusi normal.\nSelanjutnya, kita akan mencari mean dari dataset\n\n# nilai kosong dihilangkan dari data terlebih dahulu\ndata = [85, 90, 95, 95, 100, 100, 110, 105, 105,\n        110, 110, 110, 115, 115, 115, 120, 125, 130]\nmean = np.mean(data)\nprint(f'Mean dari dataset adalah: ', mean)\n\nMean dari dataset adalah:  107.5\n\n\nLalu, kita akan menghitung jarak antara nilai x dan mean\n\ntotal = 0\nfor i in data:\n    calc = (107.5-i)**2\n    total = total + calc\n\nprint(f'jarak antara nilai x dan mean adalah ', total)\n\njarak antara nilai x dan mean adalah  2412.5\n\n\nMari kita hitung standar deviasi nya\n\n# math.sqrt adalah fungsi untuk melakukan operasi akar pangkat\n# round adalah fungsi untuk membulatkan hasil operasi)\nstddev = round(math.sqrt(total/len(data)), 2)\nprint(f'nilai standar deviasi adalah ', stddev)\n\nnilai standar deviasi adalah  11.58\n\n\nHitung nilai imputasi End of Tail\n\nimp = round(mean + 3 * stddev, 1)\nprint(f'nilai imputasi end of tail adalah ', imp)\n\nnilai imputasi end of tail adalah  142.2\n\n\n\nDari kalkulasi nilai imputasi end of tail, diperoleh nilai 142.2.\nKita akan memasukkan nilai ini ke dalam dataset\n\ndf['IQ'] = df['IQ'].fillna(imp)\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n85.0\n\n\n1\n26\n90.0\n\n\n2\n29\n95.0\n\n\n3\n30\n95.0\n\n\n4\n30\n100.0\n\n\n5\n31\n142.2\n\n\n6\n44\n100.0\n\n\n7\n46\n110.0\n\n\n8\n22\n105.0\n\n\n9\n33\n105.0\n\n\n10\n35\n110.0\n\n\n11\n27\n142.2\n\n\n12\n21\n110.0\n\n\n13\n23\n110.0\n\n\n14\n45\n115.0\n\n\n15\n47\n115.0\n\n\n16\n41\n115.0\n\n\n17\n38\n120.0\n\n\n18\n37\n142.2\n\n\n19\n21\n125.0\n\n\n20\n24\n130.0\n\n\n\n\n\n\n\n\n\n8.2.3.3.3 Distribusi Skewed\nDistribusi skew atau skewness mengacu pada karakteristik asimetri dalam distribusi data. Dalam distribusi skew, ekor distribusi data cenderung condong ke salah satu sisi, baik ke kanan (positif) atau ke kiri (negatif), dibandingkan dengan pusat distribusi.\nDalam distribusi skew positif, ekor distribusi condong ke kanan, sementara nilai-nilai yang lebih kecil cenderung berada di sebelah kiri. Ini menghasilkan ekor yang panjang di sisi kanan distribusi.\n\nDalam distribusi skew negatif, ekor distribusi condong ke kiri, dengan nilai-nilai yang lebih besar cenderung berada di sebelah kiri. Ini menghasilkan ekor yang panjang di sisi kiri distribusi. Nilai rata-rata akan lebih kecil daripada median dalam distribusi ini.\n\n\n\n8.2.3.3.4 Distribusi Skewed - Hands On Coding\nPertama-tama mari kita buat data baru yang terdiri dari 21 data dengan 3 missing value. Tugas kita adalah mengisi missing value dengan imputasi end of tail.\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\ndata = [125, 130, 125, 95, 115, np.nan, 100, np.nan, 130, 110,\n        90, 110, 120, 115, 105, 85, 115, 110, 120, 100, np.nan]\n\ndf = pd.DataFrame({'age': age, 'IQ': data})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n125.0\n\n\n1\n26\n130.0\n\n\n2\n29\n125.0\n\n\n3\n30\n95.0\n\n\n4\n30\n115.0\n\n\n5\n31\nNaN\n\n\n6\n44\n100.0\n\n\n7\n46\nNaN\n\n\n8\n22\n130.0\n\n\n9\n33\n110.0\n\n\n10\n35\n90.0\n\n\n11\n27\n110.0\n\n\n12\n21\n120.0\n\n\n13\n23\n115.0\n\n\n14\n45\n105.0\n\n\n15\n47\n85.0\n\n\n16\n41\n115.0\n\n\n17\n38\n110.0\n\n\n18\n37\n120.0\n\n\n19\n21\n100.0\n\n\n20\n24\nNaN\n\n\n\n\n\n\n\nSelanjutnya kita akan mengecek jenis distribusi data menggunakan library matplotlib\n\ndata = [125, 130, 125, 95, 115, 100, 130, 110, 90,\n        110, 120, 115, 105, 85, 115, 110, 120, 100]\n\nfig = px.histogram(df, x='IQ')\nfig.show()\n\n\n                                                \n\n\nDistribusi data adalah skew negatif karena puncak dari data berada di sebelah kanan titik tengah. Mari kita hitung nilai imputasi menggunakan rumus IQR.\nInter-Quartile-Range (IQR) adalah sebuah nilai yang digunakan untuk mengukur sebaran data dalam sebuah distribusi.\n\\({IQR = Q_{3} - Q_{1}}\\)\n\\({IQR_{max} = Q_{3} + 3 * IQR}\\)\n\\({IQR_{min} = Q_{1} + 3 * IQR}\\)\nPertama-tama, kita akan hitung nilai precentile dari dataset, yang dapat kita kalkulasi dengan mudah menggunakan fungsi np.precentile dari library numpy.\n\nmedian = np.median(data)\nq1 = np.percentile(data, 25)\nq3 = np.percentile(data, 75)\n\nprint(f'Median: {median}')\nprint(f'Q1: {q1}')\nprint(f'Q3: {q3}')\n\nMedian: 112.5\nQ1: 101.25\nQ3: 120.0\n\n\nLalu kita akan menghitung nilai IQR, IQRmin, dan IQRmax\n\niqr = q3 - q1\niqrmin = q1 + 3 * iqr\niqrmax = q3 + 3 * iqr\n\nprint(f'IQR: {iqr}')\nprint(f'IQRmin: {iqrmin}')\nprint(f'IQRmax: {iqrmax}')\n\nIQR: 18.75\nIQRmin: 157.5\nIQRmax: 176.25\n\n\nAnda boleh memilih nilai diantara IQRmin dan IQRmax. Namun untuk contoh ini, kita akan ambil nilai IQRmax saja, lalu kita masukkan ke dataset.\n\ndf['IQ'] = df['IQ'].fillna(iqrmax)\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n125.00\n\n\n1\n26\n130.00\n\n\n2\n29\n125.00\n\n\n3\n30\n95.00\n\n\n4\n30\n115.00\n\n\n5\n31\n176.25\n\n\n6\n44\n100.00\n\n\n7\n46\n176.25\n\n\n8\n22\n130.00\n\n\n9\n33\n110.00\n\n\n10\n35\n90.00\n\n\n11\n27\n110.00\n\n\n12\n21\n120.00\n\n\n13\n23\n115.00\n\n\n14\n45\n105.00\n\n\n15\n47\n85.00\n\n\n16\n41\n115.00\n\n\n17\n38\n110.00\n\n\n18\n37\n120.00\n\n\n19\n21\n100.00\n\n\n20\n24\n176.25\n\n\n\n\n\n\n\n\n\n\n8.2.3.4 Regresi Linier\nTeknik imputasi regresi adalah metode untuk mengisi nilai kosong menggunakan algoritma regresi. Algoritma regresi akan memprediksi nilai kosong berdasarkan hubungan fitur nilai kosong dengan fitur lainnya.\nKelebihan - Sederhana dan mudah dipahami\n- Menggabungkan hubungan antar variabel\n- Cocok untuk data yang besar dan bersifat numerik\nKekurangan - Hanya berlaku untuk data yang linear\n- Sensitif terhadap outlier\n- Tidak dapat menangani data non-numerik\n- Bergantung kepada dua fitur\n\n8.2.3.4.1 Regresi Linier - Hands On Coding\nKelemahan utama dari teknik regresi ini adalah dataset harus mempunyai distribusi linear. Mengambil contoh dataset age (umur) dan IQ, Jika dataset tersebut adalah linear, semakin tinggi umur seseorang, maka semakin tinggi IQ orang tersebut.\nMaka dari itu, kita harus mengecek jenis data yang akan kita kerjakan, apakah data tersebut bersifat linear atau tidak. Untuk mengecek dataset, kita gunakan grafik scatter plot.\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\ndata = [125, 130, 125, 95, 115, np.nan, 100, np.nan, 130, 110,\n        90, 110, 120, 115, 105, 85, 115, 110, 120, 100, np.nan]\n\ndf = pd.DataFrame({'age': age, 'IQ': data})\n\n# hapus data NaN\ndf = df.dropna()\n\n# buat grafik scatter\nfig = px.scatter(df, x='age', y='IQ',\n                 color='age', hover_data=['age', 'IQ'])\nfig.show()\n\n\n                                                \n\n\nSecara garis besar, scatter plot membentuk sebuah garis diagonal dari pojok kiri atas ke pojok kiri bawah.\n\n\n\nGambar9. Data bersifat Linear\n\n\nUntuk menambah keyakinan kita bahwa dataset bersifat linear, kita dapat melakukan pengecekan dataset menggunakan pearson correllation. Kita harus menginstall library scipy terlebih dahulu dengan menjalankan perintah pip install scipy di terminal atau command prompt.\nPearson correlation adalah sebuah rumus yang berfungsi untuk menghitung kekuatan dan arah hubungan antara dua variable. nilai -1 mengindikasikan bahwa korelasi bersifat negatif, semakin kecil nilai variable x, maka nilai variable y akan semakin besar. nilai 0 mengindikasikan bahwa tidak ada korelasi linear antara variable. nilai 1 mengindikasikan bahwa korelasi bersifat positif.\n\ncorr, _ = pearsonr(df['age'], df['IQ'])\nprint(f'Pearsons correlation: {round(corr,2)}')\n\nPearsons correlation: -0.53\n\n\nnilai pearson -0.53 menandakan bahwa hubungan antara variabel umur dan iq adalah linear negatif yang mempunyai intensitas lemah.\nSelanjutnya, kita akan membuat model regresi. Sebelumnya, kita harus menginstall library keras dengan cara menjalankan perintah pip install keras di terminal atau command prompt.\n\nmodel = Sequential()\nmodel.add(Dense(1, input_shape=(1,)))\nmodel.compile(Adam(learning_rate=0.8), 'mean_squared_error')\n\nmodel.fit(df['age'], df['IQ'], epochs=1000, verbose=0)\n\npred = model.predict(df['age'])\n\nplt.scatter(df['age'], df['IQ'])\nplt.plot(df['age'], pred, color='red')\nplt.xlabel('Age')\nplt.ylabel('IQ')\nplt.show()\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 41ms/step\n\n\n\n\n\nLalu buat hasil prediksi dari model regresi. Nilai yang kosong adalah IQ dengan umur 27, 31, dan 37. Maka kita masukkan ketiga nilai tersebut ke model untuk di prediksi nilai IQ nya.\n\nhasilprediksi = model.predict([27, 31, 37]).round(0).astype(int)\nprint(hasilprediksi)\n\n1/1 [==============================] - ETA: 0s\n\n\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 64ms/step\n\n\n[[115]\n [112]\n [107]]\n\n\nHasil prediksi menunjukkan angka 115, 112, dan 107. Maka kita akan masukkan nilai IQ tersebut kedalam dataset\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\ndata = [125, 130, 125, 95, 115, np.nan, 100, np.nan, 130, 110,\n        90, 110, 120, 115, 105, 85, 115, 110, 120, 100, np.nan]\n\ndata[5] = hasilprediksi[0][0]\ndata[7] = hasilprediksi[1][0]\ndata[20] = hasilprediksi[2][0]\ndf = pd.DataFrame({'age': age, 'IQ': data})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n125\n\n\n1\n26\n130\n\n\n2\n29\n125\n\n\n3\n30\n95\n\n\n4\n30\n115\n\n\n5\n31\n115\n\n\n6\n44\n100\n\n\n7\n46\n112\n\n\n8\n22\n130\n\n\n9\n33\n110\n\n\n10\n35\n90\n\n\n11\n27\n110\n\n\n12\n21\n120\n\n\n13\n23\n115\n\n\n14\n45\n105\n\n\n15\n47\n85\n\n\n16\n41\n115\n\n\n17\n38\n110\n\n\n18\n37\n120\n\n\n19\n21\n100\n\n\n20\n24\n107\n\n\n\n\n\n\n\n\n\n\n8.2.3.5 Frequent\nImputasi frequent adalah teknik imputasi yang hanya bisa digunakan di jenis data kategorik. Kita mengambil nilai kategorik yang paling sering muncul, dan memasukkannya ke data yang kosong.\nKelebihan - Cocok untuk data dengan missing at random.\n- Mudah dan cepat diterapkan.\n- Cocok utk data yang memiliki skew\n- Dapat digunakan dalam produksi (mis. dalam model deployment).\nKelemahan - Mendistorsi relasi label dengan frekuensi tertinggi vs variabel lain.\n- Menghasilkan over-representation jika banyak data yang missing.\n\n8.2.3.5.1 Frequent - Hands On Coding\nLangkah pertama adalah memuat dataset yang mempunyai jenis data kategorik. Oleh karena itu, fitur IQ kita ganti oleh nilai ‘rendah’, ‘sedang’, dan ‘tinggi’\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\nIQ = ['rendah', 'sedang', 'sedang', np.nan, 'sedang', 'rendah', np.nan, 'tinggi', 'sedang', 'sedang',\n      'rendah', 'sedang', 'tinggi', 'sedang', np.nan, 'rendah', 'tinggi', 'sedang', 'tinggi', 'rendah', 'tinggi']\n\ndf = pd.DataFrame({'age': age, 'IQ': IQ})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nrendah\n\n\n1\n26\nsedang\n\n\n2\n29\nsedang\n\n\n3\n30\nNaN\n\n\n4\n30\nsedang\n\n\n5\n31\nrendah\n\n\n6\n44\nNaN\n\n\n7\n46\ntinggi\n\n\n8\n22\nsedang\n\n\n9\n33\nsedang\n\n\n10\n35\nrendah\n\n\n11\n27\nsedang\n\n\n12\n21\ntinggi\n\n\n13\n23\nsedang\n\n\n14\n45\nNaN\n\n\n15\n47\nrendah\n\n\n16\n41\ntinggi\n\n\n17\n38\nsedang\n\n\n18\n37\ntinggi\n\n\n19\n21\nrendah\n\n\n20\n24\ntinggi\n\n\n\n\n\n\n\nLalu kita hitung frekuensi dari data kategorik\n\nfreq = df['IQ'].value_counts()\nprint(freq)\n\nsedang    8\nrendah    5\ntinggi    5\nName: IQ, dtype: int64\n\n\nKategori sedang mempunyai frekuensi paling tinggi dengan nilai 8. Jadi, nilai imputasi yang akan kita gunakan adalah ‘sedang’\nSelanjutnya, kita masukkan data ‘sedang’ ke dataset.\n\ndf['IQ'] = df['IQ'].fillna('sedang')\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nrendah\n\n\n1\n26\nsedang\n\n\n2\n29\nsedang\n\n\n3\n30\nsedang\n\n\n4\n30\nsedang\n\n\n5\n31\nrendah\n\n\n6\n44\nsedang\n\n\n7\n46\ntinggi\n\n\n8\n22\nsedang\n\n\n9\n33\nsedang\n\n\n10\n35\nrendah\n\n\n11\n27\nsedang\n\n\n12\n21\ntinggi\n\n\n13\n23\nsedang\n\n\n14\n45\nsedang\n\n\n15\n47\nrendah\n\n\n16\n41\ntinggi\n\n\n17\n38\nsedang\n\n\n18\n37\ntinggi\n\n\n19\n21\nrendah\n\n\n20\n24\ntinggi\n\n\n\n\n\n\n\n\n\n\n8.2.3.6 K-Nearest Neighbor (KNN)\nImputasi menggunakan K-Nearest Neighbors (KNN) adalah sebuah metode untuk mengisi data kosong dengan mempertimbangkan nilai terdekat dari fitur lain di kategori yang sama.\nKelebihan - Lebih akurat vs mean/median/most frequent.\nKekurangan - Biaya komputasi mahal (karena KNN bekerja dengan menyimpan seluruh dataset pelatihan dalam memori).\n-Sensitif terhadap outlier dalam data (tidak seperti SVM).\n\n8.2.3.6.1 K-Nearest Neighbor (KNN) - Hands On Coding\nDataset yang kita gunakan sama dengan contoh diatas\n\nage = [25, 26, 29, 30, 30, 31, 44, 46, 22, 33,\n       35, 27, 21, 23, 45, 47, 41, 38, 37, 21, 24]\nIQ = ['rendah', 'sedang', 'sedang', np.nan, 'sedang', 'rendah', np.nan, 'tinggi', 'sedang', 'sedang',\n      'rendah', 'sedang', 'tinggi', 'sedang', np.nan, 'rendah', 'tinggi', 'sedang', 'tinggi', 'rendah', 'tinggi']\n\ndf = pd.DataFrame({'age': age, 'IQ': IQ})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nrendah\n\n\n1\n26\nsedang\n\n\n2\n29\nsedang\n\n\n3\n30\nNaN\n\n\n4\n30\nsedang\n\n\n5\n31\nrendah\n\n\n6\n44\nNaN\n\n\n7\n46\ntinggi\n\n\n8\n22\nsedang\n\n\n9\n33\nsedang\n\n\n10\n35\nrendah\n\n\n11\n27\nsedang\n\n\n12\n21\ntinggi\n\n\n13\n23\nsedang\n\n\n14\n45\nNaN\n\n\n15\n47\nrendah\n\n\n16\n41\ntinggi\n\n\n17\n38\nsedang\n\n\n18\n37\ntinggi\n\n\n19\n21\nrendah\n\n\n20\n24\ntinggi\n\n\n\n\n\n\n\nAlgoritma KNN tidak bisa menghitung data berjenis string, maka kita harus konversi dari string (huruf/kata) ke integer (angka).\n\ndf['IQ'] = df['IQ'].map({'rendah': 1, 'sedang': 2, 'tinggi': 3})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\n1.0\n\n\n1\n26\n2.0\n\n\n2\n29\n2.0\n\n\n3\n30\nNaN\n\n\n4\n30\n2.0\n\n\n5\n31\n1.0\n\n\n6\n44\nNaN\n\n\n7\n46\n3.0\n\n\n8\n22\n2.0\n\n\n9\n33\n2.0\n\n\n10\n35\n1.0\n\n\n11\n27\n2.0\n\n\n12\n21\n3.0\n\n\n13\n23\n2.0\n\n\n14\n45\nNaN\n\n\n15\n47\n1.0\n\n\n16\n41\n3.0\n\n\n17\n38\n2.0\n\n\n18\n37\n3.0\n\n\n19\n21\n1.0\n\n\n20\n24\n3.0\n\n\n\n\n\n\n\nLalu kita membuat model KNN. Hasil dari KNN bisa langsung dimasukkan ke dataset.\n\nimputer = KNNImputer(n_neighbors=3)\n# isi missing value dengan KNN, lalu dibulatkan\ndf = pd.DataFrame(np.round(imputer.fit_transform(df)), columns=df.columns)\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25.0\n1.0\n\n\n1\n26.0\n2.0\n\n\n2\n29.0\n2.0\n\n\n3\n30.0\n2.0\n\n\n4\n30.0\n2.0\n\n\n5\n31.0\n1.0\n\n\n6\n44.0\n2.0\n\n\n7\n46.0\n3.0\n\n\n8\n22.0\n2.0\n\n\n9\n33.0\n2.0\n\n\n10\n35.0\n1.0\n\n\n11\n27.0\n2.0\n\n\n12\n21.0\n3.0\n\n\n13\n23.0\n2.0\n\n\n14\n45.0\n2.0\n\n\n15\n47.0\n1.0\n\n\n16\n41.0\n3.0\n\n\n17\n38.0\n2.0\n\n\n18\n37.0\n3.0\n\n\n19\n21.0\n1.0\n\n\n20\n24.0\n3.0\n\n\n\n\n\n\n\n\n\n\n8.2.3.7 Kesimpulan\n\nSebelum melakukan imputasi kita harus mengetahui jenis missing data, tipe data, dan distribusi data\n\nTidak ada metode imputasi yang sempurna, masing masing teknik mempunyai kelebihan dan kelemahan yang unik\n\nDistribusi data sangat berpengaruh terhadap efisiensi imputasi\n\nImputasi dapat dilakukan menggunakan function dari library sklearn, feature_engine, dan keras. Namun akan jauh lebih baik jika kita menghitung/coding secara manual untuk mengetahui cara kerja algoritma tsb sebelum menggunakan function dari library."
  },
  {
    "objectID": "modul_7.html#handling-outlier",
    "href": "modul_7.html#handling-outlier",
    "title": "8  Transformasi Data",
    "section": "8.3 Handling Outlier",
    "text": "8.3 Handling Outlier\nOutlier adalah sebuah data yang mempunyai pola atau letak yang menyimpang sangat jauh dari rata rata dataset atau pola yang diharapkan dari sebuah dataset.\nOutlier dapat terjadi karena berbagai alasan, seperti kesalahan pengukuran, kesalahan entri data, variasi alami, atau peristiwa langka. Outlier dapat memiliki dampak signifikan pada analisis statistik, model pembelajaran mesin, dan interpretasi data, yang dapat mengarah pada hasil yang bias atau kesimpulan yang tidak akurat.\nOutlier harus kita atasi agar data bisa kita proses secara efisien.\n\n8.3.1 Deteksi Outlier\nOutlier dapat dideteksi menggunakan beberapa metode, antara lain\n\n8.3.1.1 Visualisasi\nAdalah sebuah teknik untuk memvisualisasikan sebuah data menjadi suatu bentuk yang dapat dilihat secara menyeluruh sehingga kita dapat menganalisa bentuk data, ukuran data, data point dan mendeteksi outlier.\nBeberapa bentuk visualisasi yang sering digunakan untuk mendeteksi outlier yaitu - Histogram\n- Scatter plot\n- Box plot\nKita akan menggunakan library plotly express untuk melakukan visualisasi, jadi jangan lupa untuk menginstall plotly dengan menjalankan pip install plotly di terminal atau cmd\nDataset yang kita gunakan adalah dataset lagu yang diambil dari spotify. Sumber dari dataset adalah https://www.kaggle.com/datasets/vatsalmavani/spotify-dataset\nNamun karena dataset ukurannya sangat besar, maka kita akan gunakan subset dataset berjumlah 100 data yang diambil secara random. Dataset versi ini dapat di download di https://github.com/rif42/AssociateDataScientist/blob/master/Module7-AssociateDataScientist/data_sampled_100.csv\nJika sudah di download, masukkan data ke dalam directory lalu muat dataset tersebut.\n\ndata = pd.read_csv('data_sampled_100.csv')\ndata.head()\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\nartists\ndanceability\nduration_ms\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\n\n\n\n\n0\n0.940\n2006\n0.330\n['Los Horóscopos De Durango']\n0.687\n204587\n0.724\n0\n2KIUMqD9ZkcxhgSVyIDP4t\n0.000273\n4\n0.248\n-2.478\n0\nMi Amor Por Ti\n48\n2006-01-01\n0.0301\n141.507\n\n\n1\n0.165\n2002\n0.109\n['Good Charlotte']\n0.515\n242933\n0.566\n0\n0eYcZLnlLKaItC1WC4B1pc\n0.000000\n6\n0.160\n-6.176\n1\nEmotionless\n42\n2002-10-04\n0.0303\n154.320\n\n\n2\n0.551\n1987\n0.795\n['Raphael']\n0.402\n159560\n0.427\n0\n6b3ub116kE1T15h1yzaiTy\n0.000000\n2\n0.222\n-11.328\n0\nYo soy aquél\n38\n1987-08-25\n0.0359\n111.065\n\n\n3\n0.557\n1998\n0.790\n['Joan Sebastian', 'Pepe Aguilar']\n0.392\n180976\n0.426\n0\n2bWImXJQNVfgrROo4Xj630\n0.000004\n9\n0.208\n-7.088\n1\nEstás Fallando\n48\n1998-09-10\n0.0404\n183.039\n\n\n4\n0.480\n1965\n0.988\n['Vince Guaraldi Trio']\n0.339\n149227\n0.243\n0\n5dwY5r2PfjMteikJViyNIT\n0.893000\n0\n0.113\n-16.851\n0\nGreat Pumpkin Waltz\n55\n1965\n0.0382\n151.481\n\n\n\n\n\n\n\nDataset ini berisi lagu-lagu yang ada di Spotify. Setiap lagu mempunyai fitur yang unik seperti popularitas, durasi, loudness, acousticness, speechiness, danceability, dll. Anda bisa melihat deskripsi data secara detail di https://developer.spotify.com/documentation/web-api/reference/get-several-audio-features\nSelanjutnya kita akan lakukan proses visualisasi data menggunakan plot histogram dengan library plotly. Fitur yang kita visualisasikan adalah loudness atau volume lagu terhadap popularitas dari lagu yang ada di dataset.\nBerikut diagram scatter plot dari dataset:\n\nfig = px.scatter(data, x='loudness', y='popularity', color='loudness',\n                 hover_data=['artists', 'name', 'loudness', 'popularity'])\nfig.show()\n\n\n                                                \n\n\nBerikut diagram histogram dari dataset:\n\nfig = px.histogram(data, x='loudness')\nfig.show()\n\n\n                                                \n\n\nBerikut diagram box plot dari dataset:\n\nfig = px.box(data, x='loudness')\nfig.show()\n\n\n                                                \n\n\nAda beberapa data yang bisa disebut outlier, salah satunya adalah lagu Thursday Afternoon - 2005 digital remaster, oleh Brian Eno yang mempunyai nilai loudness -31.8808. Alasannya adalah tingkat loudness nya jauh lebih kecil daripada yang lain.\n\n\n\n8.3.2 Kategori Outlier\nVariate dan univariate adalah dua jenis data dalam statistik. Outlier adalah observasi atau nilai yang secara signifikan berbeda dari pola atau pola umum data yang lain.\n\n8.3.2.1 Variate Data\nVariate data merujuk pada set data yang terdiri dari beberapa variabel atau fitur. Contoh umum variate data adalah dataset yang terdiri dari beberapa kolom, di mana setiap kolom mewakili variabel yang berbeda. Misalnya, jika kita memiliki dataset tentang mahasiswa yang mencakup variabel seperti tinggi, berat badan, dan usia, maka kita memiliki variate data. Outlier dalam variate data merujuk pada observasi atau nilai yang di luar kisaran yang diharapkan dalam setidaknya satu variabel.\n\n\n8.3.2.2 Univariate Data\nUnivariate data merujuk pada set data yang hanya memiliki satu variabel atau fitur. Contoh umum univariate data adalah dataset yang hanya terdiri dari satu kolom, seperti data tinggi badan seseorang. Outlier dalam univariate data merujuk pada observasi atau nilai yang sangat ekstrem atau jauh dari rentang nilai yang diharapkan.\n\ndf = pd.DataFrame({'age': [25, 26, 29, 30, 30, 31, 44, 46],\n                   'IQ': [np.NaN, 121, 91, np.NaN, 110, np.NaN, 118, 93]})\ndisplay(df)\n\n\n\n\n\n\n\n\nage\nIQ\n\n\n\n\n0\n25\nNaN\n\n\n1\n26\n121.0\n\n\n2\n29\n91.0\n\n\n3\n30\nNaN\n\n\n4\n30\n110.0\n\n\n5\n31\nNaN\n\n\n6\n44\n118.0\n\n\n7\n46\n93.0\n\n\n\n\n\n\n\nMasih ingat contoh data di imputasi data diatas? Data berisi fitur umur dan nilai IQ. Masing masing fitur hanya mempunyai 1 buah nilai. Inilah yang dimaksud dengan univariate data; data di dalam fitur hanya mempunyai satu jenis nilai.\n\n\n\n8.3.3 Mengatasi Outlier\nTidak semua outlier harus dihapus atau dihilangkan. Pada contoh data lagu di spotify, meskipun letaknya jauh dari yang lain,data ini masih valid sebagai lagu. Karena adalah lagu Thursday Afternoon - 2005 digital remaster, oleh Brian Eno mempunyai genre instrumental/ambient. Umumnya, tujuan utama genre ini adalah sebagai music background, sehingga tidak diperlukan vokal atau melodi yang keras.\nNamun terkadang outlier harus dihilangkan atau dihapus karena nilainya terlalu jauh dengan yang lain, yang dapat merubah value dari dataset secara keseluruhan, lalu dapat merubah hasil dari training data.\n\n8.3.3.1 Discretization/Binning\nDiscretization atau binning adalah proses mengubah data kontinu menjadi data diskrit dengan cara membagi rentang nilai kontinu menjadi beberapa interval atau kelompok yang disebut “bin” atau “bucket”. Tujuan utama dari discretization adalah mengurangi kompleksitas data kontinu dengan mengelompokkan nilainya ke dalam kategori atau range tertentu.\n\n\n\nGambar10. Grafik Proses Binning Dataset\n\n\nKelebihan - Dapat diterapkan pada data kategorik dan numerik.\n- Model lebih robust dan mencegah overfitting.\nKekurangan - Meningkatnya biaya kinerja perhitungan.\n- Mengorbankan informasi.\n- Untuk kolom data numerik, dapat menyebabkan redudansi untuk beberapa algoritma.\n- Untuk kolom data kategorik, label dengan frekuensi rendah berdampak negatif pada robustness model statistik.\n\n8.3.3.1.1 Discretization/Binning - Hands On Coding\nKita akan menggunakan dataset lagu spotify dengan fitur loudness untuk melakukan proses binning. Binning dilakukan menggunakan fungsi pd.cut. Fungsi ini akan membagi semua nilai yang ada di dalam sebuah fitur menjadi 3 (atau angka lain yang anda inginkan)\n\n# muat data\ndf = pd.read_csv('./data_sampled_100.csv')\n\n# binning data menjadi 5 kategori\ndf['loudness'] = pd.cut(df['loudness'], bins=3, labels=[\n                        'sunyi', 'standar', 'bising'])\n\n# urutkan data berdasarkan fitur loudness\ndf_sorted = df.sort_values(by='loudness')\n\n# visualisasikan data menggunakan histogram\nfig = px.histogram(df_sorted, x='loudness')\nfig.show()\n\n\n                                                \n\n\n\n\n\n8.3.3.2 Trimming\nTrimming adalah proses penghapusan data yang dianggap sebagai outlier. Trimming biasanya dilakukan berdasarkan presentase data yang akan di trim, contohnya 5%.\n\nKelebihan\n- Cepat dan mudah\n- Dapat memperbaiki rata-rata data\nKekurangan\n- Hilangnya data yang dapat mengandung informasi\n- Dapat menyebabkan bias terhadap variansi data\n\n8.3.3.2.1 Trimming - Hands On Coding\nSangat penting untuk melakukan proses trimming hanya pada data tidak dibutuhkan/invalid. Oleh karena itu, proses trimming seharusnya dimulai dari angka yang sangat kecil, semisal 0.0001%. Jika outlier masih nampak, maka kita tambah presentasenya sedikit demi sedikit.\nDataset yang kita gunakan masih sama, yaitu lagu spotify dengan fitur loudness. Namun karena ukuran dataset kita cukup kecil, maka kita bisa gunakan presentase trimming 1%.\nPertama kita muat data, lalu kita sortir dataset.\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf_sorted = df.sort_values(by='loudness')\ndf.head()\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\nartists\ndanceability\nduration_ms\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\n\n\n\n\n0\n0.940\n2006\n0.330\n['Los Horóscopos De Durango']\n0.687\n204587\n0.724\n0\n2KIUMqD9ZkcxhgSVyIDP4t\n0.000273\n4\n0.248\n-2.478\n0\nMi Amor Por Ti\n48\n2006-01-01\n0.0301\n141.507\n\n\n1\n0.165\n2002\n0.109\n['Good Charlotte']\n0.515\n242933\n0.566\n0\n0eYcZLnlLKaItC1WC4B1pc\n0.000000\n6\n0.160\n-6.176\n1\nEmotionless\n42\n2002-10-04\n0.0303\n154.320\n\n\n2\n0.551\n1987\n0.795\n['Raphael']\n0.402\n159560\n0.427\n0\n6b3ub116kE1T15h1yzaiTy\n0.000000\n2\n0.222\n-11.328\n0\nYo soy aquél\n38\n1987-08-25\n0.0359\n111.065\n\n\n3\n0.557\n1998\n0.790\n['Joan Sebastian', 'Pepe Aguilar']\n0.392\n180976\n0.426\n0\n2bWImXJQNVfgrROo4Xj630\n0.000004\n9\n0.208\n-7.088\n1\nEstás Fallando\n48\n1998-09-10\n0.0404\n183.039\n\n\n4\n0.480\n1965\n0.988\n['Vince Guaraldi Trio']\n0.339\n149227\n0.243\n0\n5dwY5r2PfjMteikJViyNIT\n0.893000\n0\n0.113\n-16.851\n0\nGreat Pumpkin Waltz\n55\n1965\n0.0382\n151.481\n\n\n\n\n\n\n\nSelanjutnya, kita ambil 1% dari data tersebut, hanya dari tail atau ujung belakang dari dataset. Nilai ini nantinya akan menjadi batas dari trimming kita.\n\nq = df['loudness'].sort_values().quantile(0.01)\nprint(q)\n\n-27.3332\n\n\nJadi batas dari trimming kita adalah -27.3332. Selanjutnya kita akan hilangkan data yang melebihi nilai ini.\n\ndf = df[df['loudness'] &gt; q]\nfig = px.scatter(df, x='loudness', y='popularity', color='loudness',\n                 hover_data=['artists', 'name', 'loudness', 'popularity'])\nfig.show()\n\n\n                                                \n\n\nBisa dilihat bahwa lagu Thursday Afternoon - 2005 digital remaster, oleh Brian Eno yang mempunyai loudness kurang dari batas trimming sudah hilang.\n\n\n\n8.3.3.3 Winsorizing\nWinsorizing adalah proses penggantian data outlier dengan nilai-nilai yang berada dalam distribusi yang ditentukan.\nKelebihan\n- Mempertahankan informasi\n- Mengurangi efek dari outlier\n- Mudah diimplementasikan\nKekurangan\n- Dapat menghasilkan bias\n- Pemilihan presentil dapat mempengaruhi hasil analisis data\n\n8.3.3.3.1 Winsorizing - Hands On Coding\nKita akan menggunakan dataset dan fitur yang sama, yaitu lagu spotify dengan fitur loudness. Pertama kita akan muat data, lalu tentukan batas data yang akan kita winsorize. Kita akan menggunakan nilai yang sama dengan metode sebelumnya(trimming) yaitu 1% dan batas trimming -27.3332.\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf_sorted = df.sort_values(by='loudness')\ndf.head()\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\nartists\ndanceability\nduration_ms\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\n\n\n\n\n0\n0.940\n2006\n0.330\n['Los Horóscopos De Durango']\n0.687\n204587\n0.724\n0\n2KIUMqD9ZkcxhgSVyIDP4t\n0.000273\n4\n0.248\n-2.478\n0\nMi Amor Por Ti\n48\n2006-01-01\n0.0301\n141.507\n\n\n1\n0.165\n2002\n0.109\n['Good Charlotte']\n0.515\n242933\n0.566\n0\n0eYcZLnlLKaItC1WC4B1pc\n0.000000\n6\n0.160\n-6.176\n1\nEmotionless\n42\n2002-10-04\n0.0303\n154.320\n\n\n2\n0.551\n1987\n0.795\n['Raphael']\n0.402\n159560\n0.427\n0\n6b3ub116kE1T15h1yzaiTy\n0.000000\n2\n0.222\n-11.328\n0\nYo soy aquél\n38\n1987-08-25\n0.0359\n111.065\n\n\n3\n0.557\n1998\n0.790\n['Joan Sebastian', 'Pepe Aguilar']\n0.392\n180976\n0.426\n0\n2bWImXJQNVfgrROo4Xj630\n0.000004\n9\n0.208\n-7.088\n1\nEstás Fallando\n48\n1998-09-10\n0.0404\n183.039\n\n\n4\n0.480\n1965\n0.988\n['Vince Guaraldi Trio']\n0.339\n149227\n0.243\n0\n5dwY5r2PfjMteikJViyNIT\n0.893000\n0\n0.113\n-16.851\n0\nGreat Pumpkin Waltz\n55\n1965\n0.0382\n151.481\n\n\n\n\n\n\n\nPerbedaan utama antara trimming dan winsorizing adalah, di proses trimming, nilai dibawah batas dihilangkan, namun di proses winsorizing, nilai dibawah batas digantikan dengan nilai diantara quartil 1 dan quartil 3.\nUntuk itu, kita akan menggunakan box plot karena box plot sudah memberikan nilai high fence dan low fence secara langsung.\n\nfig = px.box(df, x='loudness')\nfig.show()\n\n\n                                                \n\n\nSelanjutnya, kita akan menghitung berapa banyak data yang ada dibawah batas trimming\n\nq = df['loudness'].sort_values().quantile(0.01)\noutlier = df[df['loudness'] &lt; q]\noutlier\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\nartists\ndanceability\nduration_ms\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\n\n\n\n\n69\n0.0528\n1985\n0.976\n['Brian Eno']\n0.0918\n3650800\n0.0569\n0\n4t3Yh6tKkxXrc458pNI7zZ\n0.884\n0\n0.0842\n-31.808\n1\nThursday Afternoon - 2005 Digital Remaster\n39\n1985-10-01\n0.0358\n81.944\n\n\n\n\n\n\n\nDiketahui ada 1 buah outlier. Selanjutnya kita akan menentukan nilai q1 dan q3 sebagai batasan nilai winsorizing kita.\n\nq1 = np.percentile(df['loudness'], 25)\nq3 = np.percentile(df['loudness'], 75)\nprint(f'q1 = ', q1)\nprint(f'q3 = ', q3)\n\nq1 =  -15.43075\nq3 =  -7.177250000000001\n\n\nNilai q1 dan q3 ini akan kita gunakan untuk memberi batasan terhadap angka random yang akan kita generate sebagai nilai winsorizing kita.\n\n# nilai n menyesuaikan jumlah data outlier\noutliercount = len(outlier)\n\n# generate random number\nrandom.seed(time.time_ns())\nwinsorized_outlier = [random.randint(int(q1), int(q3))\n                      for i in range(outliercount)]\nprint(winsorized_outlier)\n\n[-7]\n\n\nIngat, nilai ini di generate secara random, jadi nilai akan berubah setiap code di run.\nSelanjutnya, kita akan memasukkan nilai winsorizing ke outlier, lalu masukkan outlier ke dataset kita.\n\nfor i in range(len(winsorized_outlier)):\n    outlier.iloc[i, 12] = winsorized_outlier[i]\n\n# masukkan outlier ke dataset induk\nfor i in range(len(outlier)):\n    id = outlier.iloc[i, 8]  # ambil ID dari data outlier\n    # cari index dari data outlier di dataset\n    index = df[df['id'] == id].index[0]\n    df.iloc[index, 12] = outlier.iloc[i, 12]  # gantikan dataset dengan outlier\n\ndf = df.sort_values(by='loudness')\ndisplay(df)\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\nartists\ndanceability\nduration_ms\nenergy\nexplicit\nid\ninstrumentalness\nkey\nliveness\nloudness\nmode\nname\npopularity\nrelease_date\nspeechiness\ntempo\n\n\n\n\n22\n0.0447\n1945\n0.90100\n['Anca Elena']\n0.573\n214571\n0.00574\n0\n4SriIcIMNhfwyA8oMH79KP\n0.317000\n9\n0.1660\n-27.288\n1\nRegard du temps\n0\n1945\n0.0678\n131.582\n\n\n19\n0.0454\n2004\n0.96100\n['Ludovico Einaudi']\n0.191\n357707\n0.05820\n0\n3weNRklVDqb4Rr5MhKBR3D\n0.890000\n8\n0.0941\n-25.398\n1\nNuvole Bianche\n72\n2004-01-01\n0.0578\n132.614\n\n\n76\n0.0590\n1965\n0.99200\n['Frédéric Chopin', 'Arthur Rubinstein']\n0.260\n325893\n0.01910\n0\n6mry9fDj4oTFudQAMRo1lV\n0.892000\n3\n0.0864\n-23.594\n0\nNocturnes, Op. 9: No. 1 in B-Flat Minor\n31\n1965\n0.0410\n62.106\n\n\n97\n0.0562\n1952\n0.22700\n['Johannes Brahms', 'Pierre Monteux']\n0.183\n122600\n0.12800\n0\n1kzteGupSBIJVs0Aff51SC\n0.785000\n0\n0.1570\n-22.986\n1\nSong of Destiny, Op. 54: III. Adagio\n0\n1952\n0.0368\n111.440\n\n\n92\n0.2360\n1926\n0.47500\n['Georgette Heyer', 'Irina Salkow']\n0.679\n134571\n0.12100\n0\n5bsAPgurneSpyfqfNCSElq\n0.000000\n4\n0.1160\n-21.337\n1\nKapitel 272 - Der Page und die Herzogin\n1\n1926\n0.9440\n145.910\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n89\n0.5670\n2016\n0.19700\n['DJ Drama', 'Chris Brown', 'Jhené Aiko', 'Tor...\n0.563\n330387\n0.69100\n1\n6Op6z8dK5XC9lGRZe4XRF2\n0.000000\n3\n0.0886\n-4.464\n0\nWishing Remix (feat. Chris Brown, Fabolous, Tr...\n57\n2016-09-29\n0.3820\n108.406\n\n\n33\n0.9380\n2007\n0.00117\n['Linkin Park']\n0.655\n189293\n0.88500\n1\n1fLlRApgzxWweF1JTf8yM5\n0.000473\n7\n0.0448\n-4.116\n1\nGiven Up\n68\n2007-05-14\n0.0438\n100.088\n\n\n60\n0.6930\n1964\n0.47200\n['Little Richard']\n0.337\n159200\n0.79700\n0\n6oSoqM0Otpmuowlzx0jYvK\n0.000015\n8\n0.1370\n-3.838\n1\nGoodnight Irene\n26\n1964-08\n0.0756\n173.437\n\n\n20\n0.0839\n2001\n0.00179\n['Slayer']\n0.238\n215693\n0.99400\n1\n0esBc6VgM4lFk3SOlL3Ys4\n0.009440\n8\n0.2020\n-3.578\n1\nDisciple\n49\n2001-01-01\n0.2500\n95.569\n\n\n0\n0.9400\n2006\n0.33000\n['Los Horóscopos De Durango']\n0.687\n204587\n0.72400\n0\n2KIUMqD9ZkcxhgSVyIDP4t\n0.000273\n4\n0.2480\n-2.478\n0\nMi Amor Por Ti\n48\n2006-01-01\n0.0301\n141.507\n\n\n\n\n100 rows × 19 columns\n\n\n\n\n\n\n8.3.3.4 Imputing\nImputing adalah proses penggantian data outlier dengan nilai-nilai yang diprediksi atau diestimasi berdasarkan karakteristik data. Teknik imputasi sudah di bahas secara detail di bab sebelumnya\nKelebihan\n- Mempertahankan informasi dan ukuran sampel\n- Meningkatkan akurasi analisis\nKekurangan\n- Pemilihan metode dan implementasi bisa cukup sulit\n- Berpotensi merusak distribusi data\n\n\n8.3.3.5 Normalization\nAdalah metode untuk mengubah skala nilai dalam dataset sehingga nilainya berkisar antara 0 dan 1. Untuk melakukan normalisasi data, kita membagi data berdasarkan nilai minimum dan maksimum dari data. Proses normalisasi baik digunakan untuk dataset yang mempunyai distribusi data non-normal atau tidak beraturan.\nRumus dari normalisasi adalah:\n\\[\nnormalized_x = \\dfrac{x - min(x)}{max(x)-min(x)}\n\\]\nKelebihan\n- Mempertahankan informasi dan ukuran sampel\n- Meningkatkan akurasi analisis\n- Mudah diimplementasikan\nKekurangan\n- Metode harus sesuai dengan karakteristik data\n- Tidak menghilangkan outlier secara langsung, hanya mengurangi efek dari outlier\n\n8.3.3.5.1 Normalization - Hands On Coding\nKita akan menggunakan dataset lagu spotify dengan fitur loudness, mirip seperti bab-bab sebelumnya.\nPertama kita cek batasan-batasan data dari fitur loudness\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf['loudness'].describe()\n\ncount    100.000000\nmean     -11.769480\nstd        5.685826\nmin      -31.808000\n25%      -15.430750\n50%      -11.280000\n75%       -7.177250\nmax       -2.478000\nName: loudness, dtype: float64\n\n\nDiketahui bahwa batas minimum data adalah 31.808000, dan batas maksimum data adalah -2.478000\nBatas-batas data ini akan kita ubah menjadi 0 - 1 menggunakan metode normalization. Sebelumnya, kita cek distribusi dari data menggunakan plot histogram.\n\nfig = px.histogram(df, x='loudness')\nfig.show()\n\n\n                                                \n\n\nLalu kita akan gunakan function minmax scaler untuk melakukan proses normalisasi terhadap data loudness.\n\n# buat objek scaler\nscaler = MinMaxScaler()\n\n# transformasi data tempo menggunakan objek scaler\ndf['loudness'] = scaler.fit_transform(df[['loudness']])\n\n# grafik histogram untuk fitur tempo\nfig = px.histogram(df, x='loudness')\nfig.show()\n\n\n                                                \n\n\nSelanjutnya, kita cek batas-batas data dari data yang sudah di normalisasi.\n\ndf['loudness'].describe()\n\ncount    100.000000\nmean       0.683209\nstd        0.193857\nmin        0.000000\n25%        0.558379\n50%        0.699898\n75%        0.839780\nmax        1.000000\nName: loudness, dtype: float64\n\n\nData telah ter-normalisasi! Namun, apa efek dari normalisasi selain mengubah rentang data? Efek yang paling umum adalah jarak data dari kedua ujung menjadi lebih sama-rata. Hal ini dapat menambah akurasi model machine learning.\n\n\n\n8.3.3.6 Standarization / Z-Score\nStandarisasi data adalah suatu proses dalam analisis data yang mengubah variabel-variabel menjadi memiliki rata-rata nol dan standar deviasi satu. Dalam standarisasi data, setiap nilai data dikurangi dengan rata-rata dari seluruh data, kemudian hasilnya dibagi dengan standar deviasi data. Dengan melakukan hal ini, nilai-nilai data akan berada pada skala yang relatif terhadap variabilitas data. Proses standarisasi menggunakan rumus sebagai berikut:\n\\[\nstandardized_x = \\dfrac{x - mean(x)}{standard deviation(x)}\n\\]\nKelebihan\n- Mean dan standar deviasi tidak berubah\n- Tidak sensitif terhadap outlier\nKekurangan\n- Tidak dapat menentukan batasan data\n- Hasil dapat berupa angka negatif\n\n8.3.3.6.1 Standardization / Z-score - Hands On Coding\nKita akan menggunakan dataset lagu spotify dengan fitur loudness, mirip seperti bab-bab sebelumnya.\nPertama kita cek batasan-batasan data dari fitur loudness\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf['loudness'].describe()\n\ncount    100.000000\nmean     -11.769480\nstd        5.685826\nmin      -31.808000\n25%      -15.430750\n50%      -11.280000\n75%       -7.177250\nmax       -2.478000\nName: loudness, dtype: float64\n\n\n\nfig = px.histogram(df, x='loudness')\nfig.show()\n\n\n                                                \n\n\nLalu kita akan gunakan function z-score scaler untuk melakukan proses normalisasi terhadap data loudness.\n\nmean = round(np.mean(df['loudness']), 2)\nstd = round(np.std(df['loudness']), 2)\nnormalized = round((df['loudness']-mean)/std, 2)\n\nprint(f'mean = ', mean)\nprint(f'std deviation = ', std)\nnormalized.describe()\n\nmean =  -11.77\nstd deviation =  5.66\n\n\ncount    100.000000\nmean      -0.000500\nstd        1.004417\nmin       -3.540000\n25%       -0.645000\n50%        0.085000\n75%        0.815000\nmax        1.640000\nName: loudness, dtype: float64\n\n\nSelanjutnya kita buat diagram histogramnya untuk mengecek perubahan distribusi data.\n\nfig = px.histogram(normalized, x=\"loudness\")\nfig.show()\n\n\n                                                \n\n\nJika kita bandingkan histogram data asli dengan data yang di standarisasi, distribusi data tidak berbeda jauh. Selain itu, batas data minimal mengecil dari -31 menjadi -3, dan batas data maksimal membesar dari -2 menjadi 1."
  },
  {
    "objectID": "modul_8.html#pendahuluan",
    "href": "modul_8.html#pendahuluan",
    "title": "8  Pemodelan Data Science",
    "section": "8.1 Pendahuluan",
    "text": "8.1 Pendahuluan\nPemodelan data science adalah proses membangun model yang dapat digunakan untuk memprediksi nilai dari suatu variabel berdasarkan nilai variabel lainnya. Model yang dibangun dapat berupa model statistika, model machine learning, atau model deep learning. Pemodelan data science merupakan salah satu tahapan penting dalam proses data science.\nPada modul ini, data yang akan dijadikan contoh adalah data kamar hotel di Yogyakarta. Data ini berisi informasi mengenai kamar hotel di Yogyakarta yang terdapat pada website Traveloka."
  },
  {
    "objectID": "modul_8.html#tahap-pemodelan-machine-learning",
    "href": "modul_8.html#tahap-pemodelan-machine-learning",
    "title": "8  Pemodelan Data Science",
    "section": "8.2 Tahap Pemodelan Machine Learning",
    "text": "8.2 Tahap Pemodelan Machine Learning\n\n8.2.1 Data Preparation\nUmumnya data terlebih dahulu dibagi menjadi dua bagian, yaitu data training dan data testing. Data training digunakan untuk membangun model, sedangkan data testing digunakan untuk menguji performa model. Data training dan data testing harus memiliki karakteristik yang sama. Data training dan data testing dapat dibagi secara acak. Data training dan data testing dapat dibagi dengan perbandingan 80:20 atau 70:30 dimana data testing seharusnya mendapatkan data yang lebih banyak dibandingkan testing agar model dapat dilatih dengan lebih baik.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# membaca dataset\ndf = pd.read_csv('./dataset/kamar-hotel-yogyakarta.csv')\n\n# Memisahkan fitur dan label\nx = df.iloc[:, 1:]\ny = df.iloc[:, 0]\n# Alternatif\n# x = df.drop('harga', axis=1)\n# y = df['harga']\n\n# membagi data menjadi data training dan data testing dengan perbandingan 80:20\n# Random state digunakan untuk mengatur agar pembagian data menjadi sama setiap kali dijalankan (dapat diisi dengan angka berapapun)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n\n# menampilkan ukuran data training dan data testing\ntrainRatio = round(x_train.shape[0]/len(df), 2)*100\ntestRatio = round(x_test.shape[0]/len(df), 2)*100\n\nprint(f'Train set: {x_train.shape[0]} ({trainRatio}%)')\nprint(f'Test set: {x_test.shape[0]} ({testRatio}%)')\n\nTrain set: 812 (80.0%)\nTest set: 203 (20.0%)\n\n\n\n\n8.2.2 Model Training\nAlgoritma yang digunakan dalam modul ini adalah Algoritma Random Forest. Algoritma Random Forest merupakan algoritma yang digunakan untuk melakukan klasifikasi dan regresi. Algoritma Random Forest merupakan pengembangan dari algoritma Decision Tree. Algoritma Random Forest mengambil keputusan berdasarkan hasil voting dari beberapa pohon keputusan.\nLibrary yang digunakan untuk membangun model Random Forest adalah sklearn. Library ini berisi algoritma untuk membangun model machine learning seperti Random Forest, Gradient Boosting, dan lainnya.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\n\nrf.fit(x_train, y_train)\n\n# menampilkan skor akurasi dari model\nprint(f'Skor R2: {rf.score(x_test, y_test)}')\n\nSkor R2: 0.7542610825873429\n\n\n\n\n8.2.3 Parameter Tuning\nParameter tuning dilakukan untuk menemukan parameter terbaik yang dapat digunakan untuk membangun model. Parameter tuning dapat dilakukan dengan menggunakan GridSearchCV atau RandomizedSearchCV yang terdapat pada library sklearn. Grid Search merupakan teknik untuk mencari parameter terbaik dengan cara mencoba semua kombinasi dari parameter yang diberikan, maka dari itu waktu komputasi akan lebih panjang. Untuk mempercepat waktu komputasi, dapat digunakan Random Search yang akan mencari parameter terbaik secara acak. Untuk detail mengenai parameter yang digunakan pada algoritma Random Forest dapat dilihat pada dokumentasi sklearn.\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# menentukan parameter yang akan dicoba\nrfParams = {\n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_depth': [None, 5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2', None],\n}\n\n# mencari parameter terbaik\nrfRandom = RandomizedSearchCV(rf, rfParams, random_state=123, n_jobs=-1)\n\nrfRandom.fit(x_train, y_train)\n\n# menampilkan parameter terbaik\nprint(f'Parameter terbaik: {rfRandom.best_params_}')\n\n# menampilkan skor akurasi dari model\nprint(f'Skor R2: {rfRandom.score(x_test, y_test)}')\n\nParameter terbaik: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 10}\nSkor R2: 0.7469064478629066\n\n\n\n\n8.2.4 Saving Model\nModel yang telah dibangun dapat disimpan dengan menggunakan library pickle. Library ini digunakan untuk menyimpan objek Python ke dalam file. Model yang telah disimpan dapat digunakan kembali tanpa harus membangun model dari awal.\n\nimport pickle\n\n# refit model dengan parameter terbaik\nrfModel = rfRandom.best_estimator_.fit(x_train, y_train) \n\n# menyimpan model\npickle.dump(rfModel, open('rfModel.pkl', 'wb'))"
  },
  {
    "objectID": "modul_9.html#apa-itu-evaluasi-model",
    "href": "modul_9.html#apa-itu-evaluasi-model",
    "title": "9  Evaluasi Data",
    "section": "9.1 Apa itu Evaluasi Model?",
    "text": "9.1 Apa itu Evaluasi Model?\nMerupakan salah satu tahap penting dalam proses machine learning yang memiliki tujuan untuk memastikan model dapat menghasilkan prediksi yang akurat."
  },
  {
    "objectID": "modul_9.html#metrik-evaluasi",
    "href": "modul_9.html#metrik-evaluasi",
    "title": "9  Evaluasi Data",
    "section": "9.2 Metrik Evaluasi",
    "text": "9.2 Metrik Evaluasi\n\nAkurasi\nPresisi\nRecall\nF1-Score\n\n\n9.2.1 Akurasi\n\nMerupakan metrik yang mengukur sejauh mana model dapat melakukan prediksi dengan benar.\nFormula atau rumus akurasi yaitu\nSemakin tinggi nilai akurasi, maka semakin baik model tersebut.\n\n\nSeberapa Penting Nilai Akurasi\n\nAkurasi dapat digunakan untuk mengevaluasi perfoma sebuah model\nAkurasi memberikan gambaran seberapa baik model yang digunakan secara keseluruhan\nDengan nilai akurasi, dapat mengetahui seberapa akurat dalam memprediksi kelas data.\n\n\n\nKelebihan dan Keterbatasan Akurasi\n\nAkurasi memiliki kelebihan sebagai matrik evaluasi yang sederhana dan sangat mudah dimengerti.\nKeterbatasan akurasi dalam mengatasi ketidakseimbangan kelas pada sebuah dataset.\n\n\n\nPahami Code berikut\n\nPerhatikan contoh code di bawah, kira-kira menghasilkan nilai Akurasi berapa persen?\nAnda dapat mencoba code di samping menggunakan dataset yang berbeda, lalu fahami dan lihat nilai Akurasinya.\nPada dataset yang digunakan, dibagi menjadi 80:20 pada tahap split data. Selanjutnya menggunakan algoritma KNN dengan nilai K=3. selanjutnya akan dilakukan prediksi pada data uji dan akurasi model yang dihitung dengan membandingkan hasil prediksi dengan label sebenarnya pada data uji menggunakan fungsi “accuracy_score”\n\n# Impor library yang diperlukan\nfrom sklearn.datasets import load_iris #dataset dari sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Muat dataset Iris\n#dapat dirubah menggunakan dataset yang lain\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model K-Nearest Neighbors (KNN) dengan k=3\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Hitung akurasi model dengan membandingkan prediksi dengan label sebenarnya pada data uji\naccuracy = accuracy_score(y_test, y_pred)\n\n# Tampilkan hasil akurasi\nprint(\"Akurasi model: {:.2f}%\".format(accuracy * 100))\n\n\n\n\n9.2.2 Presisi dan Recall\nPresisi dan Recall merupakan metrik untuk mengukur performa pada model tertentu. Semakin tinggi nilai presisi dan recall, maka semakin baik model pada kelas tertentu.\n\nPresisi\n\nPresisi mengukur sejauh mana, model dapat melakukan identifikasi dengan benar pada kelas tertentu.\nFormula atau rumus presisi sebagai berikut\n\n\n\nRecall\n\nRecall mengukur sejauh mana model dapat menemukan kembali kelas tertentu.\nFormula atau rumus dari recall\n\n\n\nPahami Code berikut\nPada dataset yang digunakan, dibagi menjadi 80:20 pada tahap split data. Selanjutnya menggunakan algoritma KNN dengan nilai K=3. selanjutnya model akan melakukan prediksi pada data uji dan presisi serta recall dari model menggunakan fungsi precision_score dan recall_score dengan parameter average=‘macro’\n# Impor library yang diperlukan\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_score, recall_score\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model K-Nearest Neighbors (KNN) dengan k=3\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Hitung presisi model\nprecision = precision_score(y_test, y_pred, average='macro')\n\n# Hitung recall model\nrecall = recall_score(y_test, y_pred, average='macro')\n\n# Tampilkan hasil presisi dan recall\nprint(\"Presisi model: {:.2f}\".format(precision))\nprint(\"Recall model: {:.2f}\".format(recall))\n\n\n\n9.2.3 F1-Score\n\nMerupakan metrik untuk menggabungkan presisi dan recall\nFormula F1-Score\nSemakin tinggi dari nilai F1-Score, maka semakin baik model pada kelas tersebut. #### Mengapa F1-Score penting? {.unnumbered}\nF1-Score cocok digunakan saat terdapat ketidakseimbangan kelas pada sebuah dataset.\nF1-Score memberikan bobot yang seimbang antara presisi dan recall.\n\n\nPahami Code berikut\nPada dataset yang digunakan, dibagi menjadi 80:20 pada tahap split data. Selanjutnya menggunakan algoritma KNN dengan nilai K=3. selanjutnya model tersebut melakukan prediksi pada data uji dan F1-score dari model dihitung menggunakan fungsi f1_score dengan parameter average=‘macro’\n# Impor library yang diperlukan\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model K-Nearest Neighbors (KNN) dengan k=3\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Hitung F1-score model\nf1score = f1_score(y_test, y_pred, average='macro')\n\n# Tampilkan hasil F1-score\nprint(\"F1-score model: {:.2f}\".format(f1score))\n\n\n\n9.2.4 Pahami Code berikut\n\nApa yang berbeda dari ketiga source code sebelumnya?\nBagaimana hasil nilai akurasi, presisi, recall, dan F1-Score?\n\n# Impor library yang diperlukan\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model K-Nearest Neighbors (KNN) dengan k=3\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Hitung akurasi model\naccuracy = accuracy_score(y_test, y_pred)\n\n# Hitung presisi model\nprecision = precision_score(y_test, y_pred, average='macro')\n\n# Hitung recall model\nrecall = recall_score(y_test, y_pred, average='macro')\n\n# Hitung F1-score model\nf1score = f1_score(y_test, y_pred, average='macro')\n\n# Tampilkan hasil akurasi, presisi, recall, dan F1-score\nprint(\"Akurasi model: {:.2f}%\".format(accuracy * 100))\nprint(\"Presisi model: {:.2f}\".format(precision))\nprint(\"Recall model: {:.2f}\".format(recall))\nprint(\"F1-score model: {:.2f}\".format(f1score))\n\n\n9.2.5 Kurva ROC dan AUC\n\nROC dan AUC merupakan metrik evaluasi model pada sebuah machine learning.\nROC atau Receiver Operating Characteristic merupakan grafik yang menggambarkan sebuah performa model pada berbagai threshold.\nAUC atau Area Under the Curve merupakan luas daerah di bawah kurva ROC yang menggambarkan performa dari keseluruhan model.\n\n\nKurva ROC\n\nROC Curve adalah grafik yang menggambarkan trade-off antara True Positive Rate (TPR) dan False Positive Rate (FPR)\nTPR adalah rasio data positif yang benar diprediksi oleh model, dibandingkan dengan total data positif\nFPR adalah rasio data negatif yang salah diprediksi sebagai positif oleh model, dibandingkan dengan total data negatif\nSemakin dekat kurva ROC ke sudut kiri atas, semakin baik performa model\n\n\n\nKurva AUC\n\nAUC adalah metrik evaluasi yang mengukur performa keseluruhan model\nAUC menghitung luas daerah di bawah kurva ROC\nNilai AUC berada dalam rentang 0 hingga 1, dengan nilai terbaik adalah\n\n\n\nPahami Code berikut\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model Logistic Regression\nmodel = LogisticRegression()\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Dapatkan probabilitas prediksi untuk kelas positif\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Hitung nilai AUC (Area Under the Curve)\nauc_score = roc_auc_score(y_test, y_prob)\n\n# Hitung nilai FPR (False Positive Rate) dan TPR (True Positive Rate) untuk kurva ROC\nfpr, tpr, _ = roc_curve(y_test, y_prob)\n\n# Plot kurva ROC\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n9.2.6 Amati dan pahami code berikut\n\nApa yang berbeda dari ketiga source code sebelumnya?\nBagaimana hasil dari code tersebut?\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n\n# Muat dataset Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Bagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Inisialisasi model Logistic Regression\nmodel = LogisticRegression()\n\n# Latih model menggunakan data latih\nmodel.fit(X_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = model.predict(X_test)\n\n# Menghitung akurasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Akurasi Model: {:.2f}\".format(accuracy))\n\n# Menghitung presisi model\nprecision = precision_score(y_test, y_pred, average='macro')\nprint(\"Presisi Model: {:.2f}\".format(precision))\n\n# Menghitung recall model\nrecall = recall_score(y_test, y_pred, average='macro')\nprint(\"Recall Model: {:.2f}\".format(recall))\n\n# Menghitung F1-score model\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"F1-Score Model: {:.2f}\".format(f1score))\n\n# Dapatkan probabilitas prediksi untuk kelas positif\ny_prob = model.predict_proba(X_test)\n\n# Hitung nilai AUC (Area Under the Curve) untuk setiap kelas\nauc_scores = []\nfor i in range(len(iris.target_names)):\n    auc_score = roc_auc_score(y_test == i, y_prob[:, i])\n    auc_scores.append(auc_score)\n    print(\"AUC Class {}: {:.2f}\".format(iris.target_names[i], auc_score))\n\n# Plot kurva ROC untuk setiap kelas\nplt.figure()\nfor i in range(len(iris.target_names)):\n    fpr, tpr, _ = roc_curve(y_test == i, y_prob[:, i])\n    plt.plot(fpr, tpr, lw=2, label='ROC curve Class {} (area = {:.2f})'.format(iris.target_names[i], auc_scores[i]))\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "modul_10.html#pendahuluan",
    "href": "modul_10.html#pendahuluan",
    "title": "10  Deployment",
    "section": "10.1 Pendahuluan",
    "text": "10.1 Pendahuluan\nDeployment model data science dilakukan untuk memastikan model yang dibuat dapat digunakan oleh orang lain. Deployment model data science dapat dilakukan dengan berbagai cara, salah satunya adalah dengan menggunakan Streamlit.  Streamlit adalah sebuah framework yang dapat digunakan untuk membuat aplikasi web dengan menggunakan bahasa pemrograman Python. Dengan menggunakan Streamlit, deployment model data science dapat dilakukan dengan mudah dan cepat."
  },
  {
    "objectID": "modul_10.html#instalasi-streamlit",
    "href": "modul_10.html#instalasi-streamlit",
    "title": "10  Deployment",
    "section": "10.2 Instalasi Streamlit",
    "text": "10.2 Instalasi Streamlit\n  Untuk menginstall Streamlit, dapat dilakukan dengan menggunakan perintah berikut pada command prompt atau terminal:\npip install streamlit"
  },
  {
    "objectID": "modul_10.html#membuat-aplikasi-web-dengan-streamlit",
    "href": "modul_10.html#membuat-aplikasi-web-dengan-streamlit",
    "title": "10  Deployment",
    "section": "10.3 Membuat Aplikasi Web dengan Streamlit",
    "text": "10.3 Membuat Aplikasi Web dengan Streamlit\nSetelah Streamlit terinstall, langkah selanjutnya adalah membuat aplikasi web dengan menggunakan Streamlit. Untuk membuat aplikasi web dengan Streamlit, dapat dilakukan dengan cara membuat file python baru dengan nama app.py. Kemudian, pada file tersebut, tuliskan kode berikut:\n\n# streamlit umumnya diinisialisasi dengan 'st'\nimport streamlit as st"
  },
  {
    "objectID": "modul_10.html#api-streamlit",
    "href": "modul_10.html#api-streamlit",
    "title": "10  Deployment",
    "section": "10.4 API Streamlit",
    "text": "10.4 API Streamlit\nAPI Streamlit dapat digunakan untuk membuat aplikasi web dengan Streamlit. API Streamlit dapat dilihat pada dokumentasi Streamlit. Berikut adalah beberapa API Streamlit yang dapat digunakan untuk membuat aplikasi web dengan Streamlit:\n\n10.4.1 Page Config\nst.set_page_config dapat digunakan untuk mengatur konfigurasi halaman. Beberapa konfigurasi yang dapat diatur adalah judul halaman, layout halaman, dan lain-lain. Berikut adalah contoh penggunaan st.set_page_config untuk mengatur judul halaman:\n\nst.set_page_config(\n    page_title=\"Estimasi Hotel Yogyakarta\",\n    page_icon=':hotel:' # :hotel: merupakan nama emoji\n)\n\n\n\n\n\n\n\n\n10.4.2 Write\nst.write dapat digunakan untuk menampilkan teks, dataframe, dan visualisasi. Format penulisan dalam method ini adalah format markdown. Berikut adalah contoh penggunaan st.write untuk menampilkan teks:\n\nst.title('Yogyakarta Hotel Price Estimation')\nst.write(\n    'For more info about this project, please visit my [**Github**](https://github.com/Liore-S/hotel-yoyakarta)')\n\n\n\n\n\n\n\n\n10.4.3 Sidebar\nst.sidebar dapat digunakan untuk membuat sidebar. Sidebar bisa digunakan sekedar untuk informasi tambahan atau bahka bisa sebagai input user. Berikut adalah contoh penggunaan st.sidebar untuk membuat sidebar:\n\nst.sidebar.header('User Input Features')\n\n\n\n10.4.4 Input User\nUntuk memasukkan elemen input user dalam sidebar dapat dilakukan dengan menggunakan st.sidebar.slider, st.sidebar.selectbox dan lain sebagainya . Berikut adalah contoh input data pada sidebar:\n\ndef user_input_features():\n    starRating = st.sidebar.slider('Star Rating', 0, 5, 3)\n    builtYear = st.sidebar.slider('Built Year', 1900, 2023, 1960)\n    size = st.sidebar.slider('Room Size (m2)', 2.0,\n                             100.0, 50.0, 0.1, format='%0.1f')\n    occupancy = st.sidebar.slider('Occupancy', 1, 5, 3)\n    childAge = st.sidebar.slider('Child Age', 0, 18, 9)\n    childOccupancy = st.sidebar.slider('Child Occupancy', 0, 5, 2)\n    breakfast = st.sidebar.checkbox('Breakfast Included')\n    wifi = st.sidebar.checkbox('Wifi Included')\n    refund = st.sidebar.checkbox('Free Cancellation / Refund')\n    livingRoom = st.sidebar.checkbox('Living Room')\n    hotelFacilitie = st.sidebar.multiselect(\n        'Hotel Facilities', (hotelFacilities))\n    roomFacilitie = st.sidebar.multiselect(\n        'Room Facilities', (roomFacilities))\n    pointInterest = st.sidebar.multiselect(\n        'Point of Interest', (nearestPoint))\n\n    # Handle Checkbox\n    breakfast = 1 if breakfast else 0\n    wifi = 1 if wifi else 0\n    refund = 1 if refund else 0\n    livingRoom = 1 if livingRoom else 0\n\n    # Handle Multiselect\n    hotelFacilitie = ','.join(hotelFacilitie)\n    roomFacilitie = ','.join(roomFacilitie)\n    pointInterest = ','.join(pointInterest)\n\n    data = {\n            'starRating': starRating,\n            'builtYear': builtYear,\n            'size': size,\n            'baseOccupancy': occupancy,\n            'maxChildAge': childAge,\n            'maxChildOccupancy': childOccupancy,\n            'isBreakfastIncluded': breakfast,\n            'isWifiIncluded': wifi,\n            'isRefundable': refund,\n            'hasLivingRoom': livingRoom,\n            'hotelFacilities': hotelFacilitie,\n            'roomFacilities': roomFacilitie,\n            'nearestPoint': pointInterest\n            }\n    features = pd.DataFrame(data, index=[0])\n    return features\n\nst.sidebar.header('User Input Features')\ndf = user_input_features()\n\n\nslider digunakan untuk memasukkan elemen input user berupa angka\ncheckbox digunakan untuk memasukkan elemen input user berupa boolean\nmultiselect digunakan untuk memasukkan elemen input user berupa list\n\n\n\n10.4.5 Handling Input User\nSetelah input user diterima, input user perlu diubah menjadi bentuk yang dapat digunakan oleh model. Untuk model ini perlu dilakukan multi-hot encoding untuk kolom fasilitas hotel, fasilitas kamar, dan point of interests, Berikut code untuk melakukan multi-hot encoding:\n\n# create function to create dataframe with 0 and 1 value\ndef create_df(dfOri, df_name, df, prefix):\n    value = prefix+dfOri[df_name][0]\n    for i in range(0, len(df.columns)):\n        column_name = df.columns[i]\n        if column_name in value:\n            df.loc[0, column_name] = 1\n        else:\n            df.loc[0, column_name] = 0\n    return df\n\n# create empty dataframe for hotelFacilities, roomFacilities, nearestPoint, with column name from hotelFacilities, roomFacilities, nearestPoint\nroomFacilities_df = pd.DataFrame(columns=roomFacilities)\nhotelFacilities_df = pd.DataFrame(columns=hotelFacilities)\nnearestPoint_df = pd.DataFrame(columns=nearestPoint)\n\ncreate_df(df, 'roomFacilities', roomFacilities_df, 'Room_')\ncreate_df(df, 'hotelFacilities', hotelFacilities_df, 'Hotel_')\ncreate_df(df, 'nearestPoint', nearestPoint_df, 'Point_')\n\ndf = df.drop(['hotelFacilities', 'roomFacilities', 'nearestPoint'], axis=1)\ndf = pd.concat([df, hotelFacilities_df, roomFacilities_df, nearestPoint_df], axis=1)\n\n# change all column data type to unit8 except the first column\ndf = df.astype({col: 'float64' for col in df.columns[:2]})\ndf = df.astype({col: 'uint8' for col in df.columns[2:]})\n\nCode diatas digunakan untuk membuat data input user menjadi sama pada data yang dilakukan training. Hal ini dilakukan agar model dapat melakukan prediksi dengan benar.\n\n\n10.4.6 Pengecekan Dataframe\nSetelah data input user diubah menjadi dataframe, perlu dilakukan pengecekan apakah dataframe tersebut sudah sesuai dengan dataframe yang digunakan untuk training. Berikut adalah code untuk mengecek dataframe:\n\n# check df column order with model column order using colOri, if not the same print the worng column\n# colOri merupakan kolom pada data training yang di export menggunaka pickle\ncolOri = colOri[1:]\nif df.columns.tolist() == colOri.all():\n    st.info(\"Column order is correct.\")\nelse:\n    mismatched_columns = [(idx, df_col, model_col) for idx, (df_col, model_col) in enumerate(zip(df.columns.tolist(), colOri)) if df_col != model_col]\n\n    if len(mismatched_columns) &gt; 0:\n        st.warning(\"The order of the columns is not the same as the model. Mismatched columns:\")\n        for idx, df_col, model_col in mismatched_columns:\n            st.write(f\"At index {idx}: DataFrame column '{df_col}' - Model column '{model_col}'\")\n\n\n\n\n\n\n\nWarning\n\n\n\nUrutan kolom hasil input user harus sama dengan urutan kolom pada data training. Jika tidak, maka akan terjadi error atau membuat hasil prediksi menjadi tidak valid.\n\n\n\n\n10.4.7 Prediksi\nUntuk melakukan prediksi, model perlu di-load terlebih dahulu. Berikut adalah code untuk melakukan prediksi:\n\n# Load Model\nxgbModel = pickle.load(open('Model/xgbModel.pkl', 'rb'))\nsvrModel = pickle.load(open('Model/svrModel.pkl', 'rb'))\nrfModel = pickle.load(open('Model/rfModel.pkl', 'rb'))\n\nDalam contoh ini digunakan 3 model, yaitu XGBoost, Support Vector Regression, dan Random Forest. Untuk melakukan prediksi, dapat dilakukan dengan menggunakan code berikut:\n\nst.write('Press button below to predict :')\nmodel = st.selectbox('Select Model', ('XGBoost', 'Random Forest', 'SVR'))\n\nif model == 'XGBoost' and st.button('Predict'):\n    bar = st.progress(0)\n    status_text = st.empty()\n    for i in range(1, 101):\n        status_text.text(\"%i%% Complete\" % i)\n        bar.progress(i)\n        time.sleep(0.01)\n\n    # Formatting the prediction\n    prediction = xgbModel.predict(df)\n    \n    formaString = \"Rp{:,.2f}\"\n    prediction = float(prediction[0])\n    formatted_prediction = formaString.format(prediction)\n    time.sleep(0.08)\n\n    # print the prediction\n    st.subheader('Prediction')\n    st.metric('Price (IDR)', formatted_prediction)\n\n    # empty the progress bar and status text\n    time.sleep(0.08)\n    bar.empty()\n    status_text.empty()\n   \nelif model == 'Random Forest' and st.button('Predict'):\n    bar = st.progress(0)\n    status_text = st.empty()\n    for i in range(1, 101):\n        status_text.text(\"%i%% Complete\" % i)\n        bar.progress(i)\n        time.sleep(0.01)\n\n    # Formatting the prediction\n    prediction = rfModel.predict(df)\n    \n    formaString = \"Rp{:,.2f}\"\n    prediction = float(prediction[0])\n    formatted_prediction = formaString.format(prediction)\n    time.sleep(0.08)\n\n    # print the prediction\n    st.subheader('Prediction')\n    st.metric('Price (IDR)', formatted_prediction)\n\n    # empty the progress bar and status text\n    time.sleep(0.08)\n    bar.empty()\n    status_text.empty()\n    \nelif model == 'SVR' and st.button('Predict'):\n    bar = st.progress(0)\n    status_text = st.empty()\n    for i in range(1, 101):\n        status_text.text(\"%i%% Complete\" % i)\n        bar.progress(i)\n        time.sleep(0.01)\n\n    # Formatting the prediction\n    prediction = svrModel.predict(df)\n    \n    formaString = \"Rp{:,.2f}\"\n    prediction = float(prediction[0])\n    formatted_prediction = formaString.format(prediction)\n    # prediction = rfModel.predict(df)\n    time.sleep(0.08)\n\n    # print the prediction\n    st.subheader('Prediction')\n    st.metric('Price (IDR)', formatted_prediction)\n\n    # empty the progress bar and status text\n    time.sleep(0.08)\n    bar.empty()\n    status_text.empty()\n\n\n\n10.4.8 Hasil akhir\nUnutuk contoh hasil akhir dapat dilihat pada link berikut: Sreamlit hotel Yogyakarta"
  },
  {
    "objectID": "project_regresi.html#deskripsi",
    "href": "project_regresi.html#deskripsi",
    "title": "13  Project Regresi Hotel Yogyakarta",
    "section": "13.1 Deskripsi",
    "text": "13.1 Deskripsi\nProject ini merupakan contoh project Data Science yang menggunakan data hotel di Yogyakarta. Project ini bertujuan untuk memprediksi harga kamar hotel berdasarkan fitur-fitur yang ada. Project ini menggunakan 3 algoritma yaitu, XGBoost, Random Forest, dan SVM. Project ini juga menggunakan teknik hyperparameter tuning untuk meningkatkan performa model. Kemudian model tersebut dilakukan deployment menggunakan streamlit"
  },
  {
    "objectID": "project_regresi.html#tentang-data",
    "href": "project_regresi.html#tentang-data",
    "title": "13  Project Regresi Hotel Yogyakarta",
    "section": "13.2 Tentang data",
    "text": "13.2 Tentang data\nData ini diperoleh dengan teknik scrapping pada website Traveloka. Data ini berbentuk sqlite yang berisikan 2 tabel bernama hotel_yogyakarta dan hotel_room_yogyakarta\n\n13.2.1 hotel_yogyakarta\nBerikut detail column pada tabel hotel_yogyakarta. Dimensi (378, 12)\n\nid: Unique id hotel\ntype: Tipe penginapan\nname: Nama hotel\nstarRating: Rating bintang hotel\nbuiltYear: Tahun dibuatnya hotel\ndescription: Deskripsi tentang hotel\nlink: URL menuju halaman hotel di Traveloka\naddress: Alamat hotel\ncity: Kota hotel\nimage: URL gambar hotel\nfacilities: Daftar fasilitas pada hotel\nnearestPointofInterests: Area populer / fasilitas umum disekitar hotel\n\n\n\n13.2.2 hotel_room_yogyakarta\nBerikut detail column pada tabel hotel_room_yogyakarta. Dimensi (1199, 16)\n\nid: Unique id hotel\nhotelId: Id hotel\nroomType: Tipe kamar hotel\ndescription: deskripsi kamar hotel\nbedDescription: deskripsi kasur kamar\nsize: Ukuran kamar (\\(m^2\\))\noriginalRate: Harga kamar per malam\nbaseOccupancy: Kapasitas kamar\nmaxChildAge: Umur maksimal anak-anak\nmaxChildOccupancy: Kapasitas kamar untuk anak-anak\nnumExtraBeds: Jumlah kasur tambahan\nisBreakfastIncluded: Fasilitas sarapan\nisWifiIncluded: Fasilitas WiFi\nisRefundable: Fasilitas refund\nhasLivingRoom: Fasilitas ruang keluarga\nfacilities: Daftar fasilitas lainnya pada kamar"
  },
  {
    "objectID": "project_regresi.html#data-analysis",
    "href": "project_regresi.html#data-analysis",
    "title": "13  Project Regresi Hotel Yogyakarta",
    "section": "13.3 Data analysis",
    "text": "13.3 Data analysis\n\n13.3.1 Import library\n\nfrom sqlite3 import connect\nimport pickle\nimport pandas as pd\nimport json\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\n\n13.3.2 Load data\nBuat koneksi ke database sqlite, lalu baca tabel hotel_yogyakarta danhotel_room_yogyakarta menjadi dataframe pandas.\n\n# SQLite3 connection\ncon = connect('./dataset/hotel-directories-ORI.sqlite3')\ndf_sql_hotel = pd.read_sql_query(\"SELECT * from hotel_yogyakarta\", con=con)\ndf_sql_room = pd.read_sql_query(\"SELECT * from hotel_room_yogyakarta\", con=con)\ncon.close()\n\n# Table columns\nprint('Kolom Tabel Hotel :')\nprint(df_sql_hotel.columns)\nprint(\"Total Baris :\", df_sql_hotel.shape[0])\nprint(\"Total Kolom :\", df_sql_hotel.shape[1])\n\nprint('-' * 50)\n\nprint('Kolom Tabel Kamar:')\nprint(df_sql_room.columns)\nprint(\"Total Baris :\", df_sql_room.shape[0])\nprint(\"Total Kolom :\", df_sql_room.shape[1])\n\nKolom Tabel Hotel :\nIndex(['id', 'type', 'name', 'starRating', 'builtYear', 'description', 'link',\n       'address', 'city', 'image', 'facilities', 'nearestPointOfInterests'],\n      dtype='object')\nTotal Baris : 378\nTotal Kolom : 12\n--------------------------------------------------\nKolom Tabel Kamar:\nIndex(['id', 'hotelId', 'roomType', 'description', 'bedDescription', 'size',\n       'originalRate', 'baseOccupancy', 'maxChildAge', 'maxChildOccupancy',\n       'numExtraBeds', 'isBreakfastIncluded', 'isWifiIncluded', 'isRefundable',\n       'hasLivingRoom', 'facilities'],\n      dtype='object')\nTotal Baris : 1199\nTotal Kolom : 16\n\n\nUntuk melihat hasil dataframe dapat dilakukan menggunakan code berikut.\n\nprint(f\"Dimensi : {df_sql_hotel.shape}\")\ndf_sql_hotel.sample(2)\n\nDimensi : (378, 12)\n\n\n\n\n\n\n\n\n\nid\ntype\nname\nstarRating\nbuiltYear\ndescription\nlink\naddress\ncity\nimage\nfacilities\nnearestPointOfInterests\n\n\n\n\n195\n3000010034920\nHotel\nHotel Cahaya Kasih\n0.0\n1996\n&lt;p&gt;&lt;b&gt;Lokasi&lt;/b&gt;&lt;br&gt;Hotel Cahaya Kasih adalah ...\nhttps://www.traveloka.com/id-id/hotel/detail?s...\nJalan Malioboro, Sosromenduran GT.I/280, Jalan...\nYogyakarta\nhttps://ik.imagekit.io/tvlk/apr-asset/dgXfoyh2...\n[\"WIFI_PUBLIC_AREA\",\"WIFI_FREE\",\"AIR_CONDITION...\n[{\"landmarkId\":\"91516349005487\",\"geoId\":null,\"...\n\n\n28\n3000010014429\nHotel\nLafayette Boutique Hotel\n5.0\n2015\n&lt;p&gt;&lt;b&gt;Lokasi&lt;/b&gt;&lt;br&gt;Lafayette Boutique Hotel b...\nhttps://www.traveloka.com/id-id/hotel/detail?s...\nJalan Ring Road Utara, No 409, Manggung, Catur...\nYogyakarta\nhttps://ik.imagekit.io/tvlk/apr-asset/dgXfoyh2...\n[\"CARPARK\",\"ELEVATOR\",\"LATE_CHECKOUT\",\"RESTAUR...\n[{\"landmarkId\":\"91607407537252\",\"geoId\":null,\"...\n\n\n\n\n\n\n\n\nprint(f\"Dimensi : {df_sql_room.shape}\")\ndf_sql_room.sample(2)\n\nDimensi : (1199, 16)\n\n\n\n\n\n\n\n\n\nid\nhotelId\nroomType\ndescription\nbedDescription\nsize\noriginalRate\nbaseOccupancy\nmaxChildAge\nmaxChildOccupancy\nnumExtraBeds\nisBreakfastIncluded\nisWifiIncluded\nisRefundable\nhasLivingRoom\nfacilities\n\n\n\n\n1146\n1000110288\n3000020012907\nFamily\nNone\nNone\n35.0\n{\"amount\":\"371901\",\"currency\":\"IDR\",\"tax\":\"780...\n4\n10\n0\n0\n1\n1\n0\n0\n[\"AIR_CONDITIONING\",\"COMPLIMENTARY_BOTTLED_WAT...\n\n\n442\n1000128391\n3000010004867\nSuite Twin\n&lt;p&gt;Suite Room Twin Beds&lt;/p&gt;\nNone\n35.0\n{\"amount\":\"583227\",\"currency\":\"IDR\",\"tax\":\"122...\n2\n8\n1\n0\n0\n1\n1\n0\n[\"AIR_CONDITIONING\",\"BATHROBES\",\"BLACKOUT_DRAP...\n\n\n\n\n\n\n\n\n# Fungsi menghitung unique value\ndef check_unique(df):\n    count = 0\n    for i in df.columns:\n        if df[i].nunique() == 1:\n            count += 1\n            print(f'{i}: {df[i].nunique()}')\n        else:\n            print(f'{i}: {df[i].nunique()}')\n    if count == 0:\n        print('No columns with only one unique value')\n\nGunakan fungsi check_unique() untuk mengecek apakah terdapat data dengan unique value kurang dari 2. Jika ada, maka data tersebut tidak akan digunakan.\n\ncheck_unique(df_sql_hotel)\n\nid: 378\ntype: 1\nname: 377\nstarRating: 8\nbuiltYear: 38\ndescription: 378\nlink: 378\naddress: 377\ncity: 1\nimage: 377\nfacilities: 372\nnearestPointOfInterests: 375\n\n\n\ncheck_unique(df_sql_room)\n\nid: 1199\nhotelId: 375\nroomType: 423\ndescription: 544\nbedDescription: 0\nsize: 87\noriginalRate: 714\nbaseOccupancy: 9\nmaxChildAge: 13\nmaxChildOccupancy: 5\nnumExtraBeds: 1\nisBreakfastIncluded: 2\nisWifiIncluded: 2\nisRefundable: 2\nhasLivingRoom: 2\nfacilities: 600\n\n\n\nNilai penghubung kedua tabel adalah id pada data hotel dan hotelId pada data kamar.\nTerdapat beberapa beberapa kolom yang tidak digunakan pada analisis ini\n\nHotel: name, description, link, address, dan image.\nKamar: id, roomType, description, dan bedDescription.\n\nTerdapat beberapa kolom dengan total nilai unik kurang dari 2\n\nHotel: type dan city.\nKamar: bedDescription dan numExtraBeds\n\nSetelah dilakukan penghapusan kolom selanjutnya tabel akan di-merge menjadi satu dataframe.\nstarRating memiliki 8 nilai unik, perlu di teliti lebih lanjut untuk detailnya.\n\n\n\n13.3.3 Menghapus Kolom\n\nhotelDrop = ['name', 'description', 'link', 'address', 'image', 'type', 'city']\nroomDrop = ['id', 'roomType', 'description', 'bedDescription', 'numExtraBeds']\n\ndf_hotel = df_sql_hotel.drop(hotelDrop, axis=1)\ndf_room = df_sql_room.drop(roomDrop, axis=1)\n\nprint('Total Hotel Table Data : ', df_hotel.shape[0])\nprint('Total Hotel Table Column : ', df_hotel.shape[1])\nprint('-' * 30)\nprint('Total Room Table Data : ', df_room.shape[0])\nprint('Total Room Table Column : ', df_room.shape[1])\n\nTotal Hotel Table Data :  378\nTotal Hotel Table Column :  5\n------------------------------\nTotal Room Table Data :  1199\nTotal Room Table Column :  11\n\n\n\n\n13.3.4 Merge Data\n\n# Rename column\ndf_hotel.rename(columns={'id': 'hotelId'}, inplace=True)\ndf_hotel.rename(columns={'facilities': 'hotelFacilities'}, inplace=True)\ndf_room.rename(columns={'facilities': 'roomFacilities'}, inplace=True)\n\n\nMenyamakan nama id pada tabel hotel dan hotelId pada tabel kamar.\nMenambah prefix hotel dan room pada tiap kolom facilities masing-masing tabel.\n\n\n# merge hotel dan room data\ndf = pd.merge(df_hotel, df_room, on='hotelId', how='inner')\n\n# remove id column\ndf.drop(columns=['hotelId'], inplace=True)\n\n# re arrange column\ndf = df[['originalRate', 'starRating', 'builtYear', 'size', 'baseOccupancy', 'maxChildAge',\n         'maxChildOccupancy', 'isBreakfastIncluded', 'isWifiIncluded', 'isRefundable',\n         'hasLivingRoom', 'hotelFacilities', 'roomFacilities', 'nearestPointOfInterests']]\n\ndf\n\n\n\n\n\n\n\n\noriginalRate\nstarRating\nbuiltYear\nsize\nbaseOccupancy\nmaxChildAge\nmaxChildOccupancy\nisBreakfastIncluded\nisWifiIncluded\nisRefundable\nhasLivingRoom\nhotelFacilities\nroomFacilities\nnearestPointOfInterests\n\n\n\n\n0\n{\"amount\":\"1008264\",\"currency\":\"IDR\",\"tax\":\"21...\n5.0\n2013\n40.0\n2\n8\n1\n1\n1\n0\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\n[{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\n\n\n1\n{\"amount\":\"1049587\",\"currency\":\"IDR\",\"tax\":\"22...\n5.0\n2013\n40.0\n2\n8\n1\n1\n1\n0\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\n[{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\n\n\n2\n{\"amount\":\"1049587\",\"currency\":\"IDR\",\"tax\":\"22...\n5.0\n2013\n40.0\n2\n8\n1\n1\n1\n0\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\n[{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\n\n\n3\n{\"amount\":\"1842975\",\"currency\":\"IDR\",\"tax\":\"38...\n5.0\n2013\n60.0\n2\n8\n0\n1\n1\n0\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\n[{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\n\n\n4\n{\"amount\":\"2396694\",\"currency\":\"IDR\",\"tax\":\"50...\n5.0\n2013\n80.0\n2\n8\n0\n1\n1\n0\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\n[{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1194\n{\"amount\":\"308540\",\"currency\":\"IDR\",\"tax\":\"647...\n1.0\nNone\n15.0\n2\n12\n1\n0\n1\n0\n0\n[\"CARPARK\",\"WIFI_PUBLIC_AREA\",\"LAN_INTERNET_AC...\n[\"AIR_CONDITIONING\",\"DESK\",\"SHOWER\",\"TELEVISION\"]\n[{\"landmarkId\":\"900000001117\",\"geoId\":null,\"na...\n\n\n1195\n{\"amount\":\"220385\",\"currency\":\"IDR\",\"tax\":\"462...\n1.0\nNone\n16.0\n2\n12\n1\n0\n1\n0\n0\n[\"CARPARK\",\"WIFI_PUBLIC_AREA\",\"LAN_INTERNET_AC...\n[\"AIR_CONDITIONING\",\"DESK\",\"SHOWER\",\"TELEVISION\"]\n[{\"landmarkId\":\"900000001117\",\"geoId\":null,\"na...\n\n\n1196\n{\"amount\":\"296425\",\"currency\":\"IDR\",\"tax\":\"622...\n2.0\n2013\n25.0\n2\n10\n1\n0\n1\n1\n0\n[\"CARPARK\",\"COFFEE_OR_TEA_IN_LOBBY\",\"SAFETY_DE...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"COFFEE_...\n[{\"landmarkId\":\"91607407537252\",\"geoId\":null,\"...\n\n\n1197\n{\"amount\":\"994485\",\"currency\":\"IDR\",\"tax\":\"208...\n2.0\n2013\n60.0\n6\n10\n1\n0\n1\n1\n0\n[\"CARPARK\",\"COFFEE_OR_TEA_IN_LOBBY\",\"SAFETY_DE...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"COFFEE_...\n[{\"landmarkId\":\"91607407537252\",\"geoId\":null,\"...\n\n\n1198\n{\"amount\":\"220385\",\"currency\":\"IDR\",\"tax\":\"462...\n0.0\nNone\n16.0\n2\n12\n1\n0\n1\n0\n0\n[\"CARPARK\",\"HAS_24_HOUR_ROOM_SERVICE\",\"ROOM_SE...\n[\"AIR_CONDITIONING\",\"NON_SMOKING_ROOMS\",\"SEATI...\n[{\"landmarkId\":\"900000001117\",\"geoId\":null,\"na...\n\n\n\n\n1199 rows × 14 columns\n\n\n\n\nDimensi pasca penggabungan adalah (1199, 14)\nMenghapus kolom hotelId karena sudah tidak diperlukan lagi.\nMengubah urutan kolom untuk mempermudah analisis, target kolom berada di kiri dan kolom array berada di kanan.\n\n\n\n13.3.5 Target Processing\n\nfor i in range(0, 5):\n    print(df.loc[i, 'originalRate'])\n\n{\"amount\":\"1008264\",\"currency\":\"IDR\",\"tax\":\"211736\"}\n{\"amount\":\"1049587\",\"currency\":\"IDR\",\"tax\":\"220413\"}\n{\"amount\":\"1049587\",\"currency\":\"IDR\",\"tax\":\"220413\"}\n{\"amount\":\"1842975\",\"currency\":\"IDR\",\"tax\":\"387025\"}\n{\"amount\":\"2396694\",\"currency\":\"IDR\",\"tax\":\"503306\"}\n\n\n\nData berbentuk JSON atau Dictionary, maka perlu diubah menjadi nilai harga dengan 1 nilai.\nKarena semua data menggunakan matauang IDR, maka currency tidak diperlukan.\n\n\n# Extract the amount from originalRate using a lambda function\ndf['rate'] = df['originalRate'].apply(lambda x: json.loads(x)['amount'])\ndf['tax'] = df['originalRate'].apply(lambda x: json.loads(x)['tax'])\ndf = df.drop(columns=['originalRate'])\n\n\n# check tipe data\nprint('Tipe data harga :', df['rate'].dtype)\nprint('Tipe data pajak :', df['tax'].dtype)\n\nTipe data harga : object\nTipe data pajak : object\n\n\n\n# ubah tipe data\ndf['rate'] = df['rate'].astype('int')\ndf['tax'] = df['tax'].astype('int')\n\nprint('Tipe data harga :', df['rate'].dtype)\nprint('Tipe data pajak :', df['tax'].dtype)\n\nTipe data harga : int32\nTipe data pajak : int32\n\n\n\n13.3.5.1 Rasio Pajak\nTahap ini bertujuan untuk mengetahui rasio pajak dari harga kamar hotel. Rasio pajak ini akan digunakan untuk menghitung harga kamar hotel setelah dikenakan pajak.\n\n# create series for original rate\noriginal_rate = df['rate']\n# create series for tax\ntax = df['tax']\n# create dataframe for original rate, tax, and tax rate\ndf_rate = pd.DataFrame({'original_rate': original_rate, 'tax': tax})\ndf_rate['tax_rate'] = df_rate['tax'] / df_rate['original_rate'] * 100\n\ndf_rate\n\n\n\n\n\n\n\n\noriginal_rate\ntax\ntax_rate\n\n\n\n\n0\n1008264\n211736\n21.000056\n\n\n1\n1049587\n220413\n20.999974\n\n\n2\n1049587\n220413\n20.999974\n\n\n3\n1842975\n387025\n21.000014\n\n\n4\n2396694\n503306\n21.000011\n\n\n...\n...\n...\n...\n\n\n1194\n308540\n64793\n20.999870\n\n\n1195\n220385\n46282\n21.000522\n\n\n1196\n296425\n62249\n20.999916\n\n\n1197\n994485\n208842\n21.000015\n\n\n1198\n220385\n46282\n21.000522\n\n\n\n\n1199 rows × 3 columns\n\n\n\n\n# count number of data with tax rate 20% and under 21%, also over 21%\ncount0 = 0\ncount_20 = 0\ncountBetween = 0\nfor i in range(len(df_rate['tax_rate'])):\n    if df_rate['tax_rate'][i] &lt; 20 and df_rate['tax_rate'][i] &gt; 0:\n        countBetween += 1\n    elif df_rate['tax_rate'][i] &gt;= 20:\n        count_20 += 1\n    elif df_rate['tax_rate'][i] == 0:\n        count0 += 1\nprint('Median pajak : ', df_rate['tax_rate'].median())\nprint('null / 0% pajak: ', count0)\nprint('Pajak diantara 0 - 20%:', countBetween)\nprint('Rasio pajak diatas 20% :', count_20)\n\n\n# plot for tax rate and give the total value on the top of the bar if the value is 0% it will not show\nsns.set(rc={'figure.figsize': (10, 7)})\nsns.set_style('darkgrid')\nax = sns.histplot(df_rate['tax_rate'], kde=False, color='dodgerblue', bins=9)\nax.set(xlabel='Tax Rate (%)', ylabel='Count')\nax.set_title('Tax Rate Distribution')\ntotal = len(df_rate['tax_rate'])\nfor p in ax.patches:\n    height = p.get_height()\n    if height != 0:\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 15,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\")\nplt.show()\n\nMedian pajak :  20.99999233666715\nnull / 0% pajak:  63\nPajak diantara 0 - 20%: 8\nRasio pajak diatas 20% : 1128\n\n\n\n\n\n\n# Menghapus kolom pajak\ndf = df.drop(columns=['tax'])\n\n# Mengubah target kolom menjadi di awal\n# sekedar untuk merapikan dataframe\ndf = df[['rate'] + [col for col in df.columns if col != 'rate']]\ndf.columns\n\nIndex(['rate', 'starRating', 'builtYear', 'size', 'baseOccupancy',\n       'maxChildAge', 'maxChildOccupancy', 'isBreakfastIncluded',\n       'isWifiIncluded', 'isRefundable', 'hasLivingRoom', 'hotelFacilities',\n       'roomFacilities', 'nearestPointOfInterests'],\n      dtype='object')\n\n\n\nPajak hotel di Yogyakarta ada di kisaran 20-22% dengan median 21%\nKarena 94% data memiliki pajak dikisaran tersebut maka nilai pajak dianggap 21%(median) secara keseluruhan.\n\n\n\n\n13.3.6 Validaasi Data\n\n13.3.6.1 Pengecekan Rating Bintang\n\n# starRating Distribution\nvalue = df.starRating.value_counts()\nprint('OriginalRate Distribution by starRating')\nprint(value)\n\n# starRating Distribution by percentage\nvalue_percentage = value / len(df) * 100\n\n# create a list of tuples where each tuple contains the value and index of each element in the value_percentage series\nvalue_percentage_list = [(value_percentage[i], i)\n                         for i in value_percentage.index]\n\n# sort the list by the value in descending order\nvalue_percentage_list_sorted = sorted(value_percentage_list, reverse=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10), dpi=80,\n                       gridspec_kw={'width_ratios': [1, 0.5]})\nsns.histplot(df, x=\"rate\", hue='starRating', palette='bright',\n             ax=ax[0]).set(title='OriginalRate Distribution by starRating')\n\n# starRating percentage plot\nsns.barplot(x=value_percentage.index, y=value_percentage.values,\n            palette='bright', ax=ax[1]).set(title='starRating percentage')\n\n# add the percentage text using the sorted list\nfor container in ax[1].containers:\n    for bar in container.patches:\n        v = bar.get_height()\n        bar_center = bar.get_x() + bar.get_width() / 2\n        ax[1].text(bar_center, v + 0.5,\n                   f'{v:.2f}%', color='black', fontweight='bold', ha='center')\nfig.tight_layout()\n\nOriginalRate Distribution by starRating\nstarRating\n0.0    332\n3.0    240\n4.0    204\n2.0    160\n1.0    153\n5.0     92\n2.5     12\n3.5      6\nName: count, dtype: int64\n\n\n\n\n\n\nRating bintang memiliki beberapa nilai dengan 0.5 (desimal), tetapi nilai tersebut hanya memiliki persentase jumlah data yang sedikit, maka dari itu rating tersebut dihilangkan angka desimalnya dari rating seharusnya.\n\n\n# ubah starRating dengan angka bulat\ndf['starRating'] = df['starRating'].replace(2.5, 2)\ndf['starRating'] = df['starRating'].replace(3.5, 3)\n\n\n# starRating Distribution\nvalue = df.starRating.value_counts()\nprint('OriginalRate Distribution by starRating')\nprint(value)\n\n# starRating Distribution by percentage\nvalue_percentage = value / len(df) * 100\n\n# create a list of tuples where each tuple contains the value and index of each element in the value_percentage series\nvalue_percentage_list = [(value_percentage[i], i)\n                         for i in value_percentage.index]\n\n# sort the list by the value in descending order\nvalue_percentage_list_sorted = sorted(value_percentage_list, reverse=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10), dpi=80,\n                       gridspec_kw={'width_ratios': [1, 0.5]})\nsns.histplot(df, x=\"rate\", hue='starRating', palette='bright',\n             ax=ax[0]).set(title='OriginalRate Distribution by starRating')\n\n# starRating percentage plot\nsns.barplot(x=value_percentage.index, y=value_percentage.values,\n            palette='bright', ax=ax[1]).set(title='starRating percentage')\n\n# add the percentage text using the sorted list\nfor container in ax[1].containers:\n    for bar in container.patches:\n        v = bar.get_height()\n        bar_center = bar.get_x() + bar.get_width() / 2\n        ax[1].text(bar_center, v + 0.5,\n                   f'{v:.2f}%', color='black', fontweight='bold', ha='center')\nfig.tight_layout()\n\nOriginalRate Distribution by starRating\nstarRating\n0.0    332\n3.0    246\n4.0    204\n2.0    172\n1.0    153\n5.0     92\nName: count, dtype: int64\n\n\n\n\n\n\nPersentase persebaran data tiap rating bintang sudah lebih baik setelah dilakukan pengubahan nilai rating bintang.\nTerlihat pada plot yang kiri bahwa terdapat ekor yang sangat panjang, ini menunjukkan adanya outlier pada kolom harga(rate)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1199 entries, 0 to 1198\nData columns (total 14 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   rate                     1199 non-null   int32  \n 1   starRating               1199 non-null   float64\n 2   builtYear                878 non-null    object \n 3   size                     1064 non-null   object \n 4   baseOccupancy            1199 non-null   int64  \n 5   maxChildAge              1199 non-null   int64  \n 6   maxChildOccupancy        1199 non-null   int64  \n 7   isBreakfastIncluded      1199 non-null   int64  \n 8   isWifiIncluded           1199 non-null   int64  \n 9   isRefundable             1199 non-null   int64  \n 10  hasLivingRoom            1199 non-null   int64  \n 11  hotelFacilities          1199 non-null   object \n 12  roomFacilities           1199 non-null   object \n 13  nearestPointOfInterests  1199 non-null   object \ndtypes: float64(1), int32(1), int64(7), object(5)\nmemory usage: 126.6+ KB\n\n\n\nKolom starRating masih bertipe data float walau sudah tidak memiliki angka desimal, maka perlu akan menjadi int\nKolom builtYear harus diganti ke tipe data int\nKolom size harus diganti ke tipe data float (tipe data dasar dari tabel sqlite adalah float)\nTerdapat nilai null pada kolom builtYear dan size yang harus ditangani\n\n\ndf['starRating'] = df['starRating'].astype('int')\nprint('Tipe data starRating :', df['starRating'].dtype)\n\nTipe data starRating : int32\n\n\n\n\n13.3.6.2 Data Cleaning\n\n13.3.6.2.1 Check Duplicate Data\n\n# show index who has duplicate value\nprint('Total duplicated row = ', df.duplicated().sum())\n# print duplicated data list index 1\ndf[df.duplicated(keep=False)]\n\nTotal duplicated row =  89\n\n\n\n\n\n\n\n\n\nrate\nstarRating\nbuiltYear\nsize\nbaseOccupancy\nmaxChildAge\nmaxChildOccupancy\nisBreakfastIncluded\nisWifiIncluded\nisRefundable\nhasLivingRoom\nhotelFacilities\nroomFacilities\nnearestPointOfInterests\n\n\n\n\n1\n1049587\n5\n2013\n40.0\n2\n8\n1\n1\n1\n0\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\n[{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\n\n\n2\n1049587\n5\n2013\n40.0\n2\n8\n1\n1\n1\n0\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\n[{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\n\n\n6\n6000000\n5\n2012\n105.0\n2\n6\n2\n1\n1\n1\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BATHROBES\",\"BATHTUB\",\"COF...\n[{\"landmarkId\":\"91607408097208\",\"geoId\":null,\"...\n\n\n7\n6000000\n5\n2012\n105.0\n2\n6\n2\n1\n1\n1\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BATHROBES\",\"BATHTUB\",\"COF...\n[{\"landmarkId\":\"91607408097208\",\"geoId\":null,\"...\n\n\n8\n1933333\n5\n2012\n40.0\n2\n6\n2\n1\n1\n1\n0\n[\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n[\"AIR_CONDITIONING\",\"BATHROBES\",\"COMPLIMENTARY...\n[{\"landmarkId\":\"91607408097208\",\"geoId\":null,\"...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1169\n141273\n0\nNone\nNone\n2\n5\n0\n0\n0\n1\n0\n[\"HAS_24_HOUR_FRONT_DESK\"]\n[]\n[{\"landmarkId\":\"91607407537252\",\"geoId\":null,\"...\n\n\n1173\n550964\n0\nNone\nNone\n2\n5\n0\n0\n0\n1\n0\n[]\n[]\n[{\"landmarkId\":\"91607407802715\",\"geoId\":null,\"...\n\n\n1174\n550964\n0\nNone\nNone\n2\n5\n0\n0\n0\n1\n0\n[]\n[]\n[{\"landmarkId\":\"91607407802715\",\"geoId\":null,\"...\n\n\n1189\n413223\n1\nNone\n9.0\n2\n10\n0\n0\n1\n1\n0\n[\"WIFI_PUBLIC_AREA\",\"ACCESS_FRIENDLY\",\"ACCESSI...\n[\"AIR_CONDITIONING\",\"DESK\",\"NON_SMOKING_ROOMS\"...\n[{\"landmarkId\":\"91607407537252\",\"geoId\":null,\"...\n\n\n1190\n413223\n1\nNone\n9.0\n2\n10\n0\n0\n1\n1\n0\n[\"WIFI_PUBLIC_AREA\",\"ACCESS_FRIENDLY\",\"ACCESSI...\n[\"AIR_CONDITIONING\",\"DESK\",\"NON_SMOKING_ROOMS\"...\n[{\"landmarkId\":\"91607407537252\",\"geoId\":null,\"...\n\n\n\n\n177 rows × 14 columns\n\n\n\n\n# drop duplicate data\ndf = df.drop_duplicates(keep='first')\ndf.shape\n\n(1110, 14)\n\n\n\nDengan menggunakan parameter keep = 'first' maka data yang duplikat akan dihapus kecuali data pertama yang muncul.\n\n\n\n13.3.6.2.2 Check Null\n\n# Jumlah baris data\njumlah_baris_ori = df.shape[0]\n\n# crate dataframe for null value\ndf_null = pd.DataFrame(df.isnull().sum(), columns=['null_value'])\ndf_null['null_value_percentage'] = df_null['null_value'] / len(df) * 100\ndf_null\n\n\n\n\n\n\n\n\nnull_value\nnull_value_percentage\n\n\n\n\nrate\n0\n0.000000\n\n\nstarRating\n0\n0.000000\n\n\nbuiltYear\n305\n27.477477\n\n\nsize\n121\n10.900901\n\n\nbaseOccupancy\n0\n0.000000\n\n\nmaxChildAge\n0\n0.000000\n\n\nmaxChildOccupancy\n0\n0.000000\n\n\nisBreakfastIncluded\n0\n0.000000\n\n\nisWifiIncluded\n0\n0.000000\n\n\nisRefundable\n0\n0.000000\n\n\nhasLivingRoom\n0\n0.000000\n\n\nhotelFacilities\n0\n0.000000\n\n\nroomFacilities\n0\n0.000000\n\n\nnearestPointOfInterests\n0\n0.000000\n\n\n\n\n\n\n\n\nTerdapat 2 data yang memiliki nilai null dengan persentase yang cukup tinggi, yaitu kolom builtYear dan size. Oleh karena itu data tersebut akan diubah dengan nilai median per rating hotel.\n\n\n# create new dataframe for null value rows\ndf_null_rows = df[df.isnull().any(axis=1)]\ndf_null_rows\n\n\n\n\n\n\n\n\nrate\nstarRating\nbuiltYear\nsize\nbaseOccupancy\nmaxChildAge\nmaxChildOccupancy\nisBreakfastIncluded\nisWifiIncluded\nisRefundable\nhasLivingRoom\nhotelFacilities\nroomFacilities\nnearestPointOfInterests\n\n\n\n\n41\n716253\n4\nNone\n28.0\n2\n5\n0\n0\n1\n0\n0\n[\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURANT_FOR_BREA...\n[\"AIR_CONDITIONING\",\"BLACKOUT_DRAPES_CURTAINS\"...\n[{\"landmarkId\":\"91510822038476\",\"geoId\":null,\"...\n\n\n42\n826447\n4\nNone\n28.0\n2\n5\n0\n0\n1\n0\n0\n[\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURANT_FOR_BREA...\n[\"AIR_CONDITIONING\",\"BLACKOUT_DRAPES_CURTAINS\"...\n[{\"landmarkId\":\"91510822038476\",\"geoId\":null,\"...\n\n\n43\n936639\n4\nNone\n35.0\n2\n5\n0\n0\n1\n0\n0\n[\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURANT_FOR_BREA...\n[\"AIR_CONDITIONING\",\"BLACKOUT_DRAPES_CURTAINS\"...\n[{\"landmarkId\":\"91510822038476\",\"geoId\":null,\"...\n\n\n44\n1212121\n4\nNone\n28.0\n2\n5\n0\n0\n1\n0\n0\n[\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURANT_FOR_BREA...\n[\"AIR_CONDITIONING\",\"BLACKOUT_DRAPES_CURTAINS\"...\n[{\"landmarkId\":\"91510822038476\",\"geoId\":null,\"...\n\n\n60\n846281\n4\n2018\nNone\n1\n0\n0\n0\n1\n0\n0\n[\"CARPARK\",\"ELEVATOR\",\"HAS_24_HOUR_ROOM_SERVIC...\n[\"TOWELS_PROVIDED\",\"NON_SMOKING_ROOMS\",\"LINENS...\n[{\"landmarkId\":\"91575379594159\",\"geoId\":null,\"...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1192\n187328\n1\nNone\n12.0\n2\n12\n1\n0\n1\n0\n0\n[\"CARPARK\",\"WIFI_PUBLIC_AREA\",\"LAN_INTERNET_AC...\n[\"DESK\",\"FAN\",\"PRIVATE_BATHROOM\",\"TELEVISION\"]\n[{\"landmarkId\":\"900000001117\",\"geoId\":null,\"na...\n\n\n1193\n440772\n1\nNone\n28.0\n4\n12\n2\n0\n1\n0\n0\n[\"CARPARK\",\"WIFI_PUBLIC_AREA\",\"LAN_INTERNET_AC...\n[\"AIR_CONDITIONING\",\"BATHTUB\",\"DESK\",\"TELEVISI...\n[{\"landmarkId\":\"900000001117\",\"geoId\":null,\"na...\n\n\n1194\n308540\n1\nNone\n15.0\n2\n12\n1\n0\n1\n0\n0\n[\"CARPARK\",\"WIFI_PUBLIC_AREA\",\"LAN_INTERNET_AC...\n[\"AIR_CONDITIONING\",\"DESK\",\"SHOWER\",\"TELEVISION\"]\n[{\"landmarkId\":\"900000001117\",\"geoId\":null,\"na...\n\n\n1195\n220385\n1\nNone\n16.0\n2\n12\n1\n0\n1\n0\n0\n[\"CARPARK\",\"WIFI_PUBLIC_AREA\",\"LAN_INTERNET_AC...\n[\"AIR_CONDITIONING\",\"DESK\",\"SHOWER\",\"TELEVISION\"]\n[{\"landmarkId\":\"900000001117\",\"geoId\":null,\"na...\n\n\n1198\n220385\n0\nNone\n16.0\n2\n12\n1\n0\n1\n0\n0\n[\"CARPARK\",\"HAS_24_HOUR_ROOM_SERVICE\",\"ROOM_SE...\n[\"AIR_CONDITIONING\",\"NON_SMOKING_ROOMS\",\"SEATI...\n[{\"landmarkId\":\"900000001117\",\"geoId\":null,\"na...\n\n\n\n\n378 rows × 14 columns\n\n\n\n\n# ubah sementara null value menjadi 0\ndf['builtYear'] = df['builtYear'].fillna(0)\ndf['size'] = df['size'].fillna(0)\n\n# ubah tipe data\ndf['builtYear'] = df['builtYear'].astype('int32')\ndf['size'] = df['size'].astype('float')\n\nprint('Tipe data builtYear :', df['builtYear'].dtype)\nprint('Tipe data size :', df['size'].dtype)\n\nTipe data builtYear : int32\nTipe data size : float64\n\n\n\n\n\n\n\n\nNote\n\n\n\nKolom yang memiliki nilai null akan membuat tipe data menjadi object, maka dari itu pada penelitian ini akan diisi dengan nilai 0 terlebih dahulu, kemudian kolom tersebut diubah tipe datanya\n\n\n\n# ubah nilai 0 pada kolom builtYear menjadi median tiap starRating\nfor i in df['starRating'].unique():\n    df.loc[(df['starRating'] == i) & (df['builtYear'] == 0),\n           'builtYear'] = df[df['starRating'] == i]['builtYear'].median()\n\n# ubah nilai 0 pada kolom size menjadi median tiap starRating\nfor i in df['starRating'].unique():\n    df.loc[(df['starRating'] == i) & (df['size'] == 0),\n           'size'] = df[df['starRating'] == i]['size'].median()\n    \n# crate dataframe for null value\ndf_null = pd.DataFrame(df.isnull().sum(), columns=['null_value'])\ndf_null['null_value_percentage'] = df_null['null_value'] / len(df) * 100\ndf_null\n\n\n\n\n\n\n\n\nnull_value\nnull_value_percentage\n\n\n\n\nrate\n0\n0.0\n\n\nstarRating\n0\n0.0\n\n\nbuiltYear\n0\n0.0\n\n\nsize\n0\n0.0\n\n\nbaseOccupancy\n0\n0.0\n\n\nmaxChildAge\n0\n0.0\n\n\nmaxChildOccupancy\n0\n0.0\n\n\nisBreakfastIncluded\n0\n0.0\n\n\nisWifiIncluded\n0\n0.0\n\n\nisRefundable\n0\n0.0\n\n\nhasLivingRoom\n0\n0.0\n\n\nhotelFacilities\n0\n0.0\n\n\nroomFacilities\n0\n0.0\n\n\nnearestPointOfInterests\n0\n0.0\n\n\n\n\n\n\n\n\nData sudah tidak memiliki nilai null\n\n\n\n\n13.3.6.3 Statistik Deskriptif\n\ndf.describe()\n\n\n\n\n\n\n\n\nrate\nstarRating\nbuiltYear\nsize\nbaseOccupancy\nmaxChildAge\nmaxChildOccupancy\nisBreakfastIncluded\nisWifiIncluded\nisRefundable\nhasLivingRoom\n\n\n\n\ncount\n1.110000e+03\n1110.000000\n1110.000000\n1110.000000\n1110.000000\n1110.000000\n1110.000000\n1110.000000\n1110.000000\n1110.000000\n1110.000000\n\n\nmean\n7.118771e+05\n2.042342\n1993.248649\n25.430982\n2.080180\n6.528829\n0.747748\n0.322523\n0.896396\n0.457658\n0.020721\n\n\nstd\n9.971952e+05\n1.643362\n122.853595\n20.378749\n0.804554\n3.013432\n0.570834\n0.467652\n0.304883\n0.498428\n0.142512\n\n\nmin\n8.264500e+04\n0.000000\n1.000000\n3.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2.708675e+05\n0.000000\n2000.000000\n16.000000\n2.000000\n5.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n50%\n4.081860e+05\n2.000000\n2012.000000\n21.000000\n2.000000\n5.000000\n1.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n75%\n7.816760e+05\n3.000000\n2015.000000\n29.000000\n2.000000\n10.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n\n\nmax\n1.156198e+07\n5.000000\n2022.000000\n350.000000\n16.000000\n12.000000\n4.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\nPada kolom builtYear terdapat nilai minimum 1 yang tidak mungkin terjadi, maka data tersebut akan dihapus.\nnilai median pada rate dan size terpaut cukup jauh dengan nilai maximum, ini menunjukkan adanya outlier pada kolom rate.\n\n\n13.3.6.3.1 Built Year Data Handling\n\n# Cek nilai unique pada kolom builtYear dibawah 2000\nprint('Nilai unique builtYear dibawah 2000 :')\nprint(df[df['builtYear'] &lt; 2000]['builtYear'].unique())\n\nprint('Nilai unique bulitYear dibawah 1900 :')\nprint(df[df['builtYear'] &lt; 1900]['builtYear'].unique())\n\nNilai unique builtYear dibawah 2000 :\n[1997 1995 1964 1990 1992 1937    1 1994 1977 1993 1996 1978 1989 1986\n 1998]\nNilai unique bulitYear dibawah 1900 :\n[1]\n\n\n\nTerdapat data yang memiliki nilai builtYear yang tidak mungkin terjadi, maka data tersebut akan dihapus.\nTidak terdapat hotel dibawah tahun 1900, maka dari itu data yang disimpan adalah data diatas tahun 1900.\n\n\n# menghapus baris yang memiliki nilai dibawah 1900 pada kolom builtYear\ndf = df[df['builtYear'] &gt; 1900]\n\n# Cek nilai unique pada kolom builtYear dibawah 2000\nprint('Nilai unique builtYear dibawah 2000 :')\nprint(df[df['builtYear'] &lt; 2000]['builtYear'].unique())\n\nNilai unique builtYear dibawah 2000 :\n[1997 1995 1964 1990 1992 1937 1994 1977 1993 1996 1978 1989 1986 1998]\n\n\n\n\n\n13.3.6.4 Outlier Handling\n\n13.3.6.4.1 Rate Data\n\n# Statistik Harga\nprint('Harga')\nprint(f'maximum value : {df.rate.max()}')\nprint(f'minimum value : {df.rate.min()}')\nprint(f'skew value : {round(df.rate.skew(), 2)}')\n\n# Distribusi harga\nsns.set_style('darkgrid')\nplt.figure(figsize=(20, 10), dpi=80)\nsns.displot(df, x=\"rate\", kind=\"kde\", fill=True).set(\n    title='OriginalRate Distribution')\nplt.show()\n\nHarga\nmaximum value : 11561983\nminimum value : 82645\nskew value : 5.33\n\n\nC:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n&lt;Figure size 1600x800 with 0 Axes&gt;\n\n\n\n\n\n\nKolom rate memiliki nilai skew yang cukup tinggi, selain itu dari plot terlihat memiliki ekor yang cukup panjang. Ini menunjukkan adanya outlier pada kolom rate.\nPenghapusan outlier dilakukan dengan menggunakan metode IQR.\n\n\n\n\n\n\n\nNote\n\n\n\nUntuk penjelasan lebih lanjut mengenai skew dapat dilihat disini\n\n\n\n# Hitung outlier pada kolom rate\nQ1 = df['rate'].quantile(0.25)\nQ3 = df['rate'].quantile(0.75)\nIQR = Q3 - Q1\n\nprint('Batas bawah :', Q1 - (1.5 * IQR))\nprint('Batas atas :', Q3 + (1.5 * IQR))\n\n# Hitung jumlah outlier\ntotal_outlier = len(df[(df['rate'] &lt; (Q1 - (1.5 * IQR))) | (df['rate'] &gt; (Q3 + (1.5 * IQR)))])\nprint('Jumlah outlier :', total_outlier)\n\nBatas bawah : -502673.875\nBatas atas : 1556801.125\nJumlah outlier : 91\n\n\n\nTerdapat 91 data outlier pada kolom rate.\n\n\n# Hapus outlier\ndf = df[(df['rate'] &gt; (Q1 - (1.5 * IQR))) & (df['rate'] &lt; (Q3 + (1.5 * IQR)))]\ndf.describe()\n\n\n\n\n\n\n\n\nrate\nstarRating\nbuiltYear\nsize\nbaseOccupancy\nmaxChildAge\nmaxChildOccupancy\nisBreakfastIncluded\nisWifiIncluded\nisRefundable\nhasLivingRoom\n\n\n\n\ncount\n1.015000e+03\n1015.000000\n1015.000000\n1015.000000\n1015.000000\n1015.000000\n1015.000000\n1015.000000\n1015.000000\n1015.000000\n1015.000000\n\n\nmean\n4.900464e+05\n1.858128\n2000.077833\n22.072798\n2.066010\n6.599015\n0.753695\n0.284729\n0.887685\n0.456158\n0.016749\n\n\nstd\n3.242374e+05\n1.554268\n27.601540\n9.015316\n0.645556\n2.969237\n0.567377\n0.451508\n0.315910\n0.498320\n0.128392\n\n\nmin\n8.264500e+04\n0.000000\n1937.000000\n3.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2.589530e+05\n0.000000\n2001.000000\n16.000000\n2.000000\n5.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n50%\n3.719010e+05\n2.000000\n2012.000000\n20.000000\n2.000000\n5.000000\n1.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n75%\n6.118455e+05\n3.000000\n2015.000000\n26.500000\n2.000000\n10.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.000000\n\n\nmax\n1.534866e+06\n5.000000\n2022.000000\n72.000000\n6.000000\n12.000000\n4.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\nNilai maksimum sudah cukup menurun setelah dilakukan penghapusan outlier.\nNilai maksimum pada kolom size juga ikut menurun.\n\n\n# Statistik Harga\nprint('Harga')\nprint(f'maximum value : {df.rate.max()}')\nprint(f'minimum value : {df.rate.min()}')\nprint(f'skew value : {round(df.rate.skew(), 2)}')\n\n# Distribusi harga\nsns.set_style('darkgrid')\nplt.figure(figsize=(20, 10), dpi=80)\nsns.displot(df, x=\"rate\", kind=\"kde\", fill=True).set(\n    title='OriginalRate Distribution')\nplt.show()\n\nHarga\nmaximum value : 1534866\nminimum value : 82645\nskew value : 1.31\n\n\nC:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n&lt;Figure size 1600x800 with 0 Axes&gt;\n\n\n\n\n\n\nNilai skew pada kolom rate sudah menjadi lebih baik setelah dilakukan penghapusan outlier.\n\n\n\n13.3.6.4.2 Size Data\n\n# Statistik Size\nprint('Size')\nprint('maximum value : {}'.format(df['size'].max()))\nprint('minimum value : {}'.format(df['size'].min()))\nprint('skew value : {}'.format(df['size'].skew()))\n\n# Distribusi size\nplt.figure(figsize=(20, 10), dpi=80)\nsns.set_style('darkgrid')\nsns.jointplot(data=df, x='size', y='rate')\nplt.show()\n\nSize\nmaximum value : 72.0\nminimum value : 3.0\nskew value : 1.2163013252656778\n\n\n&lt;Figure size 1600x800 with 0 Axes&gt;\n\n\n\n\n\n\nNilai skew sudah sangat mendekati angka 1, ini menunjukkan bahwa distribusi data sudah sangat baik.\n\n\n# Hasil data cleaning\nprint('Total baris data awal :', jumlah_baris_ori)\nprint('Total baris data yang dihapus :', jumlah_baris_ori - df.shape[0])\nprint('Total baris data setelah cleaning :', df.shape[0])\n\nTotal baris data awal : 1110\nTotal baris data yang dihapus : 95\nTotal baris data setelah cleaning : 1015\n\n\n\ndf.isnull().sum()\n\nrate                       0\nstarRating                 0\nbuiltYear                  0\nsize                       0\nbaseOccupancy              0\nmaxChildAge                0\nmaxChildOccupancy          0\nisBreakfastIncluded        0\nisWifiIncluded             0\nisRefundable               0\nhasLivingRoom              0\nhotelFacilities            0\nroomFacilities             0\nnearestPointOfInterests    0\ndtype: int64\n\n\n\n\n\n\n13.3.7 Encoding Data\n\ndf = df.reset_index(drop=True)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1015 entries, 0 to 1014\nData columns (total 14 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   rate                     1015 non-null   int32  \n 1   starRating               1015 non-null   int32  \n 2   builtYear                1015 non-null   int32  \n 3   size                     1015 non-null   float64\n 4   baseOccupancy            1015 non-null   int64  \n 5   maxChildAge              1015 non-null   int64  \n 6   maxChildOccupancy        1015 non-null   int64  \n 7   isBreakfastIncluded      1015 non-null   int64  \n 8   isWifiIncluded           1015 non-null   int64  \n 9   isRefundable             1015 non-null   int64  \n 10  hasLivingRoom            1015 non-null   int64  \n 11  hotelFacilities          1015 non-null   object \n 12  roomFacilities           1015 non-null   object \n 13  nearestPointOfInterests  1015 non-null   object \ndtypes: float64(1), int32(3), int64(7), object(3)\nmemory usage: 99.2+ KB\n\n\n\nKolom hotelfacilities, roomfacilities, dan nearestPointofInterests merupakan sebuah fitur dengan multi label. Oleh karena itu data tersebut akan dilakukan multi-hot encoding.\nProses tersebut akan dilakukan dengan library sklearn.preprocessing.MultiLabelBinarizer\n\n\n\n\n\n\n\nNote\n\n\n\nUntuk penjelasan lebih lanjut tentang multi-label dan multi-class dapat dilihat disini\n\n\n\n13.3.7.1 Check data format\n\ndf['hotelFacilities'].head(2)\n\n0    [\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\n1    [\"CARPARK\",\"COFFEE_SHOP\",\"ELEVATOR\",\"RESTAURAN...\nName: hotelFacilities, dtype: object\n\n\n\ndf['roomFacilities'].head(2)\n\n0    [\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\n1    [\"AIR_CONDITIONING\",\"BALCONY_TERRACE\",\"BATHROB...\nName: roomFacilities, dtype: object\n\n\n\ndf['nearestPointOfInterests'].head(2)\n\n0    [{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\n1    [{\"landmarkId\":\"900000001343\",\"geoId\":null,\"na...\nName: nearestPointOfInterests, dtype: object\n\n\n\nroomfacilities dan hotelFacilities memiliki format yang sama, yaitu list yang berisi string.\nnearestPointofInterests memiliki format yang berbeda, yaitu list yang berisi dictionary/json yang berisi string dan float.\n\n\n\n13.3.7.2 Data Preprocessing\n\n# create a MultiLabelBinarizer object\nmlb = MultiLabelBinarizer()\n\n\nDaftar kolom hasil encoding akan diexport menjadi file pkl yang akan digunakan pada aplikasi streamlit.\n\n\n# reformat kolom hotelFacilities\ndf['hotelFacilities'] = df['hotelFacilities'].apply(eval)\n\n# multi label binarizer untuk kolom hotelFacilities dengan preifx Hotel_\nhotel_facilities = pd.DataFrame(mlb.fit_transform(\n    df['hotelFacilities']), columns=[f'Hotel_{col}' for col in mlb.classes_])\n\nhotelNewCol = hotel_facilities.shape[1]\nprint('Jumlah kolom :', hotel_facilities.shape[1])\n\n# export hotel_facilities with pickle\nhotelFacilities = hotel_facilities.columns.tolist()\npickle.dump(hotelFacilities, open('hotelFacilities.pkl', 'wb'))\n    \nhotel_facilities.head(2)\n\nJumlah kolom : 223\n\n\n\n\n\n\n\n\n\nHotel_ACCESSIBILITY_EQUIPMENT\nHotel_ACCESSIBLE_BATHROOM\nHotel_ACCESSIBLE_PARKING\nHotel_ACCESSIBLE_PATH_OF_TRAVEL\nHotel_ACCESS_FRIENDLY\nHotel_AEROBICS\nHotel_AIRPORT_TRANSFER\nHotel_AIRPORT_TRANSFER_SURCHARGE\nHotel_AIR_CONDITIONING\nHotel_AIR_CONDITIONING_IN_RESTAURANT\n...\nHotel_VOLLEYBALL\nHotel_WATERSLIDE\nHotel_WATER_SPORT\nHotel_WEDDING_SERVICE\nHotel_WHEELCHAIR_ACCESSIBLE\nHotel_WIFI_FREE\nHotel_WIFI_PUBLIC_AREA\nHotel_WIFI_PUBLIC_AREA_SURCHARGE\nHotel_WIFI_SURCHARGE\nHotel_WINERY\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n...\n0\n1\n0\n1\n0\n1\n1\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n...\n0\n1\n0\n1\n0\n1\n1\n0\n0\n0\n\n\n\n\n2 rows × 223 columns\n\n\n\n\n# reformat kolom roomFacilities\ndf['roomFacilities'] = df['roomFacilities'].apply(eval)\n\n# multi label binarizer untuk kolom roomFacilities dengan preifx Room_\nroom_facilities = pd.DataFrame(mlb.fit_transform(df['roomFacilities']), columns=[\n                               f'Room_{col}' for col in mlb.classes_])\n\nroomNewCol = room_facilities.shape[1]\nprint('Jumlah kolom :', roomNewCol)\n\n# export room_facilities with pickle\nroomFacilities = room_facilities.columns.tolist()\npickle.dump(roomFacilities, open('roomFacilities.pkl', 'wb'))\n\nroom_facilities.head(2)\n\nJumlah kolom : 70\n\n\n\n\n\n\n\n\n\nRoom_AIR_CONDITIONING\nRoom_BALCONY\nRoom_BALCONY_TERRACE\nRoom_BATHROBES\nRoom_BATHTUB\nRoom_BLACKOUT_CURTAINS\nRoom_BLACKOUT_DRAPES_CURTAINS\nRoom_CLOTHES_DRYER\nRoom_COFFEE_TEA_MAKER\nRoom_COMPLIMENTARY_BOTTLED_WATER\n...\nRoom_SLIPPERS\nRoom_SOUNDPROOFED_ROOMS\nRoom_TELEVISION\nRoom_TELEVISION_LCD_PLASMA_SCREEN\nRoom_TOILETRIES\nRoom_TOWELS_PROVIDED\nRoom_TURNDOWN_SERVICE\nRoom_TWENTY_FOUR_HOUR_ROOM_SERVICE\nRoom_WHEELCHAIR_ACCESSIBLE\nRoom_WIFI_SURCHARGE\n\n\n\n\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n...\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n...\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 70 columns\n\n\n\n\n# reformat kolom nearestPointOfInterests\ndf['nearestPointOfInterests'] = df['nearestPointOfInterests'].apply(\n    lambda x: [item['landmarkType'] for item in json.loads(x)])\n\n# multi label binarizer untuk kolom nearestPointOfInterests dengan preifx Point_\npointOfInterests = pd.DataFrame(mlb.fit_transform(\n    df['nearestPointOfInterests']), columns=[f'Point_{col}' for col in mlb.classes_])\n\npointNewCol = pointOfInterests.shape[1]\nprint('Jumlah kolom :', pointNewCol)\n\n# export pointOfInterests with pickle\npointInterests = pointOfInterests.columns.tolist()\npickle.dump(pointInterests, open('pointInterests.pkl', 'wb'))\n\npointOfInterests.head(2)\n\nJumlah kolom : 15\n\n\n\n\n\n\n\n\n\nPoint_AIRPORT\nPoint_ATTRACTION\nPoint_BEACH\nPoint_HOSPITAL\nPoint_MONUMENT\nPoint_MUSEUM\nPoint_OFFICIAL_BUILDING\nPoint_OTHERS\nPoint_PLACE_OF_WORSHIP\nPoint_RESTAURANT\nPoint_SCHOOL\nPoint_SHOPPING_AREA\nPoint_STORE\nPoint_TERMINAL\nPoint_TRAIN_STATION\n\n\n\n\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n13.3.7.2.1 Menggabungkan hasil encoding\n\n# Total kolom encoding\ntotalNewCol = hotelNewCol + roomNewCol + pointNewCol\nprint('Total kolom encoding :', totalNewCol)\n\nTotal kolom encoding : 308\n\n\n\n# menghapus kolom hotelFacilities, roomFacilities, dan nearestPointOfInterests\ndf = df.drop(columns=['hotelFacilities', 'roomFacilities', 'nearestPointOfInterests'])\n\nprint('df shape :', df.shape)\nprint('hotel_facilities shape :', hotel_facilities.shape)\nprint('room_facilities shape :', room_facilities.shape)\nprint('pointOfInterests shape :', pointOfInterests.shape)\n\ndf = pd.concat([df, hotel_facilities, room_facilities,\n               pointOfInterests], axis=1)\ndf.head()\n\ndf shape : (1015, 11)\nhotel_facilities shape : (1015, 223)\nroom_facilities shape : (1015, 70)\npointOfInterests shape : (1015, 15)\n\n\n\n\n\n\n\n\n\nrate\nstarRating\nbuiltYear\nsize\nbaseOccupancy\nmaxChildAge\nmaxChildOccupancy\nisBreakfastIncluded\nisWifiIncluded\nisRefundable\n...\nPoint_MUSEUM\nPoint_OFFICIAL_BUILDING\nPoint_OTHERS\nPoint_PLACE_OF_WORSHIP\nPoint_RESTAURANT\nPoint_SCHOOL\nPoint_SHOPPING_AREA\nPoint_STORE\nPoint_TERMINAL\nPoint_TRAIN_STATION\n\n\n\n\n0\n1008264\n5\n2013\n40.0\n2\n8\n1\n1\n1\n0\n...\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1\n1049587\n5\n2013\n40.0\n2\n8\n1\n1\n1\n0\n...\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2\n1521212\n4\n2013\n53.0\n2\n5\n1\n1\n1\n1\n...\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n\n\n3\n696969\n4\n2013\n24.0\n2\n5\n1\n0\n1\n1\n...\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n\n\n4\n927273\n4\n2013\n28.0\n2\n5\n1\n1\n1\n1\n...\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n\n\n\n\n5 rows × 319 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1015 entries, 0 to 1014\nColumns: 319 entries, rate to Point_TRAIN_STATION\ndtypes: float64(1), int32(311), int64(7)\nmemory usage: 1.3 MB\n\n\n\n\n\n\n13.3.8 Export Data ke CSV\n\ndf.to_csv('kamar-hotel-yogyakarta.csv', index=False)\n\n\ncol = df.columns\n\n# export col with pickle\npickle.dump(col, open('col.pkl', 'wb'))\n\n\nprint(col)\n\nIndex(['rate', 'starRating', 'builtYear', 'size', 'baseOccupancy',\n       'maxChildAge', 'maxChildOccupancy', 'isBreakfastIncluded',\n       'isWifiIncluded', 'isRefundable',\n       ...\n       'Point_MUSEUM', 'Point_OFFICIAL_BUILDING', 'Point_OTHERS',\n       'Point_PLACE_OF_WORSHIP', 'Point_RESTAURANT', 'Point_SCHOOL',\n       'Point_SHOPPING_AREA', 'Point_STORE', 'Point_TERMINAL',\n       'Point_TRAIN_STATION'],\n      dtype='object', length=319)\n\n\n\n\n13.3.9 Data Anlisis\n\nvalue = df.starRating.value_counts()\nprint(value)\n\n# starRating Distribution by percentage\nvalue_percentage = value / len(df) * 100\n\n# create a list of tuples where each tuple contains the value and index of each element in the value_percentage series\nvalue_percentage_list = [(value_percentage[i], i)\n                         for i in range(len(value_percentage))]\n\n# sort the list by the value in descending order\nvalue_percentage_list_sorted = sorted(value_percentage_list, reverse=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 10), dpi=80,\n                       gridspec_kw={'width_ratios': [1, 0.5]})\nsns.histplot(df, x=\"rate\", hue='starRating', palette='bright',\n             ax=ax[0]).set(title='Rate Distribution by starRating')\n\nsns.barplot(x=value_percentage.index, y=value_percentage.values,\n            palette='bright', ax=ax[1]).set(title='starRating percentage')\n\n# add the percentage text using the sorted list\nfor i, (v, index) in enumerate(value_percentage_list_sorted):\n    ax[1].text(index, v + 0.5, str(round(v, 2)) + '%',\n               color='black', fontweight='bold', ha='center')\n\nfig.tight_layout()\n\nstarRating\n0    305\n3    225\n2    161\n1    147\n4    143\n5     34\nName: count, dtype: int64\n\n\n\n\n\n\nfiltered_0 = df[df['starRating'] == 0.0]\nfiltered_1 = df[df['starRating'] == 1.0]\nfiltered_2 = df[df['starRating'] == 2.0]\nfiltered_3 = df[df['starRating'] == 3.0]\nfiltered_4 = df[df['starRating'] == 4.0]\nfiltered_5 = df[df['starRating'] == 5.0]\n\nprint('Skew value for every starRating')\nprint(df.groupby('starRating')['rate'].skew())\n\n# OriginalRate Distribution by starRating using hisplot inside subplot\nfig, ax = plt.subplots(2, 3, figsize=(20, 10), dpi=80,\n                       gridspec_kw={'width_ratios': [1, 1, 1]})\nsns.histplot(filtered_0, x=\"rate\", ax=ax[0, 0]).set(\n    title='Rate Distribution by 0 starRating')\nsns.histplot(filtered_1, x=\"rate\", ax=ax[0, 1]).set(\n    title='Rate Distribution by 1 starRating')\nsns.histplot(filtered_2, x=\"rate\", ax=ax[0, 2]).set(\n    title='Rate Distribution by 2 starRating')\nsns.histplot(filtered_3, x=\"rate\", ax=ax[1, 0]).set(\n    title='Rate Distribution by 3 starRating')\nsns.histplot(filtered_4, x=\"rate\", ax=ax[1, 1]).set(\n    title='Rate Distribution by 4 starRating')\nsns.histplot(filtered_5, x=\"rate\", ax=ax[1, 2]).set(\n    title='Rate Distribution by 5 starRating')\nfig.tight_layout()\n\nSkew value for every starRating\nstarRating\n0    2.167239\n1    2.119075\n2    1.963175\n3    1.234573\n4    0.136950\n5    0.172502\nName: rate, dtype: float64\n\n\n\n\n\n\n# Statistik Harga tiap rating bintang hotel\n\ndfRateStat = df.groupby('starRating').agg(\n    {'rate': ['mean', 'std', 'min', 'max', lambda x: x.quantile(0.25), 'median', lambda x: x.quantile(0.75)]})\n\n# change the column name from index 4 and 6\ndfRateStat = dfRateStat.rename(\n    columns={'&lt;lambda_0&gt;': '25%', '&lt;lambda_1&gt;': '75%'})\ndfRateStat\n\n\n\n\n\n\n\n\nrate\n\n\n\nmean\nstd\nmin\nmax\n25%\nmedian\n75%\n\n\nstarRating\n\n\n\n\n\n\n\n\n\n\n\n0\n3.191067e+05\n171570.704430\n82645\n1239669\n206612.0\n289256.0\n371901.0\n\n\n1\n2.972719e+05\n149305.342175\n90909\n938609\n206612.0\n264463.0\n330579.0\n\n\n2\n3.785267e+05\n195403.723931\n114876\n1399537\n247934.0\n329752.0\n455988.0\n\n\n3\n5.947459e+05\n305081.185819\n177686\n1528926\n371901.0\n516529.0\n709703.0\n\n\n4\n8.620919e+05\n302233.532163\n321395\n1521212\n613572.5\n846281.0\n1102204.0\n\n\n5\n1.127378e+06\n247495.951682\n736969\n1534866\n909555.0\n1103609.0\n1368749.0\n\n\n\n\n\n\n\n\nPada kota Yogyakarta tidak terdapat banyak hotel bintang 5.\nMayoritas hotel di Yogyakarta adalah hotel bintang 0.\nTerdapat hotel bintang 2 yang memiliki harga setara dengan hotel bintang 5.\n\n\n\n\n\n\n\nNote\n\n\n\nDari tabel statistik tersebut mengindikasikan beberapa nilai yang tidak wajar. Untuk penelitian selanjutnya bisa dilakukan pembersihan data lebih mendalam lagi.\n\n\n\n\n\n\n\n\nNote\n\n\n\nHasil dari multi-hot encoding juga belum dilakukan pembersihan data, selain itu dengan banyaknya hasil kolom juga dapat dilakukan reduksi dimensi, contohnya menggunakan PCA. Oleh karena itu untuk penelitian selanjutnya bisa dilakukan pembersihan data lebih mendalam lagi dan dilakukan reduksi dimensi.\n\n\nMasih banyak informasi-informasi yang dapat di ambil dari data ini, seperti:\n\nPerbandingan harga hotel bintang 5 dengan hotel bintang 0.\nLandmark apa yang paling banyak dicari oleh pengunjung hotel?\nFasilitas apa yang sudah manjadi standar pada hotel bintang 3?\nDan masih banyak lagi."
  },
  {
    "objectID": "project_regresi.html#pemodelan-machine-learning",
    "href": "project_regresi.html#pemodelan-machine-learning",
    "title": "13  Project Regresi Hotel Yogyakarta",
    "section": "13.4 Pemodelan Machine Learning",
    "text": "13.4 Pemodelan Machine Learning\n\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import train_test_split, HalvingGridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom IPython import display"
  },
  {
    "objectID": "project_klasifikasi.html#klasifikasi-sentimen-analisis",
    "href": "project_klasifikasi.html#klasifikasi-sentimen-analisis",
    "title": "12  Project Klasifikasi Sentimen Analisis",
    "section": "12.1 Klasifikasi Sentimen Analisis",
    "text": "12.1 Klasifikasi Sentimen Analisis\nKlasifikasi dapat berbentuk berupa Sentimen Analisis. Sentimen Analisis merupakan gabungan dari data mining dan text mining. Secara sederhananya merupakan proses mengolah berbagai opini dan argumen dari berbagai media sosial dalam berbagai aspek seperti jasa, produk, atau sesuai dengan isi konten pada media sosial tersebut .\nUntuk dapat mengerjakan sebuah sentimen analisis, diperlukan sebuah dataset yang berisi banyak opini positif, negatif, dan atau netral. Maka dari itu pengambilan dataset merupakan proses yang penting dalam tahapan sentimen analisis. Dataset yang baik harus memiliki ukuran yang cukup besar dengan jumlah yang cukup banyak untuk meminimalkan kesalahan secara perhitungan algoritma.\nUntuk mendapatkan sebuah dataset yang berisikan opini di media sosial dapat dilakukan dengan berbagai cara. Salah satunya adalah mencari dataset yang sudah jadi pada situs penyedia dataset seperti kaggle, dataset tersebut dapat dikategorikan sebagai dataset publik. Selain dengan menggunakan dataset publik, data opini dapat dibuat dengan cara scraping atau crawling. Scraping merupakan teknik yang sering digunakan untuk mendapatkan atau mengekstrak sebuah informasi pada sebuah media sosial dan atau website secara otomatis tanpa harus melakukan penyalinan secara manual."
  },
  {
    "objectID": "project_klasifikasi.html#mencari-data-manual-dengan-scraping",
    "href": "project_klasifikasi.html#mencari-data-manual-dengan-scraping",
    "title": "12  Project Klasifikasi Sentimen Analisis",
    "section": "12.2 Mencari Data Manual dengan Scraping",
    "text": "12.2 Mencari Data Manual dengan Scraping\nBerikut adalah cara mendapatkan data untuk sentimen analisis menggunakan Komentar Youtube. Dapat menggunakan API Key berikut AIzaSyDkCRF4cmM_TtyBznV9aKptHNZqooyucqU\n# yang harus di instal\n\n!pip install google-api-python-client\n!pip install google-auth google-auth-oauthlib google-auth-httplib2\n!pip install pickle\n!pip install sastrawi\n!pip install textblob\nCode diatas merupakan library yang harus terintal.\n#library yang digunakan, jika dirasa kurang penting dapat dihapus\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport string\nimport re\nfrom nltk import word_tokenize\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom textblob import TextBlob\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nSetalah mengimport semua library, selanjutnya measukan code berikut\nfrom google.colab import drive\ndrive.mount('/content/drive')\nimport pandas as pd\n\n## Call the \"build()\" function from the Python-client\nfrom googleapiclient.discovery import build\n\napi_key = input(\"API KEY: \")\nyoutube = build(\"youtube\",\"v3\", developerKey=api_key)\nurl = input(\"VIDEOURL: \")\n\ndef get_comments(url):\n    # Get the ID of the video by splitting the URL\n    single_video_id = url.split(\"=\")[1].split(\"&\")[0]\n    # Use the list() method to extract a JSON with key information\n    # from the video.\n    video_list=youtube.videos().list(part=\"snippet\",id=single_video_id).execute()\n    channel_id= video_list[\"items\"][0][\"snippet\"][\"channelId\"]\n    title_single_video= video_list[\"items\"][0][\"snippet\"][\"title\"]\n    playlist_id = None\n    forUserName = None\n\n    nextPageToken_comments = None\n    commentsone=[]\n\n    while True:\n        #Request the first 50 videos of a channel. This is the full dictionary. The result is store in a variable called \"pl_response\".\n        #PageToken at this point is \"None\"\n        pl_request_comment= youtube.commentThreads().list(part=[\"snippet\",\"replies\"],\n                                            videoId=single_video_id,\n                                            maxResults=50,\n                                            pageToken= nextPageToken_comments)\n        pl_response_comment = pl_request_comment.execute()\n\n        ## Send the amount of views and the URL of each video to the videos empty list that was declared at the beginning of the code.\n        for i in pl_response_comment[\"items\"]:\n            vid_comments = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"]\n            comm_author = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n            comm_author_id = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorChannelId\"][\"value\"]\n            comm_date = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n            comm_likes = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n            new_var=i.get(\"replies\",\"0\")\n\n            commentsone.append({\n                \"comm_date\":comm_date,\n                \"author\":comm_author,\n                \"author_id\":comm_author_id,\n                \"likes\":comm_likes,\n                \"comment\":vid_comments,\n                \"video_id\":single_video_id\n            })\n\n\n\n        nextPageToken_comments = pl_response_comment.get(\"nextPageToken\")\n\n        if not nextPageToken_comments:\n            break\n\n    for i in commentsone[:10]:\n        print(i[\"comment\"])\n\n\n    pd.DataFrame.from_dict(commentsone).to_csv(f\"/content/drive/MyDrive/comments/dataset.csv\")\n\nget_comments(url)\npada source code scraping diatas, file yang telah di scraping akan dimasukan kedalam Google Drive dengan nama file comments, lalu nama file akan menjadi dataset.csv\nuntuk menampilkan hasil scraping dapat menggunakan perintah seperti berikut.\ndf = pd.read_csv('/content/drive/MyDrive/comments/dataset.csv')\ndf.head(500)\ndf.count()"
  },
  {
    "objectID": "project_klasifikasi.html#mencari-data-dengan-mengunjungi-website",
    "href": "project_klasifikasi.html#mencari-data-dengan-mengunjungi-website",
    "title": "12  Project Klasifikasi Sentimen Analisis",
    "section": "12.3 Mencari Data dengan mengunjungi website",
    "text": "12.3 Mencari Data dengan mengunjungi website\nData sentimen analisis bisa didapatkan melalui website penyedia data seperti kaggle dan UC Irvine Machine Learning Repository."
  },
  {
    "objectID": "project_klasifikasi.html#sentimen-analisis-menggunakan-data-scraping-youtube",
    "href": "project_klasifikasi.html#sentimen-analisis-menggunakan-data-scraping-youtube",
    "title": "12  Project Klasifikasi Sentimen Analisis",
    "section": "12.4 Sentimen Analisis menggunakan data scraping youtube",
    "text": "12.4 Sentimen Analisis menggunakan data scraping youtube\ngunakan code berikut untuk melakukan sentimen analisis\nimport nltk\nimport pandas as pd\ndata = pd.read_csv(\"/content/drive/MyDrive/comments/dataset.csv\")\ndata = data.dropna()\nprint(data.head())\ndata_nw = data.drop(['comm_date',\"author\", 'author_id',\"likes\",'video_id'], axis=1 )\ndata_nw\nFungsi code diatas adalah untuk menghapus atau melakukan drop pada kolom ‘comm_date’,“author”, ‘author_id’,“likes”,‘video_id’\ndata_nw.to_csv(\"/content/drive/MyDrive/comments/dataset_drop.csv\") #Fungsinya untuk menyimpan hasil drop\nmembuka file hasil dari dataset_drop.csv\ndata_baru = pd.read_csv(\"/content/drive/MyDrive/comments/dataset_drop.csv\")\ndata_baru.head()\ndef caseFolding(comment):\n          comment = comment.lower()\n          comment = comment.strip(\" \")\n          comment = re.sub(r'[?|$|.|!]',r'', comment)\n          comment = re.sub(r'[^a-zA-Z0-9 ]',r'', comment)\n          return comment\n\ndata_baru['comment'] = data_baru['comment'].apply(caseFolding)\nFungsi dari code diatas adalah untuk menghilangkan tanda baca serta angka yang tidak dibutuhkan dan mengubah huruf kapital menjadi huruf kecil. Lalu data disimpan ke dalam kolom comment.\nSelanjutnya simpan menggunakan perintah berikut\ndata_baru.to_csv(\"/content/drive/MyDrive/comments/dataset_bersih.csv\")\nSelanjutnya data yang sudah bersih, dapat diberi label negatif, positif, dan atau netral pada samping kolom comments. Label dapat di beri nama sentiment.\nSelanjutnya dapat dilakukan uji akurasi menggunakan Algoritma atau model yang sesuai dan yang di inginkan.\nSebagai contoh menggunakan Algoritma KNN, maka dapat menggunakan code berikut.\nimport pandas as pd\n\ndata = pd.read_csv('/content/drive/MyDrive/comments/dataset_bersih.csv')\nX = data['comment']\ny = data['sentimen']\n# Lakukan preprocessing pada teks\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nnltk.download('stopwords')\n\ndef preprocessing(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = text.strip()\n    tokens = nltk.word_tokenize(text)\n    stop_words = set(stopwords.words('indonesian'))\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    stemmer = PorterStemmer()\n    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n    return ' '.join(stemmed_tokens)\n\nX = X.apply(preprocessing)\n# Lakukan vectorization pada teks\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(X)\n# Lakukan pembagian dataset menjadi data training dan data testing\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Lakukan training model KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n# Lakukan prediksi pada data testing\ny_pred = knn.predict(X_test)\n# Lakukan evaluasi model\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1 Score:', f1)\nprint('Confusion Matrix:\\n', cm)"
  },
  {
    "objectID": "modul_7.html#section",
    "href": "modul_7.html#section",
    "title": "8  Transformasi Data",
    "section": "8.4 ",
    "text": "8.4"
  },
  {
    "objectID": "modul_7.html#dokumentasi-fitur",
    "href": "modul_7.html#dokumentasi-fitur",
    "title": "8  Transformasi Data",
    "section": "8.4 Dokumentasi Fitur",
    "text": "8.4 Dokumentasi Fitur\nDokumentasi data dapat menjembatani kesenjangan antara transaksi (pembuatan data) dan analisis (konsumsi data). Dokumentasi data yang baik memungkinkan pengguna, ataupun rekan tim untuk memahami siapa/apa/kapan/di mana/bagaimana/mengapa data tersebut dibentuk ataupun dikonsumsi.\nSecara umum, dokumentasi fitur dari sebuah dataset mempunyai beberapa poin, yaitu :\n- Nama fitur\n- Definisi\n- Tipe data\n- Skala\n- Sumber data\n- Keterangan\nMari kita lakukan sebuah dokumentasi fitur dari dataset spotify yang kita gunakan di slide sebelumnya. Dengan menggunakan code data.info() kita bisa mengetahui banyak hal tentang dataset kita\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 19 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   valence           100 non-null    float64\n 1   year              100 non-null    int64  \n 2   acousticness      100 non-null    float64\n 3   artists           100 non-null    object \n 4   danceability      100 non-null    float64\n 5   duration_ms       100 non-null    int64  \n 6   energy            100 non-null    float64\n 7   explicit          100 non-null    int64  \n 8   id                100 non-null    object \n 9   instrumentalness  100 non-null    float64\n 10  key               100 non-null    int64  \n 11  liveness          100 non-null    float64\n 12  loudness          100 non-null    float64\n 13  mode              100 non-null    int64  \n 14  name              100 non-null    object \n 15  popularity        100 non-null    int64  \n 16  release_date      100 non-null    object \n 17  speechiness       100 non-null    float64\n 18  tempo             100 non-null    float64\ndtypes: float64(9), int64(6), object(4)\nmemory usage: 15.0+ KB\n\n\n\n8.4.1 Nama Fitur\nFitur atau kolom adalah satuan data yang mendeskripsikan aspek tertentu dalam data. Di dalam dataset ini, kita mempunyai beberapa fitur, antara lain :\n- Valence\n- Year\n- Acousticness\n- Artists\n- Danceability\n- Duration_ms\n- Energy\n- Explicit\n- ID\n- Instrumentalness\n- Key\n- Liveness\n- Loudness\n- Mode\n- Name\n- Popularity\n- Release_date\n- Speechiness\n- Tempo\n\n\n8.4.2 Definisi Fitur\nSetiap fitur mempunyai definisi atau penjelasan makna tertentu. Definisi yang ada di bawah hanya sebagian dari dataset saja. Untuk definisi keseluruhan data bisa dilihat di https://developer.spotify.com/documentation/web-api/reference/get-several-audio-features.\nBeriku definisi fitur dari dataset :\n- name, Nama lagu\n- artists, Nama artis pembuat lagu\n- year, Tahun rilis lagu\n- popularity, Tingkat popularitas lagu. Nilai popularitas adalah antara 0 dan 100, dengan 100 menjadi nilai maksimal.\n- key, adalah kunci lagu dari lagu tersebut. kunci lagu direpresentasikan menggunakan angka menurut Pitch Class Notation Standard. Misalnya, 0 = C, 1 = C♯/D♭, 2 = D, dan seterusnya. Jika tidak ada kunci, maka nilai -1.\n- mode, Mode dari lagu tersebut. 1 = Major, 0 = Minor.\n\n\n8.4.3 Tipe Data\nTipe data sangatlah penting karena tipe data menentukan operasi atau function apa yang bisa dilakukan terhadap data tersebut. Sebagai contoh, untuk menentukan nilai random, kita tidak bisa menggunakan angka float (pecahan), kita harus menggunakan angka integer (bulat).\nKita dapat mengecek tipe data dari sebuah dataset menggunakan function data.info()\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 19 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   valence           100 non-null    float64\n 1   year              100 non-null    int64  \n 2   acousticness      100 non-null    float64\n 3   artists           100 non-null    object \n 4   danceability      100 non-null    float64\n 5   duration_ms       100 non-null    int64  \n 6   energy            100 non-null    float64\n 7   explicit          100 non-null    int64  \n 8   id                100 non-null    object \n 9   instrumentalness  100 non-null    float64\n 10  key               100 non-null    int64  \n 11  liveness          100 non-null    float64\n 12  loudness          100 non-null    float64\n 13  mode              100 non-null    int64  \n 14  name              100 non-null    object \n 15  popularity        100 non-null    int64  \n 16  release_date      100 non-null    object \n 17  speechiness       100 non-null    float64\n 18  tempo             100 non-null    float64\ndtypes: float64(9), int64(6), object(4)\nmemory usage: 15.0+ KB\n\n\nDefinisi dari tipe data di dataset :\n- float, angka pecahan\n- int, angka bulat\n- object, kombinasi antara teks dan angka\n\n\n8.4.4 Skala Data\nSkala atau rentang data adalah batasan-batasan data yang ada di sebuah dataset. Skala data bisa diperoleh menggunakan kode data.describe()\n\ndf = pd.read_csv('./data_sampled_100.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nvalence\nyear\nacousticness\ndanceability\nduration_ms\nenergy\nexplicit\ninstrumentalness\nkey\nliveness\nloudness\nmode\npopularity\nspeechiness\ntempo\n\n\n\n\ncount\n100.000000\n100.000000\n100.000000\n100.000000\n1.000000e+02\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n100.000000\n\n\nmean\n0.506688\n1979.040000\n0.522918\n0.537518\n2.762373e+05\n0.461174\n0.120000\n0.164812\n4.940000\n0.180223\n-11.769480\n0.730000\n34.540000\n0.110832\n119.779170\n\n\nstd\n0.284391\n25.147613\n0.354783\n0.185604\n3.724344e+05\n0.253916\n0.326599\n0.302744\n3.716792\n0.155208\n5.685826\n0.446196\n21.180046\n0.194693\n31.438102\n\n\nmin\n0.029800\n1926.000000\n0.000175\n0.091800\n6.240000e+04\n0.005740\n0.000000\n0.000000\n0.000000\n0.040300\n-31.808000\n0.000000\n0.000000\n0.024500\n62.106000\n\n\n25%\n0.291500\n1960.750000\n0.207750\n0.399500\n1.671165e+05\n0.254000\n0.000000\n0.000000\n1.000000\n0.093325\n-15.430750\n0.000000\n19.000000\n0.033700\n94.762500\n\n\n50%\n0.521000\n1981.500000\n0.596000\n0.551000\n2.015920e+05\n0.465000\n0.000000\n0.000137\n5.000000\n0.124500\n-11.280000\n1.000000\n36.500000\n0.042450\n115.329000\n\n\n75%\n0.758250\n1999.500000\n0.821000\n0.681000\n2.804202e+05\n0.631500\n0.000000\n0.141500\n8.000000\n0.199750\n-7.177250\n1.000000\n48.250000\n0.066475\n137.057250\n\n\nmax\n0.977000\n2019.000000\n0.993000\n0.880000\n3.650800e+06\n0.994000\n1.000000\n0.921000\n11.000000\n0.967000\n-2.478000\n1.000000\n73.000000\n0.957000\n205.917000\n\n\n\n\n\n\n\n\n\n8.4.5 Sumber Data\nData diperoleh dari kaggle dengan link https://www.kaggle.com/datasets/vatsalmavani/spotify-dataset\nKaggle adalah platform dataset open source. Namun dataset spotify ini dapat di scrape secara mandiri dengan mendaftar sebagai developer di spotify dan mengambil data menggunakan API spotify.\nSetelah di download data diambil 100 baris secara random untuk meningkatkan performa dari coding.\n\n\n8.4.6 Keterangan\nData mempunyai beberapa lagu yang tidak valid, jadi harus dilakukan proses data cleaning."
  },
  {
    "objectID": "modul_7.html",
    "href": "modul_7.html",
    "title": "8  Transformasi Data",
    "section": "",
    "text": "9 Pelabelan Data\nPelabelan data (data labeling) adalah proses menandai atau memberi label pada data dengan kelas atau kategori yang sesuai.\nKuantitas & kualitas data pelatihan yang secara langsung menentukan keberhasilan suatu algoritma AI sehingga tidak mengherankan jika rata-rata 80% waktu yang dihabiskan untuk proyek AI membahas data pelatihan yang mencakup proses pelabelan data.\nKeakuratan model AI Anda berkorelasi langsung dengan kualitas data yang digunakan untuk melatihnya. Hal ini menjadi satu alasan mengapa proses pelabelan data merupakan bagian integral dari alur kerja persiapan data dalam membangun model AI yang handal."
  },
  {
    "objectID": "modul_7.html#data-training",
    "href": "modul_7.html#data-training",
    "title": "8  Transformasi Data",
    "section": "9.1 Data Training",
    "text": "9.1 Data Training\nMachine learning dibagi menjadi dua, yaitu supervised dan unsupervised learning.\nUnsupervised learning menggunakan dataset tanpa label untuk menemukan sebuah pola, struktur atau hubungan antar fitur untuk melatih algoritma machine learning.\nSupervised learning menggunakan dataset yang mempunyai label untuk melatih algoritma machine learning memprediksi nilai atau mengklasifikasikan sesuatu.\nNamun, tidak semua dataset mempunyai label. Dan tidak semua dataset yang tidak mempunyai label dapat di training menggunakan unsupervised learning. Jadi, kita harus melakukan proses labeling dataset.\n\n\n\nGambar12. Pelabelan data gambar\n\n\nKetika sebuah dataset diberi sebuah label, maka label tersebut sebagai dasar kebenaran atau ground truth.\nPelabelan dataset sangatlah penting untuk proses machine learning. Proses labeling data dapat mempengaruhi kualitas dan akurasi dari sebuah model.\nSelain itu, pelabelan dataset dapat meningkatkan kualitas dari dataset, sehingga dataset dapat diproses oleh lebih banyak algoritma machine learning.\nContoh lain dari pelabelan dataset deteksi email spam :\n\nKesimpulannya, jika pelabelan dataset tidak akurat, maka hasil prediksi yang dibuat oleh algoritma machine learning tidak akan akurat"
  },
  {
    "objectID": "modul_7.html#metode-pelabelan-data",
    "href": "modul_7.html#metode-pelabelan-data",
    "title": "8  Transformasi Data",
    "section": "9.2 Metode Pelabelan Data",
    "text": "9.2 Metode Pelabelan Data\nData labeling adalah langkah yang sangat penting dalam proses pengembangan model machine learning. Proses ini terlihat simpel, namun sebenarnya tidak semudah itu untuk diimplementasikan. Sebagai data scientist/data engineer, kita harus mempertimbangkan semua faktor dan metode yang ada untuk menentukan implementasi yang terbaik. Seperti semua hal di dunia programming, setiap metode pelabelan data mempunyai kelebihan dan kekurangan. Berikut beberapa metode pelabelan data yang sering digunakan :\n\n9.2.1 Internal Labeling\nProses labeling data oleh data scientist, data engineer, atau staf expert lain yang bekerja di perusahaan.\nKekuatan\n- Kualitas labeling tinggi\n- Akurasi labeling tinggi\n- Kemanan data terkontrol\nKelemahan\n- Membutuhkan waktu dan tenaga yang banyak\n- Cukup mahal\n\n\n9.2.2 Synthetic labeling\nProses labeling data yang dilakukan dengan men-generate data baru (dengan pattern yang mirip) dari data yang sudah ada. Contohnya adalah imagen (image generator) dimana sebuah gambar diputar dan digeser sehingga menghasilkan data baru. Proses ini biasanya dilakukan untuk melatih sebuah model machine learning\nKekuatan\n- Cepat\n- Tidak membutuhkan tenaga yang banyak\nKelemahan\n- Tidak semua algoritma bisa menerima semua jenis generated data\n\n\n9.2.3 Programmatic labeling\nProses labeling data yang dilakukan oleh algoritma atau mesin, umumnya algoritma machine learning berbentuk klasifikasi, clustering, atau regresi.\nKekuatan\n- Proses labeling cepat dan dapat diskalakan dengan mudah\n- Tidak membutuhkan tenaga yang banyak\nKelemahan\n- Kualitas labeling tergantung pada kualitas dataset dan penerapan algoritma machine learning\n- Cukup mahal tergantung dengan skala dan penggunaan hardware\n- Proses training model bisa lama\n\n\n9.2.4 Outsourcing\nOutsourcing adalah kegiatan merekrut perusahaan atau organisasi untuk melakukan proses data labeling. Salah satu pertimbangan utama adalah spesialisasi dan kompetensi dari sebuah perusahaan atau organisasi tersebut.\nKekuatan\n- Simple\n- Hasil labeling akurat\nKelemahan\n- Mahal\n- Memakan waktu lama\n\n\n9.2.5 Crowdsourcing\nCrowdsourcing adalah proses dimana pekerjaan data labeling didistribusikan ke khalayak publik, umumnya freelancer melalui platform website. Contohnya adalah project ReCaptcha dimana manusia diminta untuk mengisi captcha, lalu hasilnya dimasukkan ke algoritma machine learning untuk memprediksi captcha.\nKekuatan\n- Paling efisien, dapat mengolah banyak data dalam waktu singkat\nKelemahan\n- Perlu mengembangkan workflow yang cocok untuk freelancer\n- Akurasi tergantung dengan QA dan workflow\n- Potensi kebocoran data tinggi"
  },
  {
    "objectID": "modul_7.html#penggunaan-data-labeling-dalam-pengolahan-citra",
    "href": "modul_7.html#penggunaan-data-labeling-dalam-pengolahan-citra",
    "title": "8  Transformasi Data",
    "section": "9.3 Penggunaan Data Labeling dalam Pengolahan Citra",
    "text": "9.3 Penggunaan Data Labeling dalam Pengolahan Citra\nPengolahan citra adalah kegiatan memanipulasi data yang berbentuk gambar atau video. Di bidang machine learning, pengolahan citra adalah salah satu bidang yang paling penting.\nModel machine learning pengolah citra yang sering dibuat meliputi: klasifikasi, segmentasi, deteksi objek, dan estimasi pose. Semua proses pembuatan model machine learning sangat erat kaitannya dengan pelabelan data yang digunakan di data training.\nTujuan akhir dari semua model yang telah disebutkan adalah melabeli sebuah gambar atau video dengan sesuatu.\n\n9.3.1 Image Classification\n\nImage classification model adalah jenis model dalam bidang pengolahan citra yang digunakan untuk mengklasifikasikan gambar ke dalam berbagai kategori atau kelas yang telah ditentukan sebelumnya. Tujuan dari model ini adalah untuk mengidentifikasi dan memprediksi kelas atau label yang sesuai dengan gambar yang diberikan.\n\n\n9.3.2 Image Segmentation\n\nImage segmentation adalah proses dalam pengolahan citra yang membagi atau memisahkan gambar menjadi beberapa bagian/segmen yang lebih kecil. Setiap segmen mewakili area yang memiliki karakteristik visual yang serupa, seperti warna, tekstur, atau bentuk. Tujuan dari image segmentation adalah untuk memahami struktur internal gambar dan mengidentifikasi objek yang ada di dalamnya.\nKemiripan antara segmentasi dan klasifikasi adalah objek atau hasil prediksi hanya mempunyai satu nilai\n\n\n9.3.3 Object Detection\n\nDeteksi objek adalah proses dalam pengolahan citra yang bertujuan untuk mengidentifikasi dan memetakan objek-objek tertentu dalam sebuah gambar atau video. Tujuan utama dari deteksi objek adalah untuk menemukan dan menandai lokasi serta batas-batas objek yang ada dalam sebuah gambar.\nPerbedaan object detection dengan metode-metode sebelumnya yaitu objek atau hasil prediksi mempunyai lebih dari satu hasil\n\n\n9.3.4 Pengolahan Citra - Hands On Coding\nKita akan membuat model object detection menggunakan library OpenCV. Kita juga akan menggunakan algoritma classifier bernama Haar Cascade. Untuk file lengkapnya bisa dilihat di sini link paper.\nAlgoritma ini menggunakan sebuah sistem machine learning dimana kita memberikan gambar positif dan negatif. Gambar positif adalah gambar objek yang kita ingin klasifikasikan, sedangkan gambar negatif adalah gambar objek yang tidak ingin kita klasifikasikan.\nKarena keterbatasan waktu, kita akan menggunakan model yang sudah jadi untuk melakukan deteksi objek pada gambar yang kita inginkan. Pada kasus ini, kita akan mendeteksi rambu lalu lintas stop.\n\n9.3.4.1 Download Library\nLibrary bisa di download menggunakan pip install opencv-python dan pip install matplotlib\n\n\n9.3.4.2 Import Library\nbuat file python dan import package-package sebagai berikut:\n- import cv2\n- import matplotlib.pyplot as plt\n\n\n9.3.4.3 Download Model dan Gambar Contoh\nDownload pre-trained model yang mempunyai format .xml di link ini. Jangan lupa download gambar stop sign.\n\n\n\nGambar17. Contoh gambar stop sign\n\n\n\n\n9.3.4.4 Ekstrak Model dan Gambar ke Directory\n\n\n9.3.4.5 Tampilkan Gambar Menggunakan OpenCV dan Matplotlib\n\n# buka gambar menggunakan opencv\nimg = cv2.imread(\"image.jpg\")\n\n# OpenCV membuka gambar menggunakan metode BRG (blue red green)\n# namun kita ingin membuka dengan metoded RGB (red green blue)\n# kita harus konversi dari BRG ke RGB\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# kita juga membutuhkan gambar versi grayscale (hitam putih)\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# menampilkan gambar menggunakan matplotlib\nplt.subplot(1, 1, 1)\nplt.imshow(img_rgb)\nplt.show()\n\n\n\n\n\n\n9.3.4.6 Buat Algoritma Deteksi Objek\n\n# load model yang sudah dilatih untuk mendeteksi stop sign\n# model berbentuk file xml\nstop_data = cv2.CascadeClassifier('stop_data.xml')\n\n# buat ukuran kotak minimum agar ukuran kotak yang terdeteksi tidak terlalu kecil\nfound = stop_data.detectMultiScale(img_gray,\n                                   minSize=(5, 5))\n\n# hitung jumlah objek yang ditemukan.\namount_found = len(found)\n\n# jika objek tidak ditemukan maka tidak dilakukan apa-apa\nif amount_found != 0:\n\n    # jika objek yang ditemukan lebih dari satu, maka :\n    for (x, y, width, height) in found:\n\n        # kita gambar sebuah kotak hijau di objek yang ditemukan\n        cv2.rectangle(img_rgb, (x, y),\n                      (x + height, y + width),\n                      (0, 255, 0), 5)\n\n# tampilkan hasil citra menggunakan plt\nplt.subplot(1, 1, 1)\nplt.imshow(img_rgb)\nplt.show()\n\n\n\n\n\n\n9.3.4.7 Eksperimen!\nCari gambar rambu lalu lintas stop di internet atau ambil foto secara langsung, lalu masukkan gambar tersebut ke coding. Terapkan algoritma dan lihat apakah algoritma deteksi objek berjalan dengan sempurna.\nPetunjuk : edit line cv2.imread(“[nama file foto]”) untuk menggunakan gambar atau foto anda sendiri"
  },
  {
    "objectID": "modul_7.html#penggunaan-data-labeling-dalam-natural-language-processing",
    "href": "modul_7.html#penggunaan-data-labeling-dalam-natural-language-processing",
    "title": "8  Transformasi Data",
    "section": "9.4 Penggunaan Data Labeling dalam Natural Language Processing",
    "text": "9.4 Penggunaan Data Labeling dalam Natural Language Processing\nNatural Language Processing atau NLP mengacu pada analisis bahasa manusia dan bentuknya selama interaksi baik dengan manusia lain maupun dengan mesin. Menjadi bagian dari linguistik komputasi awalnya, NLP telah berkembang lebih lanjut dengan bantuan Artificial Intelligence dan Deep Learning.\n\n9.4.1 Sentiment Analysis\n\nAdalah sebuah proses pemberian label sentimen terhadap sebuah teks (kata, kalimat, atau paragraf) berdasarkan ekspresi perasaan manusia. Tujuan utamanya adalah untuk mengkategorikan teks berdasarkan sentimen yang diekspresikan oleh teks.\nTeks dianalisa dengan mempertimbangkan beberapa faktor, antara lain : pemilihan kata, konteks, subjektivitas, dan beberapa faktor lain. Lalu sebuah sentimen berupa perasaan (marah, sedih, bahagia, bingung) dilabelkan ke teks tersebut.\n\n\n9.4.2 Named Entity Recognition (NER)\n\nAdalah sebuah proses pemberian label entitas dalam sebuah teks. Entitas adalah sebuah kategori luas yang dapat diatur oleh pembuat algoritma. Contoh dari entitas antara lain, nama seseorang, lokasi, negara, organisasi, besaran, jumlah uang, dan lain lain.\nTujuan utama dari NER adalah mengekstrak informasi entitas dari sebuah teks. Dalam proses labeling, algoritma harus memperhatikan konteks, struktur kata, dan arti tersirat dalam sebuah teks untuk mengidentifikasi sebuah entitas secara akurat.\n\n\n9.4.3 Part of Speech Tagging (POS-tagging)\n\nAdalah sebuah proses pemberian label kategori tata-bahasa (grammar) dalam sebuah teks. Pada umumnya sebuah kalimat terdiri dari beberapa kata yang termasuk dalam kategori grammar kata benda (noun), kata sifat (adjective), keterangan (adverb), kata ganti (pronoun), dan lain-lain.\nTujuan utama POS-tagging adalah untuk menentukan kategori grammar dari sebuah kata di dalam sebuah konteks yang ada di teks. POS-tagging sangatlah penting karena grammar atau tata-bahasa dapat mengubah makna dari sebuah kalimat secara keseluruhan.\n\n\n9.4.4 Hands On Coding\nDalam praktikum ini kita akan membuat algoritma Named Entity Recognition menggunakan library spaCy dan sebuah artikel.\n#### Download Library dan Dataset yang Dibutuhkan\n- python -m pip install -U pip setuptools wheel\n- pip install -U spacy\n- python -m spacy download en_core_web_sm\n\n9.4.4.1 Import library\n\nimport spacy\n\nfrom spacy import displacy\n\n\n\n9.4.4.2 Masukkan Model NER SpaCy\n\nNER = spacy.load(\"en_core_web_sm\")\n\n\n\n9.4.4.3 Tentukan Artikel yang Akan di Proses. Artikel Harus Berbahasa Inggris.\n\nrawtext = \"Asus Zenfone 10 will please everyone who finds the current flagship cellphones too bulky: the compact and relatively light smartphone comes with a lot of memory on request, which also works as fast as an arrow. It scores with very high system performance, long software updates, fast WiFi 7 and a stylish, IP-certified case that is also available in brighter color variants. At a price of US$749, however, the smartphone competes with the Samsung Galaxy S23 or the Apple iPhone 13 mini and you have to accept a few weaknesses. For example, compared to Samsung's flagship, the camera lacks the optical zoom capability, which reduces the flexibility of the camera setup. Compared to the iPhone, the images appear less sharp. The screen could also be a little brighter, the strong heating of the phone under high load is annoying and the battery life is average. The fact that Asus installs a USB 2.0 port in its Zenfone 10 is also no reason for joy. But users who want a 3.5mm port combined with high performance will hardly get past the Zenfone 10.\"\n\n\n\n9.4.4.4 Masukkan Artikel ke Fungsi NER, Lalu tampilkan hasilnya\n\ntext1 = NER(rawtext)\n\nfor word in text1.ents:\n    print(word.text, '-', word.label_)\n\nAsus Zenfone - PERSON\nWiFi - PERSON\n7 - CARDINAL\nIP - ORG\n749 - MONEY\nSamsung - ORG\nApple - ORG\n13 - CARDINAL\nSamsung - ORG\niPhone - ORG\nAsus - PERSON\n2.0 - CARDINAL\nZenfone 10 - LAW\n3.5mm - QUANTITY\n\n\n\n\n9.4.4.5 Kesimpulan\nHasil dari NER adalah sebuah kata yang dideteksi dan kategori dari kata tersebut. Namun bisa dilihat bahwa hasil dari algoritma tidak 100% akurat. Ada beberapa hasil yang labelnya tidak sesuai, contohnya “Asus - PERSON”. Algoritma melabeli kata Asus sebagai sebuah nama orang, padahal Asus seharusnya adalah sebuah organisasi (ORG).\n\n\n9.4.4.6 Eksperimen!\nCari teks dari artikel, wikipedia, berita, lalu masukkan teks tersebut ke algoritma NER dan analisa hasil dan akurasi dari algoritma.\nPetunjuk : masukkan teks ke dalam variable rawtext sebagai string (“”)"
  },
  {
    "objectID": "modul_7.html#analisa-kualitas-dan-akurasi-data-untuk-pelabelan",
    "href": "modul_7.html#analisa-kualitas-dan-akurasi-data-untuk-pelabelan",
    "title": "8  Transformasi Data",
    "section": "9.5 Analisa Kualitas dan Akurasi Data untuk Pelabelan",
    "text": "9.5 Analisa Kualitas dan Akurasi Data untuk Pelabelan\nAkurasi dalam pelabelan data mengukur seberapa dekat pelabelan dengan ground truth, atau seberapa baik fitur berlabel dalam data set konsisten dengan kondisi dunia nyata. Dalam model pemrosesan bahasa alami (NLP) contohnya adalah seberapa akurat model memberikan label sentimen terhadap sebuah teks.\nKualitas dalam pelabelan data adalah tentang akurasi dataset secara keseluruhan. Apakah pekerjaan semua pemberi label terlihat sama? Apakah pelabelan secara konsisten akurat di seluruh data set?\n\n9.5.1 Definisi Data yang Berkualitas\nProses pelabelan data selalu bertumpu pada kualitas data. Trash in, trash out adalah sebuah mantra yang populer di dunia pengolahan data. Artinya, data yang tidak berkualitas tidak dapat menghasilkan produk yang berkualitas.\nMaka dari itu, data harus kita tentukan kualitas dan akurasinya agar kita dapat menentukan ekspektasi terhadap hasil dari pelabelan data yang akan kita lakukan. Pada umumnya data yang berkualitas mempunyai :\n- Tidak ada nilai kosong/korup\n- Nilai unik tiap entry\n- Variasi seimbang\n- Metode pengambilan data yang tangguh\n- Dokumentasi lengkap\n- Format yang rapi\n\n\n9.5.2 Faktor-faktor yang mempengaruhi kualitas proses pelabelan data\n\n9.5.2.1 Pemahaman Data\nData mempunyai banyak fitur dan value. Pengetahuan tentang fitur data dan nilai-nilai yang ada didalamnya akan sangat membantu untuk menentukan proses pelabelan data.\n\n\n9.5.2.2 Kompetensi Developer/Trainer\nDeveloper harus mempunyai pengetahuan yang luas di bidang machine learning atau bidang yang relevan. Developer harus bisa menentukan kualitas dataset, langkah-langkah data preprocessing, algoritma yang sesuai, dan cara untuk mengukur akurasi pelabelan.\n\n\n9.5.2.3 Ketangguhan Workflow\nProses pelabelan data pasti mempunyai tahap-tahap yang ditetapkan oleh kepemimpinan. Proses atau workflow tersebut harus tahan terhadap gangguan-gangguan seperti human error, machine error, perubahan requirements, ambiguitas, sehingga proses dapat berjalan lancar selamanya. Salah satu proses terpenting yaitu QA, yang akan kita bahas di slide selanjutnya\n\n\n\n9.5.3 Metode QA untuk mengukur kualitas data\n\n9.5.3.1 Consensus Algorithm\nAdalah sebuah metodologi untuk mencapai sebuah keputusan mengenai kualitas data dengan cara mengumpulkan persetujuan (consensus) dari semua atau sebagian orang yang melakukan proses data labeling.\n\n\n9.5.3.2 Benchmarking dan Gold Standard\nAdalah proses membandingkan hasil data labeling dari beberapa model dengan mengaplikasikan model tersebut ke dataset yang sering digunakan. Tujuan utamanya dalah untuk memperoleh batas bawah (baseline) atau reference point untuk mengevaluasi kualitas dari model. Gold standard adalah sebuah hasil pelabelan dari sebuah model berkualitas tinggi terhadap dataset berkualitas tinggi. Gold standard merepresentasikan teknologi terbaik, aplikasi terakurat, dan dataset berkualitas paling tinggi, hasilnya adalah nilai akurasi yang paling tinggi diantara riset atau percobaan lain.\n\n\n9.5.3.3 Cronbach Alpha Test\nAdalah sebuah metode untuk mengukur tingkat konsistensi dari beberapa variabel yang mempunyai variabel laten yang sama. Variabel laten adalah variabel yang tidak mempunyai metrik atau ukuran secara tersendiri. Untuk mengukur variabel laten, diperlukan beberapa variabel lain yang saling berhubungan dan mempunyai konsistensi tinggi. Cronbach Alpha berfungsi untuk mengukur variabel lain ini.\nContohnya adalah kita kana mengukur tingkat ekstroversi seseorang. Tingkat ekstroversi ini adalah variabel laten karena kita tidak bisa mengukur tingkat ekstroversi dengan sendirinya. Maka diperlukan kuesioner dengan 5 pertanyaan. Hasil dari 5 pertanyaan inilah yang akan kita tes dengan Cronbach Alpha Test untuk menentukan apakah mereka merepresentasikan tingkat ekstroversi seseorang.\n\n\n\n9.5.4 Hands On Coding - Cronbach Alpha Test\nRumus dari Cronbach Alpha test adalah:\n\\[\n\\alpha = \\dfrac{k}{k-1}(1-\\dfrac{\\Sigma s^2_i}{s^2_X})\n\\]\n\\(\\alpha\\) = koefisien reliabilitas \\(k\\) = jumlah item set \\(s^2_i\\) = nilai variance setiap item i dimana i = 1, 2, …, k, \\(s^2_X\\) = nilai variance dari semua item\nNilai α diatas 0.7 termasuk cukup bagus untuk sebuah paper.\nSebagai contoh, kita akan membuat sebuah sistem untuk mengecek kepuasan pelanggan terhadap 5 produk baru kita. Pelanggan diberikan sebuah kuesioner dan pelanggan akan memberi rating dari 1-5 terhadap 5 produk baru kita.\n\n9.5.4.1 Install dan Import Library\n\npip install numpy\n\npip install scipy\n\n\n\n9.5.4.2 Buat Data Menggunakan NumPy\nAnda dapat menambahkan data baru\n\ndata = np.array([[4,3,5,2,4], [5,4,4,3,5], [3,2,3,4,3], [4,4,5,5,4]])\nprint(data)\n\n[[4 3 5 2 4]\n [5 4 4 3 5]\n [3 2 3 4 3]\n [4 4 5 5 4]]\n\n\n\n\n9.5.4.3 Hitung Rata-Rata dari Setiap Item/Fitur\n\nitem_means = np.mean(data,axis=0)  \nprint(item_means)\n\n[4.   3.25 4.25 3.5  4.  ]\n\n\n\n\n9.5.4.4 Hitung Variansi dari Total Skor dan Skor Setiap Item\n\ntotal_var = np.var(np.sum(data, axis=1))\nitem_var = np.var(data, axis=0, ddof=1)\nprint(f'total variance : ',total_var)\nprint(f'item variance : ',item_var)\n\ntotal variance :  7.5\nitem variance :  [0.66666667 0.91666667 0.91666667 1.66666667 0.66666667]\n\n\n\n\n9.5.4.5 Hitung Rata-Rata dari Setiap Item/Fitur\n\nnum_items = len(item_means)\ncronbach_alpha = (num_items / (num_items - 1)) * (1-(np.sum(item_var) / total_var))\nprint(f'Nilai Cronbach Alpha adalah : ', cronbach_alpha)  \n\nNilai Cronbach Alpha adalah :  0.4444444444444444\n\n\nHasil nilai Cronbach Alpha adalah 0.4. Nilai ini menunjukkan bahwa konsistensi dari kuesioner yang terdiri dari 5 fitur (item 1 - 5) tergolong rendah dan tidak mencerminkan kepuasan pelanggan secara akurat.\nPada umumnya, nilai Cronbach Alpha yang tergolong bagus adalah diatas 0.7"
  },
  {
    "objectID": "modul_7.html#keamanan-pelabelan-data",
    "href": "modul_7.html#keamanan-pelabelan-data",
    "title": "8  Transformasi Data",
    "section": "9.6 Keamanan Pelabelan Data",
    "text": "9.6 Keamanan Pelabelan Data\nPelabelan data adalah sebuah pekerjaan yang memakan banyak waktu dan tenaga. Proses pelabelan data mempunyai banyak cabang dan workflow, dan di setiap langkah workflow selalu ada resiko kebocoran data.\nTerlebih jika pekerjaan pelabelan data dikerjakan oleh organisasi lain yang berada di luar kendali kita. Oleh karena itu, kita harus membuat sebuah workflow yang dapat mencegah kebocoran data, serta paham tentang prinsip-prinsip dasar tentang keamanan data.\n\n9.6.1 Resiko Keamanan Outsourcing Data Labeling\n\nMengakses data dari jaringan yang tidak aman atau menggunakan perangkat tanpa perlindungan malware\n\nMengunduh atau simpan sebagian data (mis., screen capture, flash drive)\n\nMemberi label data saat berada di tempat umum\n\nTidak memiliki pelatihan, konteks, atau akuntabilitas terkait dengan aturan keamanan untuk pekerjaan labeling\n\nBekerja di lingkungan fisik atau digital yang tidak disertifikasi untuk mematuhi peraturan data (mis., HIPAA, SOC 2).\n\n\n\n9.6.2 Tiga area yang perlu menjadi perhatian untuk menjaga keamanan dokumen\n\nOrang dan Tenaga Kerja: Ini dapat mencakup pemeriksaan latar belakang untuk pekerja dan mungkin mengharuskan pemberi label untuk menandatangani perjanjian kerahasiaan (NDA) atau dokumen serupa yang menguraikan persyaratan keamanan data.\n\nTeknologi dan Jaringan: Pekerja mungkin diminta untuk menyerahkan perangkat yang mereka bawa ke tempat kerja, seperti ponsel atau tablet.\n\nFasilitas dan Ruang Kerja: Pekerja dapat duduk di tempat yang menghalangi orang lain untuk melihat pekerjaan mereka."
  }
]